---
output: 
    pdf_document:
        fig_caption: yes
        number_sections: true
fontsize: 12pt
geometry: margin=1in
bibliography: References.bib
csl: harvard.csl
---

<!-- maketile -->
\begin{center}
\Large\scshape{Explosive Prices for South African Art} \\ 
\vspace{1em}
\large\normalfont{Laurie Binge}\footnote{PhD candidate at the Department of Economics at Stellenbosch University. Corresponding author at: lhbinge@gmail.com} \\
\large\normalfont{Willem H. Boshoff}\footnote{Associate Professor at the Department of Economics at Stellenbosch University. Email address: wimpie2@sun.ac.za} \\
\normalsize\textit{Stellenbosch University, Stellenbosch, South Africa.} \\
\normalsize\normalfont{\today} 
\end{center}
\begin{small}

South African art has experienced a surge in popularity over the last few decades, with large price increases and record prices at local and international auctions. This paper looks for evidence of a bubble in the South African art market by testing for mildly explosive prices. The outcome of the test depends critically on the estimation of a reasonably accurate quality-adjusted price index, which can be challenging for unique assets such as art. The choice of methodology determines whether there is compelling evidence of a bubble and what origination and termination dates are identified. This paper estimates three sets of art price indices, based on the three main methodologies used to estimate quality-adjusted price indices for unique assets. The regression-based indices seem to point to consistent evidence of explosive price behaviour in the run-up to the financial crisis. Further analysis indicates that the bubble was relatively dispersed throughout the market, although prices seem to have been especially explosive for high-end oil paintings by the top artists. 

\vspace{0.5em}
\noindent{\textbf{JEL Classification:} C43, Z11, E31, G12} \\
\noindent{\textbf{Keywords:} South African Art, Hedonic Price Index, Pseudo Repeat Sales, Explosive Prices }
\end{small}
\renewcommand{\thefootnote}{\arabic{footnote}}

#Introduction
South African art has experienced a surge in popularity over the last few decades. The South African art market is relatively established and well-developed, and has grown markedly over the last two decades, both in terms of the number of transactions and total turnover [@Fedderke2014]. Artworks by South African artists have exhibited large price increases and have reached record prices at international and local auctions, both for the country's "masters" - including Irma Stern, Walter Battiss, and JH Pierneef - and contemporary artists like William Kentridge [@Naidoo2013]. In 2011, for instance, Bonhams in London sold Irma Stern's *"Arab Priest"* for a hammer prices of £2.7 million, a world record for a South African artwork at auction. Also in 2011, Stern's *"Two Arabs"* was sold by Strauss & Co. for a hammer price of R19 million, a record for a South African auction. 

The increase in the popularity of South African art, both locally and abroad, has sparked a vibrant market for collectors and investors. The interest in South African art, at least partly as an investment vehicle, is commensurate with international trends, where fine art has become an important alternative asset class in its own right [@Renneboog2014]. In addition to the potential for appreciation in value, artworks may be used to aid portfolio diversification, as collateral for loans, or to take advantage of slacker regulatory and tax rules. Thus, unlike pure financial investments, artworks are durable goods with consumption and investment good characteristics [@Renneboog2014].

The large price increases and record prices for South African artworks at local and international auctions, especially between 2008 and 2011, prompted commentators at the time to claim that the market was overheating and suggest the possibility of a "bubble" in the market (e.g. @Rabe2011; @Hundt2010; @Curnow2010). According to the New Palgrave Dictionary of Economics, "*bubbles refer to asset prices that exceed an asset's fundamental value because current owners believe they can resell the asset at an even higher price*" [@Brunnermeier2008]. A bubble consists of a sharp rise in a given asset price, above a level sustainable by fundamentals, followed by a sudden collapse [@Kraussl2016]. 

When it comes to the art market, however, it is particularly challenging to determine the fundamental value from which prices potentially deviate. In the case of stocks, dividends have been used to obtain the expected cash flow as a measure of fundamental value. Rents and convenience yields can potentially be used for real estate prices and commodity prices [@Penasse2014]. In contrast, artworks do not generate a future income stream (e.g. dividends or rents) that can be discounted to determine the fundamental value. Artworks usually have little inherent value, unless the materials used have a high intrinsic value [@Spaenjers2015]. Instead, artworks are acquired for a kind of non-monetary utility or aesthetic dividend, sometimes described as "aesthetic pleasure" [@Gerard-Varet1995]. This dividend can be seen as the rent one would be willing to pay to own the artwork over a given period. It can reflect aesthetic pleasure, but may also provide additional utility as the signal of wealth [@Mandel2009]. The price of an artwork should equal the present value of future private utility dividends over the holding period, plus the expected resale value. The value of the dividend is unobservable and is likely to vary greatly across collectors, based on their motivations and characteristics [@Penasse2014]. Thus, it almost impossible to determine the fundamental value of art [@Kraussl2016]. 

To overcome this issue, this paper follows @Kraussl2016 in using a direct method of bubble detection developed by @Phillips2011. The approach is based on a right-tailed augmented Dickey-Fuller (ADF) test, which can detect explosive behaviour directly in time series. @Phillips2011 originally applied the method to stock prices. They showed that there was evidence of explosiveness in stock prices, but not dividend yields, implying that price explosiveness could not be explained by developments in fundamentals. 

The test, however, requires the estimation of a reasonably accurate quality-adjusted price index, which can be challenging for unique assets such as art and real estate. The choice of methodology will determine whether there is compelling evidence of a bubble and what origination and termination dates are identified. Each methodology has shortcomings and the danger is that the biases inherent in each methodology may be driving the results. 

This paper uses three broad methodologies to develop quarterly price indices for South African art. The use of quarterly indices allows the paper to investigate higher frequency movements in art prices. Simple central tendency indices are estimated as a baseline for comparison, but do not adequately control for quality-mix changes over time. The hedonic regression method is able to control more adequately for quality-mix changes, but has the shortcoming of potential omitted variable bias. Repeat sales indices suffer less from potential omitted variable bias, but have the shortcoming of potential sample selection bias. In this case the scarcity of repeat sales observations in the database limits the usefulness of the classical repeated sales approach. The paper proposes a simple hybrid repeat sales method to address the problem of scarcity of repeat sales observations and to some extent the potential omitted variable bias inherent in the hedonic method. The paper then compares and evaluates the indices in terms of smoothness, which helps to determine the bubble period more accurately.

The regression-based indices seem to point to consistent evidence of mildly explosive price behaviour in the run-up to the financial crisis, between 2005/06 and 2008. Different segments of the market are then investigated to find out whether this bubble was dispersed throughout the market. The results indicate that the bubble was relatively dispersed, although prices seem to have been especially explosive for high-end oil paintings by the top artists.

#South African Art Auction Data
```{r cleaning, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##=====================##
## READING IN THE DATA ##
##=====================##
suppressMessages(library(zoo))
suppressMessages(library(ggplot2))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(stargazer))
suppressMessages(library(micEcon))
suppressMessages(library(quantreg))
suppressMessages(library(McSpatial))
suppressMessages(library(quantmod))
suppressMessages(library(xtable))
suppressMessages(library(scales))
suppressMessages(library(tseries))
suppressMessages(library(urca))
suppressMessages(library(mFilter))

setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\Art Price Index\\R Code")

datums <- read.csv("datums.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
datums$Datum <- as.Date(datums$Datum, format = "%Y/%m/%d")

assets <- read.csv("Assets.csv", header=TRUE, na.strings = "", skipNul = TRUE)

artdata <- read.csv("Auction database.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE, 
                    colClasses=c("character","numeric","numeric","numeric","numeric","factor","factor","factor","character",
                                 "factor","factor","factor","character","factor","factor","factor","numeric","character",
                                 "numeric","numeric","numeric","numeric","numeric","numeric"))

##===================##
## CLEANING THE DATA ##
##===================##
artdata$date <- as.Date(artdata$date)
artdata$med_code <- factor(artdata$med_code, labels=c("Drawing", "Watercolour", "Oil", "Acrylic", "Print/Woodcut",
                                                      "Mixed Media","Sculpture","Photography", "Other"))
artdata$ah_code <- factor(artdata$ah_code, labels=c("5th Avenue","Ashbeys","Bernardi","Bonhams","Russell Kaplan",
                                                    "Stephan Welz","Strauss","Christies"))
artdata$timedummy <- factor(as.yearqtr(artdata$date, "%Y-%m-%d"))
artdata$lnprice <- log(artdata$price)
artdata$lnarea <- log(artdata$area)
artdata$lnarea2 <- artdata$lnarea*artdata$lnarea
#inteaction term: sculptures often only reported with 1 dimension (height)
artdata$lnsculpt_area <- ifelse(artdata$med_code=="Sculpture", artdata$lnarea, 0)
artdata$counter <- as.numeric(artdata$timedummy)

##----------------------
##Rank Artists by Volume
##----------------------
#Rank by Total Volume (all)
rankings <- count(artdata, artist)
rankings$rank_all <- dense_rank(desc(rankings$n))    #rank by density, with no gaps (ties = equal)
rankings$rank_total <- row_number(desc(rankings$n))  #equivalent to rank(ties.method = "first")
rankings$n <- NULL
artdata <- merge(artdata, rankings, by.x="artist", by.y="artist",all.x=TRUE)
```

```{r loadrep, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#------------------------------------------------------------------------------------------------------------------------------
#Load pre-calculated dataset (for speed) from API_3.R
artdata <- read.csv("artdata_lnrep2.csv", header=TRUE)
artdata$med_code <- factor(artdata$med_code, labels=c("Acrylic","Drawing","Mixed Media","Oil","Other",        
                                                      "Photography","Print/Woodcut","Sculpture","Watercolour"))

##----------------------
##Rank Artists by Value
value <- aggregate(artdata$hammer_price, by=list(artdata$artist), FUN = sum)
volume <- aggregate(artdata$hammer_price, by=list(artdata$artist), FUN = length)
value <- cbind(value,volume[,2],value$x/volume$x)    
colnames(value) <- c("artist","value","volume","ave_price")

rankings$value <- row_number(desc(value$value)) 
rankings$volume <- row_number(desc(value$volume))
rankings$ave_price <- row_number(desc(value$ave_price))

artdata <- merge(artdata, rankings[,c(1,4,6)], by.x="artist", by.y="artist",all.x=TRUE)
```

The literature on estimating art price indices has relied almost exclusively on publicly available auction prices. It is generally accepted that auction prices set a benchmark that is also used in the private market [@Candela2001; @Renneboog2012]. Private sales prices are likely anchored by auction prices and are likely to be highly correlated over time for similar artworks, even if their levels are different [@Olckers2015]. This paper relies on publicly available auction prices, which are the only consistently available price data for the South African art market. 

The indices are based on data recorded by AuctionVault. This data cover sales of South African art at 8 auction houses[^13] between 2000 and 2015. The database includes 52,059 sales by 4,123 different artists. The following characteristics are available for each auction record: hammer price, artist name, title of work, medium, size, whether or not the artwork is signed, dated and titled, auction house, date of auction, and the number of distinct works in the lot. 

[^13]: These are: 5th Avenue, Ashbeys, Bernardi, Bonhams, Christies, Russell Kaplan, Stephan Welz & Co and Strauss & Co.

Strauss & Co and Stephan Welz & Co are the two local auction houses that have handled the bulk of sales in recent years, with auctions in Cape Town and Johannesburg. Bonhams in London is the only major international auction house with a dedicated South African art department, and has two major South African art sales annually. The auction houses follow an open ascending auction, where the winner pays the highest bid. A sale is only made if the hammer price is above the secret reserve price. Otherwise the artwork is unsold and is said to be bought in [@Fedderke2014]. Like most studies, the database lacks information on buy-ins and the analysis is forced to disregard the potential sample selection problem. The following section briefly discusses the variables available in the database and included in models of art prices.

##Artwork characteristics
*Artist reputation*: Hedonic models typically include dummy variables to control for the artists. However, some artists often have to be excluded from estimation, due to a lack of degrees of freedom. Alternatively, a reputation variable can be constructed, either from the art literature, or from the auction data itself. The models in this paper are estimated using a continuous reputation variable estimated with the 2-step hedonic approach suggested by @Kraussl2008. This approach allows the use of every auction record, instead of only those auction records that belong to a sub-sample of selected artists. 

*Size*: The most common variable used to describe the physical characteristics of an artwork is its size or surface area. The models use the natural logarithm of the surface area of the artwork in $cm^2$. The models also include size and medium interaction terms. This is particularly important for sculptures, as the size of a sculpture is usually only recorded in terms of its height (in cm). 

*Auction house*: Dummy variables for the auction houses are also typically included. The more prominent auction houses usually have a positive effect on prices. One reason might be that the more renowned auction houses will offer higher quality work [@Kraussl2010a]. Thus, the variables might be picking up otherwise unobservable quality differences and do not necessarily reflect auction house certification [@Renneboog2012]. Moreover, different auction houses charge different commissions to both buyers and sellers. For example, Strauss & Co reported a buyer's premium of 10%-15%, while Bonhams charged premiums of up to 25% [@Olckers2015]. The hammer prices exclude these premiums and are therefore not a perfect measure of the cost to the buyer and revenue to the seller. For the purposes of a price index the auction house dummies should capture the different premiums charged by the auction houses. 

*Mediums*: Average prices vary across mediums. This might be due to the durability of the medium, the stage of production the medium is associated with (e.g. preparatory drawings) and in some cases the replacement value of the materials used (e.g. sculptures cast in bronze). Oil paintings traditionally earn the highest prices. The availability of copies may decrease the prices of prints and photographs relative to other mediums. Studies typically include dummy variables for the different mediums as defined in their data [@Kraussl2010a]. The models use the 9 mediums defined in the dataset; the same mediums used by @Olckers2015.

*Authenticity dummies*: Models often include dummies for whether the artwork is signed and dated. There might be a premium for these attributes, as there is less uncertainty about authenticity [@Renneboog2014]. These dummies are included in the models and are expected to have positive coefficients. 

*Number of works in the lot*: The models also control for cases in which more than one artwork was sold in the same auction lot. This is because the recorded size corresponded to each artwork separately and not the group. Moreover, it is possible that lots including more than one artwork fetch a lower price per artwork than if they sold separately.

#Methodology
##Estimating Art Price Indices
The construction of price indices for unique assets is challenging for at least two reasons [@Jiang2014]. Firstly, the low frequency of trading means that only a subset of the market is traded at a given time, while the prices of non-transacted items are unobservable. Secondly, the heterogeneity of these items means that the quality of assets sold is not constant over time. Thus, the composition of items sold will generally differ between periods, making it difficult to compare prices over time [@Hansen2009]. Constructing an index for unique assets, such as artworks, therefore requires a different approach than is used for indices of stocks, bonds and commodities. Four broad measurement techniques have been used to construct these indices: central tendency, hedonic, repeat sales, and hybrid methods [@Eurostat2013].  

###Central Tendency Methods
The simplest way to construct a price index is to calculate a measure of central tendency from the distribution of prices. The median is often preferred to the mean as a measure of central tendency, because price distributions are generally positively skewed [@Hansen2009]. These average measures have the advantage of being simple to construct and do not require detailed data. Despite its advantages, an index based on average prices does not account for the difficulties mentioned above. For assets such as artworks, central tendency indices may be more dependent on the mix of objects that come to market than changes in the underlying market. If there is a correlation between turning points in asset price cycles and compositional and quality changes, then an average could be especially inaccurate [@Hansen2009].

Stratified central tendency measures can control for compositional changes in assets sold over time to some extent, by separating the sample into subgroups according to individual characteristics such as artist and medium. After constructing a measure of the central tendency for each subgroup, the aggregate mix-adjusted index is typically calculated as a weighted average of the indices for the subgroups (e.g. a Fisher index) [@Eurostat2013]. However, these mix-adjusted measures adjust only for the variation in the quality of assets across the subgroups. The number of subgroups may be increased to reduce the quality-mix problem, if the data permits, although some quality-mix changes will likely remain [@Hansen2009]. However, this will reduce the average number of observations per subgroup and raise the standard error of the overall index [@Eurostat2013]. If the subgroups become very small, small changes can have a large impact on the index. As a consequence of these difficulties, the repeat sales and hedonic methods have dominated the international literature, especially with regard to art price indices. 

Two central tendency price indices are estimated at a quarterly frequency to act as a baseline in comparing the indices resulting from the different methodologies. The median index is simply the median price for each quarter. The Fisher index is a mix-adjusted central tendency index, which is stratified by artist and medium. The base periods are allowed to vary for each index point and the index points are then chained together to form the overall chain-link index. 

###Hedonic Methods
The hedonic method is derived from the microeconomic theory of implicit prices, which supposes that utility is derived from the characteristics or attributes of goods [@Lancaster1966]. @Griliches1961 first applied the hedonic method for the valuation of automobiles. @Rosen1974 provides a theoretical framework in which the hedonic price function emerges from the interaction between buyers and sellers. 

The hedonic method controls for quality-mix changes by attributing implicit prices to a set of value-adding characteristics of the individual item. Hedonic regressions control for the observable attributes of an asset to obtain an index reflecting the price of a "standard asset" [@Renneboog2002]. The approach entails regressing the logarithm of the sales price on the relevant attributes. The standard hedonic model usually takes the following form: 
$$\ln P_{it} = \sum_{t=1}^T \delta_t D_{it} + \sum_{j=1}^J \beta_{jt} X_{jit} + \sum_{k=1}^K \gamma_{kt} Z_{kit} + \epsilon_{it}$$
where $P_{it}$ represents the price of item $i$ at time $t$ $(t=1, ..., T)$; $D_{it}$ is a time dummy variable taking the value of 1 if item $i$ is sold in period $t$ and 0 otherwise, $X_{jit}$ is a set of $j$ $(j=1, ..., J)$ observed attributes of item $i$ at time $t$; $Z_{kit}$ is a set of $k$ $(k=1, ..., K)$ unobserved attributes that also influence the price; and $\epsilon_{it}$ is a random (white noise) error term. 

The coefficients on the time dummies provide an estimate of the average increase in prices between periods, holding the change in any of the measured quality dimensions constant [@Griliches1961]. In other words, they capture the "pure price effect" [@Kraussl2010]. The price index is then simply the series of estimated coefficients: $\hat{\delta_1}, ..., \hat{\delta_T}$.

The most common form of the hedonic equation assumes that the implicit prices (i.e. the coefficients $\beta_t$ and $\gamma_t$) are constant over the entire sample. However, when demand and supply conditions (e.g. tastes) change, the implicit prices of the attributes may change [@Renneboog2012]. One way to allow for shifts in parameters is to employ an adjacent-periods regression [@Triplett2004]. Separate regressions are estimated for adjacent time periods and the sequence of shorter indices are then chain-linked together to form the continuous overall index [@McMillen2012]. This method allows the coefficients, and therefore the implicit prices assigned to the characteristics, to vary in each regression [@Triplett2004]. There is a trade-off in selecting the length of the estimation window. Shorter estimation windows decrease the likelihood of large breaks but also decrease the number of observations used to estimate the parameters [@Dorsey2010]. 

In addition to the standard full sample hedonic index, two adjacent-period indices are calculated by estimating separate models for 1-year and 2-year adjacent sub-samples. A 5-year overlapping-periods index is also estimated, allowing gradual shifts in the implicit prices [@Shimizu2010].

The majority of studies on art price indices have used hedonic models to construct the indices, due to the lack of repeat sales of artworks and the availability of information on many of their important attributes. @Anderson1974 was the first to apply a hedonic regression to art prices. More recent examples include: @Renneboog2002, who estimated an index of Belgian paintings; @Kraussl2010, who studied the prices of the top 500 artists in the world; @Kraussl2010a, who analysed the performance of art in Russia, China and India; and @Kraussl2014 who analysed art from the Middle East and Northern Africa region. 

The primary difficulty with hedonic price indices is this potential omitted variable bias. If the functional form is misspecified or omitted variables are correlated with sales timing, it will result in misspecification or omitted variable bias, which will bias the indices [@Jiang2014]. Although omitted variables are a problem in every model, relatively detailed data is available for art, which should capture a large part of the variation in sales prices. Omitted variable bias should therefore be less of a problem than for other unique assets like real estate, and the omitted variable bias is often small in practice [@Triplett2004; @Renneboog2012].

###Repeat Sales Model
The repeat sales method is an alternative estimation method for quality-adjusted price indices, based on price changes of items sold more than once. It was initially proposed by @Bailey1963a to calculate house price changes. It was subsequently extended by @Case1987 and is currently used to produce the S&P/Case-Shiller Home Price Indices in the US. The repeat sales method tracks the sale of the same item over time. It aggregates sales pairs and estimates the average return on the set of items in each period [@Kraussl2010]. As a result, it does not require the measurement of quality, only that the quality of each item be constant over time [@Case1987]. 

The repeat sales model can be derived from the hedonic model, if the hedonic model is differenced with respect to consecutive sales of items that have sold more than once in the sample period [@McMillen2012]. In the standard repeat sales model the dependent variable is regressed on a set of dummy variables corresponding to time periods. This estimating equation provides unbiased estimates of pure time effects without having to correctly specify the item attributes or the functional form of the hedonic equation [@Deng2012]. By differencing the hedonic equation it also potentially controls for omitted variables. It also has the advantage of not being data intensive, as the only information required to estimate the index is the price, the sales date and a unique identifier (e.g. the address of the property). 

A few studies have utilised the repeat sales method to estimate art price indices. These studies have typically relied on very large sales databases, due to the infrequency of repeat sales of individual artworks. @Mei2002 constructed the seminal repeat sales index of art prices for the period 1875-2000. Their methodology is currently used to produce the Mei Moses Art Index for Beautiful Asset Advisors. Other examples include @Korteweg2013 and @Goetzmann2011, who used a database of over a million sales dating back to the 18th century. 

A disadvantage of the repeat sales method is the possibility of sample selection bias. Items that have traded more than once may not be representative of the entire population of items. For example, if cheaper artworks sell more frequently than expensive artworks, but high-quality artworks appreciate faster, a repeat sales index will tend to have a downward bias [@Eurostat2013]. The biggest problem with the repeat sales method in the current context is that single-sale data is discarded. This is problematic because the resale of a specific artwork may only occur infrequently, which substantially reduces the total number of observations available. Only 515 true repeat sales pairs could be identified in the sample, which limits the usefulness of the classical repeated sales approach in this case.

###Hybrid Models
An interesting perspective is to view the repeat sales specification as an extreme solution to a matching problem. The repeat sales approach requires an exact match to estimate the index. The idea behind the imperfect matching method proposed by @McMillen2012 is that some items may be similar enough to control for many of the differences in (observable and unobservable) attributes. The objective is to match sales observations over time, according to some criterion, so as to cancel out as many as possible of the differences in attributes. This involves a trade-off between the within-pair "similarity" and the sample size [@Guo2014].

This paper applies a simple hybrid repeat sales model to art prices for the first time. This procedure is similar in spirit to the "pseudo repeat sales" (ps-RS) procedure suggested by @Guo2014. The first ps-RS sample is created by matching artworks all the hedonic attributes, except the title of the artwork. Matching by this criteria increases the number of repeat sales pairs to 6,642, which includes the 515 true repeat sales or exact matches. The second ps-RS sample allows the sample to increase further by matching on all the hedonic attributes except the title and the presence of a signature and date on the artwork, i.e. the authenticity dummies. This increases the pseudo repeat sales sample to 7,965 sales pairs. 

The differential hedonic equation is then applied to the pseudo repeat sales samples, where artwork $i$ in quarter $t$ and artwork $h$ in quarter $s$ form a matched pair $(t>s)$:
$$\ln P_{it} - \ln P_{hs} = \sum_{j=1}^J \beta_j (X_{itj} - X_{hsj}) + \sum_{t=0}^T \delta_t G_{it} + \epsilon_{iths}$$
where $G_{it}$ is again a time dummy equal to 1 if the later sale occurred in quarter $t$, -1 if the former sale in the pair occurred in quarter $s$, and 0 otherwise; and $\epsilon_{iths}$ again represents a white noise residual.

For the first ps-RS sample, the only remaining independent variable is the difference in the auction house dummies $(X_{it1} - X_{hs1})$. This takes account of possible differences in quality and commission structures. In the second ps-RS sample the independent variables represent the differences in the auction house dummies and the differences in the two authenticity dummies. The independent variables therefore include indicators of the relatively small and easy to measure within-pair differentials in attributes between the two items.

The ps-RS approach mitigates the problem of potential omitted variable bias with the hedonic method. Taking first differences between similar items will control for omitted variables when they are the same for the two items that form the pseudo sales pairs. For example, if Van Gogh's *Sunflowers* paintings are treated as repeat sales, taking first differences would control for attributes such as theme, style, material, prominence, and the stage of the artist's career. Other potentially significant variables might include an array of interaction and non-linear terms. The ps-RS approach also mitigates the problems of small sample sizes and sample selection bias with repeat sales methods by using more of the transaction data [@McMillen2012]. @Calomiris2016 used a similar procedure, based on the differential hedonic equation, in analysing slave price indices. They argued that the similarities between the indices provided confidence that temporal variation in unobservable characteristics were not dictating the results.

There is no consensus regarding the preferred approach of constructing quality-adjusted price indices, either theoretically or empirically. The specific methodology adopted is dependent on the data available. Indices estimated with the different methodologies may provide different results for the bubble detection tests. The danger is that the biases inherent in each methodology may be driving the results (e.g. the omitted variable bias may be correlated with the cycle).

##Bubble Detection Framework
The adverse effects of bubbles and their related crises have led to a large literature on financial crises and the detection of bubbles in asset prices, including the seminal work by @Kindleberger2005 and the modelling approach by @Phillips2011. 

The most commonly used detection methods are based on the present value model and the rational bubble assumption. According to the present value model, under rational expectations, the price of an asset is equal to the present value of its future income stream, i.e. the expected fundamental value: $$P_t = \frac{1}{1+r_f} E_t(P_{t+1} + \gamma_{t+1})$$ where $r_f$ is the constant discount rate, $P_t$ is the asset price at time $t$, and $\gamma_{t+1}$ is the payment received (e.g. dividends, rents or a convenience yield) for owning the asset between $t$ and $t+1$. When $t+n$ is far into the future, $\frac{1}{1+r_f} E_t(P_{t+n})$ does not affect $P_t$, as it tends to zero as $n$ becomes infinitely large. The present value or market fundamental solution could be written as: $$F_t = E_t[\sum_{i=1}^n \frac{1}{1+r_f} (\gamma_{t+n})]$$ 

If a gap between the market fundamental solution and the actual price exists and the terminal condition does not hold, an additional "bubble component", $B_t$, has to be added to the solution of equation: $P_t = F_t + B_t$ [@Yiu2013]. In this case $F_t$ is called the fundamental component of the price and $B_t$ is any random variable that satisfies the following condition: $$B_t = \frac{1}{1+r_f} E_t(B_{t+n})$$ 

The statistical properties of $P_t$ are determined by those of $F_t$ and $B_t$. In the absence of a bubble, when $B_t=0$, the degree of non-stationarity in $P_t$ is controlled by the nature of the series $F_t$, which in turn is determined by the properties of $\gamma_t$. The current price of the commodity is therefore determined by market fundamentals: for example, if $\gamma_t$ is an I(1) process then $P_t$ would be an I(1) process. 

When a bubble is present, if $B_t \neq 0$, current prices $P_t$ will exhibit explosive behaviour, as $B_t$ reflects a stochastic process in which the expected value of next period's value, forecast on the basis of the current period's information, is greater than or equal to the current period's value [@Kraussl2016]. In the absence a structural change in the fundamental process or explosiveness in the fundamentals, a period of explosive prices must have a non-fundamental explanation. Under the assumed properties of $\gamma_t$, the observation of mildly explosive behaviour in $P_t$ (i.e. non-stationarity of an order greater than unit root non-stationarity) will offer evidence of bubble behaviour. This expression embodies an explosive property and introduces "bubble" movements in the price over the fundamental component [@Areal2013]. Thus, the theory predicts that if a bubble exists, prices should inherit its explosiveness property. This enables the formulation of statistical tests that try to detect evidence of explosiveness in the data [@Caspi2013].

Given the different stochastic properties of the fundamental and bubble components, early tests were based on unit root and cointegration tests. @Campbell1987 suggested a unit root test for explosiveness in prices, based on the idea that the gap between the asset price and the fundamental value will exhibit explosive behaviour during the process of bubble formation. @Diba1988 showed that if fundamental values are not explosive, the explosive behaviour in prices is a sufficient condition for the presence of bubble. 

However, unit root and cointegration tests are not capable of detecting explosive prices when a series contains periodically collapsing bubbles. @Evans1991 showed that these tests could not differentiate between a periodically collapsing bubble and a stationary process. A series containing periodically collapsing bubbles could therefore be interpreted by the standard unit root tests as a stationary series, leading to the incorrect conclusion that the data contained no explosive behaviour [@Phillips2011]. 

A number of methods have been proposed to deal with this critique [@Yiu2013]. The recursive tests proposed by @Phillips2011 and @Phillips2012 are not subject to this criticism and can effectively distinguish unit root processes from periodically collapsing bubbles, as well as date-stamp their origin and collapse. The tests proposed by @Phillips2011 are based on the idea of repeatedly implementing a right-tailed unit root test. The method involves the estimation of an autoregressive model, starting with a minimum fraction of the sample and incrementally expanding the sample forward. 

The model typically takes the following form:
$$\Delta y_t = \alpha_w + (\delta_w - 1) y_{t-1} + \sum_{i=1}^k \phi_w^i \Delta y_{t-i} + \epsilon_t$$
where $y_t$ is the asset price series, $\alpha$, $\delta$ and $\phi$ are the parameters to be estimated, $w$ is the sample window size, $k$ is the lag order, and $\epsilon_t$ is the white noise error term. 

A sample of Augmented Dickey-Fuller test statistics are calculated from each regression. The null hypothesis of a unit root $(\delta = 1)$ is tested against the right-tailed alternative of mildly explosive behaviour $(\delta > 1)$. The supremum value of the ADF sequence is then used to test for mildly explosive behaviour. By looking directly for evidence of explosive behaviour, the test avoids the risk of misinterpreting a rejection of the null hypothesis due to stationary behaviour. 

The method also allows for date-stamping of the origination and termination dates by matching the time series of the recursive test statistics to the critical value sequence. In other words, each element of the estimated ADF sequence is compared to the corresponding right-tailed critical values of the ADF statistic to identify a bubble period. The estimated origination point of a bubble is the first observation in which ADF value crosses the corresponding critical value (from below), while the estimated termination point is the first observation thereafter when the ADF value crosses below the critical value [@Caspi2013]. Simulations by @Homm2012 indicated that the procedure worked satisfactorily against other time series tests for the detection of bubbles and was particularly effective for real-time bubble detection.

Various studies have used the method to investigate bubbles in a number of asset markets, including real estate [@Jiang2014; @Balcilar2015], commodities [@Areal2013; @Figuerola2015] and art [@Kraussl2016]. The results from these studies have often suggested the existence of periods of explosive prices between 2006/07 and 2008. 

#Results 
##Art Price Indices
```{r naive, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##----------------------##
##---CENTRAL TENDENCY---##
##----------------------##
naive_index <- aggregate(artdata$price, by=list(artdata$timedummy), FUN=median, na.rm=TRUE)
naive_index$index <- naive_index$x
naive_index$index <- naive_index$index/naive_index[1,2]*100
naive_index$index <- as.numeric(naive_index$index)
colnames(naive_index) <- c("Date","Median","Median_Index")

#------------------------------
#Stratify by artist and medium
#get quantity and median price per group per quarter    
strat_p <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=median)
strat_q <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=sum)
strat_q$x <- strat_q$x/strat_p$x        #the count q

chain2 <- function(strat_p, strat_q, kwartaal1,kwartaal2) {
    strat_p1 <- subset(strat_p, strat_p$Group.1==kwartaal1)
    strat_q1 <- subset(strat_q, strat_q$Group.1==kwartaal1)
    strat_p2 <- subset(strat_p, strat_p$Group.1==kwartaal2)
    strat_q2 <- subset(strat_q, strat_q$Group.1==kwartaal2)
    #get sample of median prices and quantities for specific artist for the two quarters
    strat_pc <- merge(strat_p1, strat_p2, by=c("Group.2","Group.3"))
    strat_qc <- merge(strat_q1, strat_q2, by=c("Group.2","Group.3"))
    #Laspeyres (keeps quantity weights fixed at base)
    Lasp <- sum(strat_pc$x.y*strat_qc$x.x,na.rm=TRUE)/sum(strat_pc$x.x*strat_qc$x.x,na.rm=TRUE)
    #Paasche (keeps quantity weights fixed at end)
    Paas <- sum(strat_pc$x.y*strat_qc$x.y,na.rm=TRUE)/sum(strat_pc$x.x*strat_qc$x.y,na.rm=TRUE)
    return(c(Lasp,Paas))
}

datum <- levels(artdata$timedummy)
ketting2 <- chain2(strat_p,strat_q,datum[1],datum[2])
ketting2 <- rbind(ketting2,chain2(strat_p,strat_q,datum[2],datum[3]))
for(i in 3:63) {
    ketting2 <- rbind(ketting2,chain2(strat_p,strat_q,datum[i],datum[(i+1)]))
}
ketting2 <- as.data.frame(ketting2)
ketting2$V3 <- sqrt(ketting2[,1]*ketting2[,2])  #Fisher index is the geometric mean
ketting2$V4[1] <- ketting2$V3[1]*100
for(i in 2:63) {                                #use the growth rates to generate the index
    ketting2$V4[i] <- ketting2$V4[(i-1)]*ketting2$V3[i]
}
ketting2$Date <- as.factor(datum[-1])
colnames(ketting2) <- c("Las","Paas","Fisher","Fisher_Index","Date")

naive_index <- merge(naive_index, ketting2, by.x="Date", by.y="Date",all=TRUE)
naive_index[1,4:7] <- 100
naive_indices <- naive_index[,c(1,3,7)]
```

```{r hedonicrep, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#-------------------
# FULL SAMPLE MODEL
#-------------------
full_model <- function(data, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                 "nr_works","artist","timedummy")) {
    modeldata <- data
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    model_all <- lm(expl_vars, data=modeldata)
    time_results <- summary(model_all)$coefficients[grepl("timedummy", rownames(summary(model_all)$coefficients)),1]
    time_results <- as.data.frame(time_results)
    time_results$Index <- exp(time_results$time_results)*100
    time_results$Date <- sub("timedummy","",row.names(time_results))
    time_results <- merge(datums,time_results,by="Date",all=TRUE)[,c(1,4)]
    return(time_results)
}

#-----------------------------
# OVERLAPPING PERIODS (1-year)
#-----------------------------
overlap1y_model <- function(data, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                      "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:16) {
        modeldata <- data
        modeldata <- subset(modeldata, modeldata$counter>(i*4-5)& modeldata$counter<(i*4+1))
        model <- lm(expl_vars, data=modeldata)  
        time_results <- as.data.frame(summary(model)$coefficients[grepl("timedummy", rownames(summary(model)$coefficients)),1])
        time_results$Date <- sub("timedummy","",row.names(time_results))
        colnames(time_results) <- c("Coef","Date")
        res_list[[i]] <- time_results
    }
    #Merge all results
    overlap <- rep_results
    overlap <- merge(overlap, res_list[[1]], by="Date", all=TRUE)
    overlap[,3] <- exp(overlap[,3])*100
    for(i in 2:16) {
        overlap <- merge(overlap, res_list[[i]], by = "Date",all=TRUE)
        overlap[,(i+2)] <- exp(overlap[i+2])*100
    } 
    #Calculate index
    overlap$ind <- overlap[,3]
    overlap[2,19] <- overlap[3,19]*overlap[2,2]/overlap[3,2]   #Interpolate
    overlap$teller <- as.numeric(substring(overlap$Date,1,4))-1997
        
    for(i in 3:62) {
        j <- overlap[(i+1),20]
        if(is.na(overlap[i,j])) {
            overlap[(i+1),19] <- overlap[i,19]*overlap[(i+1),j]/100
        } else { 
            overlap[(i+1),19] <- overlap[i,19]*overlap[(i+1),j]/overlap[i,j] 
        }   
    }
    colnames(overlap) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5","Index_m6",
                           "Index_m7","Index_m8","Index_m9","Index_m10","Index_m11","Index_m12","Index_m13",
                           "Index_m14","Index_m15","Index_m16","Index_Adjacent1y","teller")
    overlap$Date <- factor(levels(artdata$timedummy)[-1])
    return(overlap)
}

#-----------------------------
# OVERLAPPING PERIODS (2-year)
#-----------------------------
overlap2y_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                      "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:8) {
        modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*8-9)& modeldata$counter<(i*8+1))
        model <- lm(expl_vars, data=modeldata)  
        time_results <- as.data.frame(summary(model)$coefficients[grepl("timedummy", rownames(summary(model)$coefficients)),1])
        time_results$Date <- sub("timedummy","",row.names(time_results))
        colnames(time_results) <- c("Coef","Date")
        res_list[[i]] <- time_results
    }
    #Merge all results
    overlap2 <- rep_results
    overlap2 <- merge(overlap2, res_list[[1]], by="Date", all=TRUE)
    overlap2[,3] <- exp(overlap2[,3])*100
    for(i in 2:8) {
        overlap2 <- merge(overlap2, res_list[[i]], by = "Date", all=TRUE)
        overlap2[,(i+2)] <- exp(overlap2[i+2])*100
    } 
    #Calculate index
    overlap2$ind <- overlap2[,3]
    overlap2[2,11] <- overlap2[3,11]*overlap2[2,2]/overlap2[3,2]   #Interpolate
    overlap2$teller <- c(3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,
                         7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10)
    for(i in 7:62) {
        j <- overlap2[(i+1),12]
        if(is.na(overlap2[i,j])) {
            overlap2[(i+1),11] <- overlap2[i,11]*overlap2[(i+1),j]/100
        } else { 
            overlap2[(i+1),11] <- overlap2[i,11]*overlap2[(i+1),j]/overlap2[i,j] 
        }   
    }
    colnames(overlap2) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5",
                            "Index_m6","Index_m7","Index_m8","Index_Adj2y","teller")
    overlap2$Date <- factor(levels(artdata$timedummy)[-1])
    return(overlap2)
}

#----------------------
#ROLLING 5-YEAR WINDOWS
#----------------------
rolling_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                    "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:12) {
        modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*4-4)&modeldata$counter<(i*4+17))
        model <- lm(expl_vars, data=modeldata)  
        time_results <- as.data.frame(summary(model)$coefficients[grepl("timedummy", rownames(summary(model)$coefficients)),1])
        time_results$Date <- sub("timedummy","",row.names(time_results))
        colnames(time_results) <- c("Coef","Date")
        res_list[[i]] <- time_results    
    }
    
    #Merge all results
    rolling <- rep_results
    rolling <- merge(rolling, res_list[[1]], by="Date", all=TRUE)
    rolling[,3] <- exp(rolling[,3])*100
    for(i in 2:12) {
        rolling <- merge(rolling, res_list[[i]], by = "Date", all=TRUE)
        rolling[,(i+2)] <- exp(rolling[i+2])*100
    }    
    
    #Calculate index
    rolling$ind <- rolling[,3]
    rolling[2,15] <- rolling[3,15]*rolling[2,2]/rolling[3,2]  #interpolate
    rolling$teller <- c(3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,
                        10,10,10,10,11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14)
    for(i in 19:62) {  #chaining
        j <- rolling[(i+1),16]
        rolling[(i+1),15] <- rolling[i,15]*rolling[(i+1),j]/rolling[i,j]
    }
    
    colnames(rolling) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5","Index_m6",
                           "Index_m7","Index_m8","Index_m9","Index_m10","Index_m11","Index_m12","Index_Rolling","teller")
    rolling$Date <- factor(levels(artdata$timedummy)[-1])
    return(rolling)
}

#========================================================================================
list_expl_vars <- c("lnarea","ah_code","med_code","dum_signed","dum_dated",  
                    "nr_works","lnrep","lnarea:med_code","timedummy")
rep_results <- full_model(artdata,list_expl_vars)
suppressMessages(rep_overlap1 <- overlap1y_model(artdata,list_expl_vars))
suppressMessages(rep_overlap2 <- overlap2y_model(artdata,list_expl_vars))
suppressMessages(rep_rolling <- rolling_model(artdata,list_expl_vars))

#----------------------------------------------
hedonic_indices <- rep_overlap1[,c(1,2)]
colnames(hedonic_indices) <- c("Date","Hedonic_full")
hedonic_indices <- cbind(hedonic_indices,Adjacent_1y=rep_overlap1[,19])
hedonic_indices <- cbind(hedonic_indices,Adjacent_2y=rep_overlap2[,11])
hedonic_indices <- cbind(hedonic_indices,Rolling_5y=rep_rolling[,15])
hedonic_indices <- cbind(Date=factor(levels(artdata$timedummy)),
                         rbind(c(seq(100,100, length.out=4)),hedonic_indices[,-1]))
```

```{r repeatsales, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##=====================##
## REPEAT SALES METHOD ##
##=====================##
#check for duplicates (how many)
sum(duplicated(artdata[,c("artist","title","med_code","area","dum_signed","dum_dated","nr_works")]))
allDup <- function(value) {  #identify duplicated values
    duplicated(value) | duplicated(value, fromLast = TRUE)
}
rsartdata <- artdata[allDup(artdata[,c("artist","title","med_code","area","dum_signed","dum_dated","nr_works")]),]
rsartdata <- transform(rsartdata, id = as.numeric(interaction(artist,factor(title),med_code,factor(area),factor(dum_signed),
                                                              factor(dum_dated), factor(nr_works),drop=TRUE)))

repdata <- repsaledata(rsartdata$lnprice,rsartdata$counter,rsartdata$id)  #transform the data to sales pairs
repdata <- repdata[complete.cases(repdata),]
repeatsales <- repsale(repdata$price0,repdata$time0,repdata$price1,repdata$time1,mergefirst=1,
                       graph=FALSE)   #generate the repeat sales index
RS_index <- exp(as.data.frame(repeatsales$pindex))*100   

n <- as.numeric(sub("Time ","",row.names(RS_index)))
n[1] <- 1
RS_index$Date <- levels(rsartdata$timedummy)[unique(c(repdata$time0,repdata$time1))[order(unique(c(repdata$time0,repdata$time1)))]][n] #missing values

RS_index <- merge(datums,RS_index, by="Date", all=TRUE)[,-2]
colnames(RS_index) <- c("Date","Repeat Sales_Index")            
```

```{r psRSsample, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##------------------------------------------------------------------
##--------------- Pseudo Repeat Sales ------------------------------
##------------------------------------------------------------------
#do the same but expand it to not match by title - i.e. all other attributes are the same
sum(duplicated(artdata[,c("artist","med_code","area","dum_signed","dum_dated","nr_works")]))
rsartdata1 <- artdata[allDup(artdata[,c("artist","med_code","area","dum_signed","dum_dated","nr_works")]),]
rsartdata1 <- transform(rsartdata1, id = as.numeric(interaction(artist,med_code,factor(area),factor(dum_signed),
                                                                factor(dum_dated),factor(nr_works), drop=TRUE)))

repdata1 <- cbind(repsaledata(rsartdata1$lnprice,rsartdata1$counter,rsartdata1$id),
                  repsaledata(rsartdata1$ah_code,rsartdata1$counter,rsartdata1$id)[,4:5]) #transform the data to sales pairs
repdata1 <- repdata1[complete.cases(repdata1),]
colnames(repdata1) <- c("id","time0","time1","price0","price1","ah_code0","ah_code1")

dy <- repdata1$price1 - repdata1$price0
ah0 <- model.matrix(~repdata1$ah_code0)
ah1 <- model.matrix(~repdata1$ah_code1)
dah <- ah1 - ah0

timevar <- levels(factor(c(repdata1$time0, repdata1$time1)))
nt = length(timevar)
n = length(dy)
xmat <- array(0, dim = c(n, nt - 1))
for (j in seq(1 + 1, nt)) {
    xmat[,j-1] <- ifelse(repdata1$time1 == timevar[j], 1, xmat[,j-1])
    xmat[,j-1] <- ifelse(repdata1$time0 == timevar[j],-1, xmat[,j-1])
}
colnames(xmat) <- paste("Time", seq(1 + 1, nt))

ps.RS <- lm(dy ~ dah + xmat + 0)
RS_index1 <- summary(ps.RS)$coefficients[grepl("Time", rownames(summary(ps.RS)$coefficients)),1]
RS_index1 <- as.data.frame(RS_index1)
RS_index1$index <- exp(RS_index1$RS_index1)*100
RS_index1$Date <- levels(rsartdata1$timedummy)[-1]
RS_index1 <- RS_index1[,c(2,3)]

##=====================##
#do the same but expand it to not match by title or authenticity dummies
sum(duplicated(artdata[,c("artist","med_code","area","nr_works")]))
rsartdata2 <- artdata[allDup(artdata[,c("artist","med_code","area","nr_works")]),]
rsartdata2 <- transform(rsartdata2, id = as.numeric(interaction(artist,med_code,factor(area),factor(nr_works), drop=TRUE)))

repdata2 <- cbind(repsaledata(rsartdata2$lnprice,rsartdata2$counter,rsartdata2$id),
                  repsaledata(rsartdata2$ah_code,rsartdata2$counter,rsartdata2$id)[,4:5],
                  repsaledata(rsartdata2$dum_signed,rsartdata2$counter,rsartdata2$id)[,4:5],
                  repsaledata(rsartdata2$dum_dated,rsartdata2$counter,rsartdata2$id)[,4:5])
repdata2 <- repdata2[complete.cases(repdata2),]
colnames(repdata2) <- c("id","time0","time1","price0","price1","ah_code0","ah_code1","sign0","sign1","date0","date1")

dy <- repdata2$price1 - repdata2$price0
dsign <- repdata2$sign1 - repdata2$sign0
ddate <- repdata2$date1 - repdata2$date0
ah0 <- model.matrix(~repdata2$ah_code0)
ah1 <- model.matrix(~repdata2$ah_code1)
dah <- ah1 - ah0

timevar <- levels(factor(c(repdata2$time0, repdata2$time1)))
nt = length(timevar)
n = length(dy)
xmat <- array(0, dim = c(n, nt - 1))
for (j in seq(1 + 1, nt)) {
    xmat[,j-1] <- ifelse(repdata2$time1 == timevar[j], 1, xmat[,j-1])
    xmat[,j-1] <- ifelse(repdata2$time0 == timevar[j],-1, xmat[,j-1])
}
colnames(xmat) <- paste("Time", seq(1 + 1, nt))

ps.RS <- lm(dy ~ dah + dsign + ddate + xmat + 0)
RS_index2 <- summary(ps.RS)$coefficients[grepl("Time", rownames(summary(ps.RS)$coefficients)),1]
RS_index2 <- as.data.frame(RS_index2)
RS_index2$index <- exp(RS_index2$RS_index2)*100
RS_index2$Date <- levels(rsartdata2$timedummy)[-1]
RS_index2 <- RS_index2[,c(2,3)]

#------------------------------------------------------------------------
RS_indices <- merge(RS_index, RS_index1, by="Date", all=TRUE)
RS_indices <- merge(RS_indices, RS_index2, by="Date", all=TRUE)
RS_indices <- cbind(Date=factor(levels(artdata$timedummy)),
                         rbind(c(seq(100,100, length.out=3)),RS_indices[,-1]))
colnames(RS_indices) <- c("Date","Repeat Sales","pseudo-RS1","pseudo-RS2")            
```

Figure 1 illustrates representative indices for the three methodologies: median index, the 1-year adjacent period hedonic index and the second version (larger sample) of the ps-RS index. The two regression-based indices seem to point to a similar general trend in South African art prices. The simple median index, on the other hand, does not reflect this trend and is much more volatile than the regression-based indices. Table 1 reports the correlations in the growth rates between the various indices. There is a significant positive correlation between the regression-based indices. 

```{r figure10, echo=FALSE, cache = TRUE, warning=FALSE, fig.height=4, fig.width=7.5, fig.cap="Comparing South African art price indices (2000Q1=100)"}
all_indices <- cbind(naive_indices,hedonic_indices[-1],RS_indices[-1])
all_indices[is.na(all_indices)]<- 100
index_plot <- melt(all_indices[,c(1,2,5,10)], id="Date")  
index_plot$Date <- as.Date(as.yearqtr(index_plot$Date, format = "%Y Q%q"))
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line(aes(linetype=variable))
g <- g + scale_linetype_manual(values = c(4,1,2))
g <- g + ylab("Index") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g
```

```{r table2, echo=FALSE, results='asis', message=FALSE, warning=FALSE, cache = TRUE}
# Check correlations (in growth rates)
source("corstarsl.R")
temp_indices <- all_indices[-1:-3,]
colnames(temp_indices) <- c("Date","Median","Fisher","Hedonic","Adj1y","Adj2y","Roll","RepSale","ps.RS1","ps.RS2")
for(i in 2:ncol(temp_indices)) {temp_indices[,i] <- as.numeric(temp_indices[,i]) }
ts.all_indices <- as.ts(temp_indices[,-1],start =c(2000,4),end=c(2015,4),frequency=4) 
returns <- as.data.frame(diff(log(ts.all_indices)))
xt <- xtable(corstarsl(returns), caption="Correlations in growth rates (dlogs)")
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9)
```

All of the regression-based indices peak in 2008Q1, which is before the peak in sales and annual median prices in the sample. All of the measures indicate that quality-adjusted art prices increased significantly between 2005 and 2008 and then declined relatively sharply after the financial crisis, similar to other asset prices [@Shimizu2010]. This conforms to the idea that there was a surge in the popularity of South African art over the period, as well as the idea of the formation of a bubble, with a dramatic rise and subsequent decrease in prices. 

The hedonic and ps-RS indices exhibit similar trends and high correlations over the sample period, even though the hybrid repeat sales indices are based on smaller subsamples of the data. This shows that there is some consistency in the estimates from the different methodologies. The ps-RS method provides a type of robustness check on the hedonic indices. This provides some confidence that the indices provide a relatively accurate measure of the price movements in the South African art market and that the results are robust to changes in methodology. It implies that the potential omitted variable and sample selection bias may not be too pervasive in this case and are not driving the dramatic price increases [@Calomiris2016]. 

###Evaluation
This section evaluates the different art price indices in terms smoothness, to determine which index provides the most credible gauge of overall price movements in this case. In other applications, the quality of price indices has often been evaluated based on the diagnostic metrics of the underlying regressions, such as the standard errors of the residuals (see e.g. @Hansen2009 for real estate indices). 

However, @Guo2014 argued that the regression residuals do not represent errors in the price index, and hence do not directly reflect inaccuracy in the index returns. Even if an index is perfectly accurate, measuring the central tendency of market price changes in each period, the regression would still have residuals and the time dummy coefficients might still have large standard errors, resulting simply from the dispersion of individual art prices around the central tendency. When datasets become large, the regression diagnostics are often good simply because of the size of the sample. In this case not all of the indices are generated with regression models, and the regressions models that are employed differ in their specifications (in levels or first differences) and the underlying data used for estimation. 

@Guo2014 suggested that signal-to-noise metrics, based directly on the index produced, are a more appropriate for judging the quality of the price index, as opposed to the underlying model. Random error in the coefficient estimation leads to "noise" in the index. Signal-to-noise metrics directly reflect the accuracy of the index returns and the economic significance of random error in the indices. The volatility (Vol) and the first order autocorrelations (AC(1)) of the index returns are signal-to-noise metrics that may be useful to compare the amount of noise in the indices. 

The ideal price index filters out the noise-induced volatility to leave only the true market volatility. In addition to the true volatility, the noise (random error) in the index causes excess volatility in the index returns. Excess volatility decreases the first order autocorrelation in index returns. Other things being equal, the lower the volatility and the higher the AC(1), the less noisy and more accurate the index. Thus, lower Vol or higher AC(1) will indicate a better quality art price index in the sense of less noise.

@Guo2014 suggest another test of index quality in terms of minimising random error that is based on the Hodrick-Prescott (HP) filter. The HP filter is a spline fitting procedure that divides a time series into smoothed trend and cyclical components. The idea is to examine which index has the least deviation from its smoothed HP component, by comparing the sum of squared differences between the index returns and the smoothed returns. 

Another option is to compare the smoothness coefficients proposed by @Froeb1994. The smoothness coefficient is defined as the average long run variance of a time series divided by the average short run variance. The idea is to obtain the spectral density of the time series, which shows the contribution of all frequencies to the data series. The smoothness measure is then taken as the average of the lower half of the frequency range (i.e. low frequency movements) over the average of the upper half of the frequencies (i.e. higher frequencies). A higher smoothness coefficient indicates a larger portion of variance in the low frequencies and a smoother time series. 

Table 2 reports these four metrics of index smoothness for the art price indices. The 1-year adjacent period hedonic index performs the best in terms of these metrics, with the lowest volatility and highest AC(1) in returns, the smallest deviation from its smoothed returns, and the highest smoothness coefficient. However, the smoothness coefficients of the regression-based indices are not significantly different.

```{r table3, echo=FALSE, results='asis', warning=FALSE, message=FALSE, cache = TRUE}
##Comparing index smoothness
# Check std dev or volatility en AC(1)
ac.1 <- numeric()
eval <- data.frame()
HPdev <- numeric()
smoothness <- numeric()

vol <- apply(returns,MARGIN=2, FUN=sd, na.rm=TRUE)
for(i in 1:ncol(returns)) {
    ac.1[i] <- acf(returns,na.action = na.pass, plot = FALSE, lag.max = 1)$acf[,,i][2,i]
}

hp <- temp_indices
for(i in 2:10) {
    hp[,i] <- hpfilter(temp_indices[,i],freq = 1600)[2]
}
ts.hp <- as.ts(hp[,-1],start =c(2000,1),end=c(2015,4),frequency=4)
hpreturns <- as.data.frame(diff(log(ts.hp)))
for(i in 1:ncol(returns)) {
    HPdev[i] <- sum((hpreturns[,i] - returns[,i])^2)
}

#spectral density
smooth <- function (datavec,k,l) {     # calculates smoothness coefficient for 'datavec' with 
    # 'k' specifies the width of the Daniell window which smooths the raw periodogram 
    ## Step 1: Calculate and record power spectral density using 'speccalcs'
    speccalcs <- spec.pgram(datavec,spans=c(k,l),demean=TRUE,plot=FALSE)
    spectra <- speccalcs$spec
    
    ## Step 2 Take natural logs of power spectral frequencies
    logspec <- log(spectra)
    n <- length(logspec)
    m <- n/2
    p1 <- mean(logspec[1:m])
    p2 <- mean(logspec[(m+1):n])
    smcoef <- p1-p2
    smcoefvar <- (pi^2)/6*((1/m)+(1/(n-m)))
    smcoefse <- sqrt(smcoefvar)
    #list(smcoef,smcoefse)
    return(smcoef)
}
for(i in 1:10) { smoothness[i] <- smooth(all_indices[,i],3,3) }

eval <- cbind(Vol=vol,AC.1=ac.1[1:9],HPDeviation=HPdev,Smoothness=smoothness[-1])
xt <- xtable(eval, caption="Smoothness Indicators", digits=c(0,3,3,2,2))
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9) 
```

##Bubble Detection Results
This section tests whether the South African art market has exhibited bubble-like behaviour over the sample period, focusing on a specific aspect of bubbles: explosive prices. This section follows the convention of using the log value of real asset prices, deflated with the CPI (e.g. @Kraussl2016, @Caspi2013 and @Balcilar2015). In this case there is only one potential bubble episode, so the @Phillips2011 method should be sufficient to provide consistent evidence of mildly explosive behaviour.

The model starts with 3 years and expands the sample by one observation each time. In this case, there does not seem to be a deterministic drift present in the log real art price indices and the intercept is not statistically significant at conventional levels. In their original study @Phillips2011 use a random walk without drift to estimate the null hypothesis. @Phillips2014 argue that an empirically more realistic description of explosive behaviour is given by models formulated without a drift or deterministic trend. Nevertheless, as a robustness check the models were formulated with and without a drift term included. Lags are included to take possible autocorrelation of the residuals into account and the number of lags $k$ is chosen with the Akaike Information Criterion. Critical values for the tests are derived from Monte Carlo simulations of a random walk process, both including and excluding a drift term, with 2000 replications.   

```{r bubbles, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#==============================#
# Bubbles: Explosive Behaviour
#==============================#
#Make them real
real_indices <- all_indices
for(i in 2:ncol(all_indices)) { 
    for(j in 1:64) {
        real_indices[j,i] <- all_indices[j,i]/assets$CPI[j]*100 
    }
}

#Calculate test statistics
y_indices <- log(real_indices[,-1])
bubble.nc <- list()
bubble.c <- list()
for(i in 1:ncol(y_indices)) {
    bubble1 <- numeric()
    bubble2 <- numeric()
    for(j in 12:64) {
        y <- y_indices[1:j,i]
        toets1 <- ur.df(y, type= "none", lags = 4, selectlags = c("AIC"))
        toets2 <- ur.df(y, type= "drift", lags = 4, selectlags = c("AIC"))
        bubble1 <- rbind(bubble1,toets1@teststat)
        bubble2 <- rbind(bubble2,toets2@teststat)
    }
    bubble.nc[[i]] <- bubble1
    bubble.c[[i]] <- bubble2
}

##--------------------------------------------------------------------------
#Calculate critical values
K1 <- numeric()
K2 <- numeric()
K3 <- numeric()
K4 <- numeric()

for(j in 12:64) {
    set.seed(123)                           #for replicability
    reps <- 2000                            #Monte Carlo replications
    burn <- 100                             #burn in periods: first generate a T+B sample
    #obs <- 62                              #To make "sure" that influence of initial values has faded
    obs <- j                                #ultimate sample size
    tstat.nc <- numeric()
    tstat.c <- numeric()
    tstat.ct <- numeric()
    tstat.lc <- numeric()
    
    for(i in 1:reps) {     
        e <- rnorm(obs+burn)
        e[1] <- 0
        Y1 <- cumsum(e)
        DY1 <- diff(Y1)
        
        y1 <- Y1[(burn+1):(obs+burn)]               #trim off burn period
        dy1 <- DY1[(burn+1):(obs+burn)]             
        ly1 <- Y1[burn:(obs+burn-1)] 
        trend <- 1:obs
        
        EQ1 <- lm(dy1 ~ 0 + ly1)       
        tstat.nc <- rbind(tstat.nc,summary(EQ1)$coefficients[1,3]) 
        EQ2 <- lm(dy1 ~ ly1)            
        tstat.c <- rbind(tstat.c,summary(EQ2)$coefficients[2,3])  
    }                                       
    #hist(tstat.nc)
    K1 <- rbind(K1,quantile(tstat.nc, probs=c(0.9,0.95,0.99)))
    K2 <- rbind(K2,quantile(tstat.c, probs=c(0.9,0.95,0.99)))
}   #Provides a vector of critical values

bubble.test1 <- numeric()
bubble.test2 <- numeric()

for(k in 1:9) { 
    bubble.test1 <- cbind(bubble.test1,bubble.nc[[k]])
    bubble.test2 <- cbind(bubble.test2,bubble.c[[k]][1:53])
}
bubble.test1 <- as.data.frame(bubble.test1)
bubble.test2 <- as.data.frame(bubble.test2)
bubble.test1 <- cbind(bubble.test1,K1)
bubble.test2 <- cbind(bubble.test2,K2)

Dates <- levels(artdata$timedummy)[-1:-11]
bubble.test1$Date <- Dates
bubble.test2$Date <- Dates

colnames(bubble.test1)[1:9] <- colnames(all_indices[-1])
colnames(bubble.test2)[1:9] <- colnames(all_indices[-1])
```

The supremum ADF test statistics from to both formulations are above the 95% critical values for each of the indices, except for the median index. Therefore, the null hypothesis of a unit root may be rejected in favour of the alternative hypothesis for each of the indices, except the median index. Figure 2 illustrates the date stamping procedure for three representative series (without drift): the median index, the 1-year adjacent period hedonic index and the second version (larger sample) of the ps-RS index. The figures compare the ADF test static sequence to the 95% and 99% critical value sequences. In both cases the test statistic sequences breach the 95% critical values in the run-up to the financial crisis (2005 and 2006 respectively), before falling below the critical values. The sequence of test statistics for the ps-RS index is higher than for the hedonic index, and breaches the 99% critical value.

```{r figure15, echo=FALSE, message=FALSE, warning=FALSE, cache = TRUE, fig.height=4, fig.width=7.5, fig.cap="Test statistics and critical values for models without drift"}
index_plot <- bubble.test1[,c(1,4,9,11,12,13)]
index_plot <- melt(index_plot, id="Date")  # convert to long format
index_plot$Date <- as.Date(as.yearqtr(index_plot$Date, format = "%Y Q%q"))
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 0.8) 
g <- g + geom_line(aes(linetype=variable))
g <- g + scale_linetype_manual(values = c(6,1,5,3,3))
g <- g + ylab("") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g
```

Table 3 reports the origination and termination dates for all of the indices, based on 95% critical values. The test statistic sequences for the hedonic indices all indicate a period of explosive prices beginning around 2006/2007 and ending in 2008. The test statistics for the ps-RS indices indicate periods of explosive behaviour that were slightly longer, beginning around 2005/2006 and ending in 2008 or even 2010, depending on the specification. The preferred index in terms of smoothness (i.e. the 1-year adjacent index) suggests a period of bubble formation from 2007Q1 to 2008Q3. @Phillips2012 recommend that only explosive periods lasting more than log(T) units of time should be identified as bubble periods. In this case this implies that the bubble should be at least 4 quarters in length and virtually all of the explosive periods identified satisfy this requirement.   

```{r dates, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#report  bubble period dates
datum1 <- data.frame()
datum2 <- data.frame()
datums1 <- data.frame()
datums2 <- data.frame()

bubble.test1 <- bubble.test1[,c(-1,-7)]
bubble.test2 <- bubble.test2[,c(-1,-7)]

for(i in 1:7) {
    for(l in 1:53) {
        if(bubble.test1[l,i]>bubble.test1$"95%"[l]) { 
            datum1[l,i] <- bubble.test1[l,"Date"]
        }
        if(bubble.test2[l,i]>bubble.test2$"95%"[l]) { 
            datum2[l,i] <- bubble.test2[l,"Date"]
        }
    }
    NonNAindex <- which(!is.na(datum1[,i]))
    firstNonNA <- min(NonNAindex)
    datums1[1,i] <- datum1[firstNonNA,i]
    if (NonNAindex[NROW(NonNAindex)-1]==(max(NonNAindex)-1)) { 
        lastNonNA <- max(NonNAindex)
    } else lastNonNA <- NonNAindex[NROW(NonNAindex)-1]
    datums1[2,i] <- datum1[lastNonNA,i]
    
    NonNAindex <- which(!is.na(datum2[,i]))
    firstNonNA <- min(NonNAindex)
    datums2[1,i] <- datum2[firstNonNA,i]
    lastNonNA <- max(NonNAindex)
    datums2[2,i] <- datum2[lastNonNA,i]
}    

datum <- rbind(datums1,datums2)
datum <- t(datum)
datum <- rbind(c("Start","End","Start","End"), datum)
rownames(datum) <- c(' ', colnames(bubble.test1)[1:7])
```

```{r table5, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
xt <- xtable(datum, caption="Dates of explosive behaviour")
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(paste0('& \\multicolumn{2}{c}{  No Drift  } & \\multicolumn{2}{c}{ Drift }', collapse=''), '\\\\')

print(xt, "latex",comment=FALSE, add.to.row=addtorow, include.colnames=F,
      caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9)
```

The dates identified correspond with many of the explosive periods identified in the literature for a range of assets. In the context of art, @Kraussl2016 identified bubble periods for the "*Post-war and Contemporary*" art segment between 2006 and 2008 and for the "*American*" art segments between 2005 and 2008, which also corresponds to the pre-financial crisis period. They found evidence of the formation of another bubble in these market segments around the start of 2011. This is not present in the South African art market, which has remained relatively flat since 2010. This is mirrored by South Africa's experience during the Great Recession, which was not as deep as in most developed countries, but was more protracted.

###Art Market Segments
```{r robust2, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
source("full_model.R")
list_expl_vars <- c("lnarea","ah_code","med_code","dum_signed","dum_dated",  
                    "nr_works","artist","med_code:lnarea","timedummy")
model4.1 <- full_model_a(artdata[artdata$price<=2400,],list_expl_vars) 
model4.2 <- full_model_a(artdata[artdata$price>2400 & artdata$price<22000,],list_expl_vars) 
model4.3 <- full_model_a(artdata[artdata$price>=22000,],list_expl_vars) 

pr_seg <- cbind(model4.1[,c(1,2)],model4.2[,2],model4.3[,2])
colnames(pr_seg) <- c("Date", "Lower", "Middle", "Upper")
pr_seg <- cbind(Date=factor(levels(artdata$timedummy)),
                rbind(c(seq(100,100, length.out=2)),pr_seg[,-1]))

model4.1 <- full_model_a(artdata[artdata$ave_price>=1693,],list_expl_vars) 
model4.2 <- full_model_a(artdata[artdata$ave_price>262 & artdata$ave_price<1693,],list_expl_vars) 
model4.3 <- full_model_a(artdata[artdata$ave_price<=262,],list_expl_vars) 

ave_seg <- cbind(model4.1,model4.2[,2],model4.3[,2])
colnames(ave_seg) <- c("Date", "Lower", "Middle", "Upper")
ave_seg <- cbind(Date=factor(levels(artdata$timedummy)),
                 rbind(c(seq(100,100, length.out=2)),ave_seg[,-1]))

#---------------------------------------------------
mediums <- read.csv("mediums.csv", header=TRUE, sep=",",na.strings = "NA", skipNul = TRUE)
colnames(mediums) <- c("Date","Drawing", "Watercolour", "Oil", "Acrylic", "Print/Woodcut","Mixed Media","Sculpture","Photography", 
           "Other")

#---------------------------------------------------
quant_results <- read.csv("quant_results.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)[,c(5,2,3,4)]
colnames(quant_results) <- c("Date","tau=0.25","tau=0.50","tau=0.75")
quant_results <- cbind(Date=factor(levels(artdata$timedummy)),
                       rbind(c(seq(100,100, length.out=2)),quant_results[,-1]))
```

Different segments of the South African art market exhibit different price trends over time. This section examines different segments of the market, in order to establish in which segments the marked price increases occurred. The caveat is that slicing the data thinly results in small sample sizes and more volatile indices. This makes it more difficult to discern a pattern and to distinguish the signal from the noise.  

The South African art market may be segmented for a number of reasons. Small investors are generally not able to invest in more expensive works, while wealthy individuals may be less tempted to buy at the lower end of the market, where works do not signal the same social status. The more expensive parts of the market may be more prone to speculation [@Renneboog2012]. @Fedderke2014 suggested that the South African art market should be segmented into three price ranges and found different hedonic relationships for the three market segments. 

Thus, historical rates of appreciation may have varied across the price distribution [@Renneboog2012]. In order to test this possibility, different part of the price distribution may be investigated separately by estimating hedonic indices for different parts of the distribution. Three indices are estimated for the bottom 25% of the price distribution ("Lower"), the interquartile range ("Middle"), and the upper 25% of the distribution ("Upper").[^20] The indices suggest that the dramatic price increases occurred in the upper part of the price distribution, which includes artworks with a hammer price of more than R22,000.

[^20]: The models are estimated with the full sample hedonic method. The models include dummy variables for all the artists that sold more than one artwork during the sample period. The adjacent-period hedonic and ps-RS models are used to confirm the results. In many cases, however, there are too few observations to estimate full indices.

The market may also be segmented by artist value. In order to examine the price appreciation of the more expensive artists' work, the artists are ranked according to the average value of their artworks sold in the sample (i.e. average price per artwork). Separate hedonic regression were estimated for the bottom 25% of the value distribution (i.e. for all artists in the lower part of the value distribution), the interquartile range, and the upper 25% of the distribution. Price increased more dramatically for artworks by the more expensive artists, which in this case includes the top 262 artists in terms of average price per artwork.

Another potential segmentation is by medium category. It is possible that historical rates of appreciation have varied widely over time for specific media. Separate hedonic models may be estimated for each of the different mediums. Oil paintings are by far the largest category, representing 52% of the volume and 78% of the value of artworks in the sample. The indices indicate that oil was the medium that experienced the largest price increases.

Finally, quantile regressions provide an alternative means to investigate different parts of the price distribution and are also more robust to potential outliers. OLS regressions provide estimates for the conditional means, whereas quantile regressions can characterise the entire distribution of the dependent variable. The indices resulting from quantile regressions for the 25th, 50th, and 75th percentiles are relatively similar. The lower end of the market seems to have depreciated slightly less after the peak in 2008. Overall the results seem to indicate that the dramatic price increases occurred in more expensive or high-end parts of the art market, and especially for oil paintings. 

The results for the origination and termination dates are reported in Table 4. The results indicate that the bubble process was relatively dispersed throughout the market. Although prices seem to have been especially explosive for high-end oil paintings by top artists, the bubble detection tests also provide some evidence of explosive prices for other medium categories and less expensive artists. The origination and termination dates also seem to be relatively consistent in suggesting a bubble formation period between 2006/07 and 2008.

```{r bubble_seg, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#---------------------------------------
#Segmented versions
make_real <- function(indeks) {
    for(i in 2:ncol(indeks)) { 
        for(j in 1:64) {
            indeks[j,i] <- indeks[j,i]/assets$CPI[j]*100 
        }
    }
    return(indeks)
}
alt <- cbind(pr_seg[,],ave_seg[,-1],mediums[,c(-1,-5,-9,-10)],quant_results[,-1])
alt <- make_real(alt)

y_indices <- log(na.approx(alt[,-1]))
bubble.nc <- list()
bubble.c <- list()
for(i in 1:ncol(y_indices)) {
    bubble1 <- numeric()
    bubble2 <- numeric()
    for(j in 12:64) {
        y <- y_indices[1:j,i]
        toets1 <- ur.df(y, type= "none", lags = 4, selectlags = c("AIC"))
        toets2 <- ur.df(y, type= "drift", lags = 4, selectlags = c("AIC"))
        bubble1 <- rbind(bubble1,toets1@teststat)
        bubble2 <- rbind(bubble2,toets2@teststat)
    }
    bubble.nc[[i]] <- bubble1
    bubble.c[[i]] <- bubble2
}

bubble.test1 <- numeric()
bubble.test2 <- numeric()

for(k in 1:ncol(y_indices)) { 
    bubble.test1 <- cbind(bubble.test1,bubble.nc[[k]])
    bubble.test2 <- cbind(bubble.test2,bubble.c[[k]][1:53])
}
bubble.test1 <- as.data.frame(bubble.test1)
bubble.test2 <- as.data.frame(bubble.test2)
bubble.test1 <- cbind(bubble.test1,K1)
bubble.test2 <- cbind(bubble.test2,K2)

Dates <- levels(artdata$timedummy)[-1:-11]
bubble.test1$Date <- Dates
bubble.test2$Date <- Dates

colnames(bubble.test1)[1:ncol(y_indices)] <- colnames(alt[-1])
colnames(bubble.test2)[1:ncol(y_indices)] <- colnames(alt[-1])

datum1 <- bubble.test1
datum1[!is.na(datum1)] <- NA
datum2 <- bubble.test1
datum2[!is.na(datum2)] <- NA
datums1 <- data.frame()
datums2 <- data.frame()

for(i in 1:15) {
    for(l in 1:53) {
        if(bubble.test1[l,i]>bubble.test1$"95%"[l]) { 
            datum1[l,i] <- bubble.test1[l,"Date"]
        }
        if(bubble.test2[l,i]>bubble.test2$"95%"[l]) { 
            datum2[l,i] <- bubble.test2[l,"Date"]
        }
    }
    if(sum(!is.na(datum1[,i]))>1) {
        NonNAindex <- which(!is.na(datum1[,i]))
        firstNonNA <- min(NonNAindex)
        datums1[1,i] <- datum1[firstNonNA,i]
        if (NonNAindex[NROW(NonNAindex)-1]==(max(NonNAindex)-1)) { 
            lastNonNA <- max(NonNAindex)
        } else lastNonNA <- NonNAindex[NROW(NonNAindex)-1]
    }
    
    datums1[2,i] <- datum1[lastNonNA,i]
    
    if(sum(!is.na(datum2[,i]))>1) {
        NonNAindex <- which(!is.na(datum2[,i]))
        firstNonNA <- min(NonNAindex)
        datums2[1,i] <- datum2[firstNonNA,i]
        lastNonNA <- max(NonNAindex)
    }    
    datums2[2,i] <- datum2[lastNonNA,i]
}    

datum <- rbind(datums1,datums2)
datum <- t(datum)
datum <- rbind(c("Start","End","Start","End"), datum)
rownames(datum) <- c(' ', colnames(bubble.test1)[1:15])
rownames(datum)[2:7] <- c("price_lower","price_middle","price_upper",
                          "value_lower","value_middle","value_upper")
datum[is.na(datum)] <- ""    
```

```{r table6, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
xt <- xtable(datum, caption="Dates of explosive behaviour by segment")
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(paste0('& \\multicolumn{2}{c}{  No Drift  } & \\multicolumn{2}{c}{ Drift }',collapse=''), '\\\\')

print(xt, "latex",comment=FALSE, add.to.row=addtorow, include.colnames=F,
      caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9)
```

#Discussion
The regression-based indices provide relatively consistent results in terms of the explosive periods in the South African art market, with a potential bubble most likely beginning in 2006 and ending in 2008. Although the method provides a consistent basis for identifying periods of explosive behaviour, it does not provide an explanation of the bubble episode. The findings are compatible with several different explanations, including rational bubbles, herd behaviour, and rational responses to fundamentals [@Phillips2011].

The results implicitly assume that the aesthetic or utility dividends associated with South African art did not exhibit explosive behaviour over the period. Aesthetic dividends fluctuate over time as they depend on buyers' willingness to pay for art, which in turn depends on preferences and wealth. However, preferences regarding art and culture would have had to fluctuate dramatically to explain the movements in art prices over the period. Even if trends can temporarily emerge for specific artists or schools of art, previous findings in the literature have shown that preferences tend to be very stable, even in the long run [@Penasse2014]. The aesthetic dividend can also fluctuate as wealth fluctuates over time [@Spaenjers2015]. The literature has provided evidence supporting this idea, with @Goetzmann2011 finding cointegrating relationships between top incomes and art prices. However, it is unlikely that aesthetic dividends, or factors such as collectors' preferences and wealth, experienced similar explosive behaviour over the period.

The large increase in art prices between 2005 and 2008 does not seem to be due to a fundamental shift in the types of artworks that were sold over that period. For instance, the top 100 artists in terms of volumes sold, which accounts for 60% of the volume traded and 90% of total turnover, has remained remarkably stable over time. Even if the exact same artworks were not being resold, the same artists' work still made up the vast majority of the market, and the hedonic model controls for the different artists. It is unlikely that the results are driven by sales of systematically better or higher quality artworks by specific artists that appreciated in price over that period, and by sales of systematically lower quality artworks by those artists after the crisis. Moreover, paintings are not sold at auction only to profit from price appreciation or capital gains. A substantial portion of consignments come from the so-called three D's: Debt, Divorce and Death. In other words, many sellers are forced to sell their artworks, even if those artworks have not experienced the largest price appreciation.

@Penasse2014 argue that limits to arbitrage induce a speculative component to art prices. High transaction costs and short-selling constraints could lead to prices diverging from fundamental levels, as they prevent arbitrageurs from pulling back prices to fundamentals [@Balcilar2015]. When prices are high, pessimists would like to short sell, but instead simply stay out of the market or sell to optimists at inflated prices. Optimists may be willing to pay higher prices than their own valuations, because they expect to resell to even more optimistic investors in the future. The difference between their willingness to pay and their own optimistic valuation is the price of the option to resell the asset in the future. The price of the resale option imparts a bubble component in asset prices, and can explain price fluctuations unrelated to fundamentals. These market failures hamper the ability of markets to correct price inefficiencies and implies that periods of bubble-like behaviour could exist with relatively little scope for arbitrage. This is especially applicable to art markets, where transaction costs are high, short selling is not possible, and without a rental market the only possibility to make a profit is by reselling at a higher price [@Penasse2014]. 

@Kindleberger2005 argued that a boom in one market often spills over into other markets. A famous example in the context of art is the link between the boom in Japanese stock and real estate prices and the Impressionist art market in the second half of the 1980s. @Hiraki2009 found a strong correlation between Japanese stock prices and the demand for art by Japanese collectors, leading to an increase in the price of Impressionist art during this period. @Kraussl2016 found corroborating evidence of a bubble period in the "Impressionist and Modern" art segment between 1986 and 1991. During this period Japanese credit was freely available, backed by increasing values of stocks and real estate, which led to a consumption and investment spree through the wealth effect. Japanese investors invested heavily in international art and particularly French Impressionist art in the late 1980s. Luxury consumption by Japanese art collectors increased international art prices until the art bubble burst as a direct consequence of the collapse of the Japanese real estate market [@Penasse2014].

Similarly, the run-up to the financial crisis saw large increases in asset prices and credit expansion in South Africa. It is likely that these conditions contributed to the explosive behaviour in South African art prices between 2006 and 2008. The financial crisis caused the bubble to burst and led to a decline in South African art prices. 

#Conclusion
This paper used a direct method of bubble detection developed by @Phillips2011 to test for explosive behaviour in South African art prices. The test requires the estimation of a reasonably accurate quality-adjusted price index, which can be challenging for unique assets such as art. Each methodology has shortcomings and the danger is that the biases inherent in each methodology may be driving the results. This paper estimates three sets of art price indices, based on the three main methodologies used to estimate quality-adjusted price indices for unique assets.

The regression-based indices seem to point to consistent evidence of mildly explosive price behaviour in the run-up to the financial crisis, between 2005/06 and 2008. The bubble seems to have been relatively dispersed throughout the market, although prices seem to have been especially explosive for high-end oil paintings by the top artists.

#References



