---
output: 
    pdf_document:
        fig_caption: yes
        number_sections: true
fontsize: 11pt
geometry: margin=1in
bibliography: References.bib
csl: harvard.csl
---

<!-- maketile -->
\begin{center}
\Large\scshape{Estimating a South African Art Price Index} \\ 
\vspace{1em}
\large\normalfont{Laurie Binge}\footnote{PhD candidate at the Department of Economics at the University of Stellenbosch.} \\
\normalsize\textit{Stellenbosch University} \\
\normalsize\normalfont{This draft: \today} 
\end{center}

\begin{small}
Although the South African art market has grown markedly over the last two decades, there is currently very little research on this market. The aim of the paper is two-fold: first to construct measures of the overall price movements in South African art prices over time and then to use these measures to test whether there was a bubble in the market over the period. Three broad methodologies are used to estimate quality-adjusted price indices for South African art since the turn of the millennium: central tendency methods, hedonic methods and hybrid repeat sales methods. The regression-based indices point to similar trends over the period, which shows the importance of regression-based methods to produce quality-corrected price indices. According to these measures, the South African art market experienced a huge price increase in the run-up to the Great Recession. The art price indices are then used to look for mildly explosive price behaviour in the market over the sample period. The regression-based indices seem to point to consistent evidence of explosive prices in the run-up to the Great Recession. 

\vspace{0.5em}
\noindent{\textbf{JEL Classification:} X01, Z01, Z01} \\
	\noindent{\textbf{Keywords:} South African Art, Hedonic Price Index, Pseudo Repeat Sales, Explosive Prices }
\end{small}

\renewcommand{\thefootnote}{\arabic{footnote}}

#Introduction
Contemporary African art, previously seen as a niche market, has experienced a surge in popularity over the last few decades. The South African art market in particular has received a lot of attention and has grown markedly over the last two decades, both in terms of the number of transactions and total turnover [@Fedderke2014]. Artworks by South African artists have reached record prices at international and local auctions, both for the country's "masters" - including Irma Stern, Walter Battiss, and JH Pierneef - and contemporary artists like William Kentridge [@Naidoo2013]. For example, in 2011 Bonhams in London sold Irma Stern's *"Arab Priest"* for a hammer price of £2.7 million, a world record for a South African artwork at auction. Also in 2011, Stern's *"Two Arabs"* was sold by Strauss & Co. for a hammer price of R19 million, a record for a South African auction house. The increase in interest in South African art, both locally and abroad, has sparked a vibrant market for collectors and investors. 

In his book *The Value of Art*, @Findlay2012 argues that collectors have three main motives for collecting art: a genuine love of art, social promise and investment possibilities. The first motive relates to the essential (or intrinsic) value of art and is often called aesthetic pleasure. The second relates to the social value or status consumption of art. To the extent that art is a luxury good, collectors may derive utility from the signal of wealth that it conveys [@Mandel2009]. The third motive involves the commercial value of art and relates to the investment possibilities of these passion investments. In addition to the potential for appreciation in value, artworks may be used to aid portfolio diversification, as collateral for loans, or to take advantage of slacker regulatory and tax rules. Thus, unlike pure financial investments, artworks are durable goods with consumption and investment good characteristics [@Renneboog2014].

The increase in the popularity of South Africa art, at least partly as an investment vehicle, is commensurate with international trends, where fine art has become an important asset class in its own right. In 2010 around 6% of total wealth was held in so-called passion investments, which include art, wine, antiques and jewellery [@Renneboog2014]. In 2013, art made up around 17% of high net worth individuals' allocations to passion investments [@Capgemini2013]. Of all these passion investments, art is the most likely to be acquired for its potential appreciation in value [@Capgemini2010]. 

To date there has been little research on the South African art market and particularly trends in art prices. It is important to analyse price movements over time in order to understand the dynamics of the market and to be able to answer questions about the development of the market. The first aim of the paper is to estimate accurate indicators of overall price movements in the South African art market since the turn of the millennium. 

However, estimating accurate indicators of real alternative assets like art can be difficult. Artworks are less liquid than traditional assets and have a low transaction frequency, which means that only a small part of the overall market is traded at any given time. They are also typically unique or heterogeneous, which makes different artworks difficult to compare. These features make it challenging to measure the state of the overall market over time.

Three broad methodologies have been used to develop price indices for assets with these features: central tendency methods, hedonic methods and repeat sales methods. This paper will compare and contrast various art price indices estimated with these methodologies. While each method has strengths and weaknesses, the indices estimated with the regression-based methods seem to point to the same general trend in South African art prices, which shows the importance of regression-based methods to produce quality-corrected price indices. According to these measures, the South African art market experienced a huge price increase in the run-up to the Great Recession. 

At the time many commentators claimed that the market was overheating and suggested the possibility of a "bubble" in the market [see for example @Rabe2011; @Hundt2010; @Curnow2010]. The second aim of the paper is look for evidence of a bubble in the South African art market over the period, by looking for mildly explosive behaviour in the estimated price indices. The regression-based indices seem to point to consistent evidence of explosive prices in the run-up to the Great Recession.

The rest of the paper proceeds as follows. Section 2 provides an outline of the methodologies applied in the literature and a brief review of the relevant literature. Section 3 looks at the available data for South African art. Sections 4, 5 and 6 report the results from the three estimation methods. Section 7 compares and evaluates the results. Section 8 introduces the bubble detection literature. Section 9 reports the results of the test for mildly explosive behaviour and Section 10 concludes.

```{r cleaning, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##=====================##
## READING IN THE DATA ##
##=====================##
suppressMessages(library(zoo))
suppressMessages(library(ggplot2))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(stargazer))
suppressMessages(library(micEcon))
suppressMessages(library(quantreg))
suppressMessages(library(McSpatial))
suppressMessages(library(quantmod))
suppressMessages(library(xtable))
suppressMessages(library(scales))
suppressMessages(library(tseries))
suppressMessages(library(urca))

setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\PhD Proposal Readings\\Art Price Index\\R Code")
artdata <- read.csv("Auction database.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE, 
                    colClasses=c("character","numeric","numeric","numeric","numeric","factor","factor","factor","character",
                                 "factor","factor","factor","character","factor","factor","factor","numeric","character",
                                 "numeric","numeric","numeric","numeric","numeric","numeric"))

##===================##
## CLEANING THE DATA ##
##===================##
artdata$date <- as.Date(artdata$date)
artdata$med_code <- factor(artdata$med_code, labels=c("Drawing", "Watercolour", "Oil", "Acrylic", "Print/Woodcut",
                                                      "Mixed Media","Sculpture","Photography", "Other"))
artdata$ah_code <- factor(artdata$ah_code, labels=c("5th Avenue","Ashbeys","Bernardi","Bonhams","Russell Kaplan",
                                                    "Stephan Welz","Strauss","Christies"))
artdata$timedummy <- factor(as.yearqtr(artdata$date, "%Y-%m-%d"))
artdata$lnprice <- log(artdata$price)
artdata$lnarea <- log(artdata$area)
artdata$lnarea2 <- artdata$lnarea*artdata$lnarea
#inteaction term: sculptures often only reported with 1 dimension (height)
artdata$lnsculpt_area <- ifelse(artdata$med_code=="Sculpture", artdata$lnarea, 0)
artdata$counter <- as.numeric(artdata$timedummy)

##----------------------
##Rank Artists by Volume
##----------------------
#Rank by Total Volume (all)
rankings <- count(artdata, artist)
rankings$rank_all <- dense_rank(desc(rankings$n))    #rank by density, with no gaps (ties = equal)
rankings$rank_total <- row_number(desc(rankings$n))  #equivalent to rank(ties.method = "first")
rankings$n <- NULL

artdata <- merge(artdata, rankings, by.x="artist", by.y="artist",all.x=TRUE)
```

#Methodologies for Constructing Art Price Indices
The first step is to estimate accurate measures of overall price movements in the art market. This is a prerequisite to try and examine whether there was evidence of a bubble over the period. This section introduces the various methodologies typically used to construct quality-adjusted prices indices for heterogeneous assets like art, real estate and wines.

The construction of price indices for real alternative asset markets is challenging for at least two reasons [@Jiang2014]. Firstly, the low frequency of trading means that only a subset of the market is traded at a given time, while the prices of non-transacted assets are unobservable. Secondly, the heterogeneity of individually unique assets means that the quality of assets sold is not constant over time. Thus, the composition of assets sold will generally differ between periods, making it difficult to compare prices over time [@Hansen2009]. Constructing an index for individually unique assets, like art, therefore requires a different approach than is used for indices of stocks, bonds and commodities. Four broad measurement techniques have been used to construct these indices [@Eurostat2013]:

a)	Naïve or central tendency methods
b)	Hedonic regressions
c)	Repeat sales regressions
d)	Hybrid models

The following sections provide a brief introduction to these methodologies. The literature does not provide an a priori indication of the most appropriate method and, in practice, the data dictates the choice.

##Central tendency or naïve methods
The simplest way to construct a price index is to calculate a measure of central tendency from the distribution of prices. As price distributions are generally skewed, the median is often preferred to the mean. These average measures have the advantage of being simple and easy to construct and do not require detailed data. 

However, an index based on average prices does not account for the difficulties mentioned above. For assets like artworks, naïve indices may be more dependent on the mix of objects that come to market, than changes in the underlying market. For instance, if there is an increase in the share of higher quality assets, an average measure will show an increase in price, even if the prices in the market did not change [@Hansen2009]. Hence, such a measure may not be representative of the price movements of all the assets in the market. If there is a correlation between turning points in asset price cycles and compositional and quality changes, then an average could be especially inaccurate [@Eurostat2013].

An improvement can be made by stratification of the data. Stratified measures control for variations in prices across different types of assets by separating the sample into subgroups according to individual characteristics such as artist and medium. The Fisher index, which is the geometric mean of the well-known Laspeyres and Paasche indices[^2], is often recommended (Eurostat, 2013). Stratified measures are currently used by ABSA, FNB and Standard Bank to construct property price indices for South Africa. However, scholarly work rarely employs central tendency indices, as these mix-adjusted measures adjust only for specific types of compositional change. The repeat sales and the hedonic regression methods have dominated in the international literature.  

[^2]: The Laspeyres index holds the quantity weights fixed in the base period, while the Paasche index holds the quantity weights fixed at the comparison period.

##Hedonic regression methodology
Artworks are heterogeneous assets, with a variety of characteristics that make them unique. The hedonic regression method recognises that the prices of heterogeneous goods can be described to some extent by their characteristics [@Eurostat2013]. In the context of art, characteristics may include physical (e.g. medium) and non-physical attributes (e.g. artist reputation). The hedonic approach estimates the value attached to each of these attributes. 

The hedonic approach entails regressing the logarithm of the sales price on the relevant attributes, as well as time dummies, which capture the "pure price effect" [@Kraussl2010]. The standard hedonic model usually takes the following form: 
$$\ln P_{it} =\alpha+\sum_{j=1}^z\beta_jX_{ij}+\sum_{t=1}^\tau\gamma_tD_{it}+\epsilon_{it}$$
where $P_{it}$ represents the price of artwork $i$ at time $t$, $X_{ij}$  is a series of characteristics of item $i$ at time $t$, and $\beta_j$ reflects the coefficient values (implicit prices) of the attributes, $D_{it}$  is the time dummy variable, which takes the value 1 if item $i$ is sold in period $t$ and 0 otherwise, and $\epsilon_{it}$ represents the error term.

The hedonic method therefore controls for quality changes by attributing implicit prices to a set of value-adding characteristics of the individual asset. Hedonic regressions control for the observable characteristics of an asset to obtain an index reflecting the price of a "standard asset" [@Renneboog2002]. It is also possible to allow the coefficients (the implicit prices assigned to characteristics) to evolve over time with the adjacent-period method, which is discussed in more detail below [@Triplett2004]. 

Thus, the hedonic approach can circumvent the problems of heterogeneity of individually unique assets, changes in composition and quality, as well as the exclusion of single-sale data (a problem with repeat sales regressions) [@Hansen2009]. However, the choice of the attributes in a hedonic regression involves subjective judgement and is limited by data availability. If relevant variables are omitted or the functional form is incorrectly specified, it will result in omitted variable or misspecification bias, which will bias the parameter estimates and therefore the indices [@Jiang2014]. In practice, some omitted variable bias will most likely be present when estimating a hedonic model, although the sign and magnitude of the bias and its impact on the price index are difficult to predict [@Eurostat2013].  

##Repeat sales regression method
The repeat-sales method seeks to avoid the problem of heterogeneity by tracking the sale of the same asset over time [@Jiang2014]. This method aggregates sales pairs and estimates the average return on the set of assets in each period [@Kraussl2010]. 

In the standard repeat sales model the dependent variable is regressed on a set of dummy variables corresponding to time periods. The coefficients are estimated only on the basis of changes in asset prices over time. The basic regression takes the following form:
$$\ln\frac{P_{t+1}}{P_t} =\sum_{i=1}^t\gamma_id_{i}+\epsilon_{i}$$
where $P_t$ is the purchase price for time $t$; $\gamma_i$ is the parameter to be estimated for time $i$; $d_i$ represents the monthly dummy variables (-1, 0, 1) indicating the occurrence of $P_t$; and $\epsilon_i$ is a white noise residual.

The repeat sales method avoids having to correctly specify the characteristics and functional form that determine asset value (a problem with hedonic models). By only using assets that have been sold at least twice, the method controls for other factors contributing to the variation in price growth. It also has the advantage of not being data intensive, as the only information required to estimate the index is the price, the sales date and a unique identifier (e.g. the address of the property). The repeat sales method has often been applied in the construction of real estate indices, where there is a lack of detailed information on each sale (which is necessary for the hedonic method).

A disadvantage of the repeat sales method is that single-sale data is discarded. This is problematic for these assets because the resale of a specific item may only occur infrequently, which reduces the total number of available observations substantially. Another problem is the possibility of sample selection bias. Assets that have traded more than once may not be representative of the entire population of assets. For example, if cheaper artworks sell more frequently than expensive artworks, but high-quality artworks appreciate faster, a repeat sales index will tend to have a downward bias [@Eurostat2013]. Several studies have investigated this source of bias and the size and direction of the bias has varied by the sample under investigation.

##Hybrid models
A hybrid model approach involves a combination of the repeat sales and hedonic approaches. The hybrid formulation exploits the control of variation inherent in repeat sales pairs and avoids the problem of possible misspecification inherent in the hedonic methodology [@Bester2010]. By combining the two methods, the hybrid approach tries to exploit all the sales data, while addressing the problems of sample selection bias, inefficiency and changes in quality [@Eurostat2013]. In other words, the hedonic model has less sample selection bias but potentially greater specification bias, whereas the repeat sales model has less specification bias but potentially more sample selection bias. Some combination of the two might lead to an index that reduces both sample selection and specification bias [@Jiang2014].

In the context of real estate, for instance, @Case1991 used single-sale and repeat-sale properties to jointly estimate price indices using generalised least squares regressions. More recently, @Guo2014 developed a "pseudo repeat sales" procedure to construct more reliable price indices for newly constructed homes. Their procedure matched the sales prices of very similar new units in order to construct a large pseudo repeat sales sample. This approach is discussed in more detail below.

As previously mentioned, there is no consensus regarding the preferred approach of constructing constant-quality price indices, either theoretically or empirically. However, there is reason to believe that constructing more advanced measures may provide a better guide to pure price changes than a simple median [@Hansen2009]. The specific methodology adopted is dependent on the data available. Art price indices tend to employ some variant of the hedonic method, due to the availability of more detailed data on characteristics and a lack of repeat sales of artworks. The following section provides a brief literature review of the estimation of art price indices. The empirical sections then consider the alternative approaches to gauge their performance and to see if they point to the similar aggregate trends.

##A brief literature review
A number of academic studies have constructed art price indices for various art markets around the world. These studies have typically been interested in evaluating the risk-adjusted returns to investigate whether the art market provides potential diversification benefits for an investment portfolio. The interest in investing in art has recently received a large boost from the increase in the availability of art price data [@Campbell2009].  

These studies have typically relied on publically available auction prices.[^3] Art is also sold privately, either directly by artists or through dealers. However, dealers' sales records are generally not available, as releasing such information may be damaging to the dealer's business and dealers have an incentive to give the impression that there is high demand for their artworks. Nevertheless, it is generally accepted that auction prices set a benchmark that is also used in the private market [@Renneboog2012]. For instance, if an artwork sells for a lower price at auction than the prices offered by a dealer, buyers would likely move to another dealer or simply purchase at auction. Thus, prices for private sales are likely anchored by auction prices and are likely to be highly correlated for the same works [@Olckers2015].

[^3]: Auctions account for around half of the art market according to The European Fine Art Fair Art Market Report 2014.

The majority of studies have used hedonic models to construct indices, due to the lack of repeat sales of artworks and the availability of information on many of their important attributes. @Anderson1974 was the first to apply a hedonic regression to art prices. More recent examples include: @Renneboog2002, who estimated an index of Belgian paintings; @Kraussl2010, who studied the prices of the top 500 artists in the world; @Kraussl2010a, who analysed the performance of art in Russia, China and India; and @Kraussl2014 who analysed art from the Middle East and Northern Africa region. 

In estimating art price indices, studies typically to set up some form of selection criteria for which artists to include in the index calculation. The number of artists is constrained by the number of artist dummies that can be included in the model (i.e. degrees of freedom). A common criterion has been historical importance, measured as the frequency with which an artist was mentioned in a collection of art literature. @Kraussl2008 argued that availability and liquidity are better criteria from an investor's point of view, as the index would reflect artworks actually traded in the market. This implies that selection could be based on the number of sales, rather than historic relevance. @Kraussl2008 developed a two-step hedonic approach, which allows the use of every auction record, instead of only those auction records that belong to a sub-sample of selected artists. This approach is discussed in more detail below. 

Hedonic models typically include characteristics that are relatively easily observable and quantifiable. The attributes include the artist, the auction house, the size, the medium, the theme, whether the artwork is signed, and the artist's living status [@Kraussl2010a]. Although omitted variables are a problem in every model, hedonic pricing is particularly suitable for luxury consumption goods, where a limited number of key characteristics often determine the willingness to pay for an item. In any event, the omitted variable bias is often small in practice [@Triplett2004; @Renneboog2012].

Multi-period pooled hedonic regressions have been criticised for holding the hedonic coefficients fixed over the entire sample. The stability of the coefficients in a pooled regression can also become an issue as the number of periods expands. The adjacent-period method can deal with this by estimating separate overlapping hedonic models and then chaining the sequence of shorter indices together to form a continuous time series. Thus, it allows the coefficients, and therefore the implicit prices assigned to the characteristics, to vary in each regression [@Triplett2004].  

A few studies have utilised the repeat sales method to estimate art price indices. These studies have typically relied on a very large sales databases, due to the infrequency of repeat sales of individual artworks. Indeed, for artworks the resale of a specific item may occur only very rarely, which might be related to the very high transaction costs involved. @Mei2002 constructed the seminal repeat sales index of art prices for the period 1875-2000. The resulting index returns were compared to a range of assets. Their methodology is currently used to produce the Mei Moses Art Index for Beautiful Asset Advisors. @Goetzmann2011 used a long-term repeat sales art market index to investigate the impact of equity markets and top incomes on art prices. These indices followed the @Case1987 methodology and were based on over a million sales dating back to the 18th century.

@Korteweg2013 constructed a repeat sales index based on a large database of repeat sales between 1972 and 2010. They argued that standard repeat sale indices suffer from a sample selection problem, as sales are endogenously related to asset performance. If artworks with higher price increases were more likely to trade, the index would be biased and not representative of the entire market. In periods with few sales it would be possible to observe large positive returns, even if overall values were declining. A Heckman selection model, predicting whether an artwork actually sold, was used to correct for this bias. The correction decreased the returns to an investment in art significantly.

Bought-in lots (i.e. items that do not reach the reserve price and remain unsold) are always a problem when constructing these indices. Most studies lack data on buy-ins and are forced to ignore the problem. @Collins2009 developed a hedonic index that corrected for sample selection bias from buy-ins. They argued that because auctions have high proportions of unsold lots (typically 30%-40%), price indices suffer from non-randomness in the data. A sample based only on sold lots systematically excludes "less fashionable" artworks, potentially introducing a bias in the sample of prices. A Heckman selection model was used to address this issue.[^4] The results confirmed a statistically significant sample selection problem, in line with similar studies in the property market. 

[^4]: The nature of sample selection bias is different in the approaches. The repeat sales method ignores all information on single sales, so that it may not represent the population. The hedonic method only uses sold items, so that bias may arise from unsold items.

###South African art price literature
In the South African context, @Olckers2015 created a proxy for the cultural value of art by constructing an Art Critic Index based on a survey of the South African art literature. The auction results (1996-2012) were obtained from AuctionVault's online database. Using a hedonic model they found that cultural value was positively correlated with auction prices, i.e. the economic value of art. Interestingly, they singled out and analysed some specific artists that were outliers in this relation.

@Fedderke2014 studied the relationship between South Africa's two major fine art auction houses: Strauss & Co and Stephan Welz & Co. The analysis was based on a hand-coded dataset of auction prices. They developed a theoretical framework to consider the interaction between the market leader (Strauss) and the market follower (Stephan Welz). The model predicted that the market follower would be forced to issue excessive price estimates to attract sellers, at the cost of higher buy-in rates. The predictions were tested by employing a hedonic model to construct a counterfactual for auction prices. Both direct and indirect tests confirmed the predictions of the theoretical model. 

Citadel, a wealth manager, has been publishing the Citadel Art Price Index (CAPI) since 2011. The CAPI is intended to outline general trends in the South African art market. It uses an adjacent-period hedonic regression model, based on the top 100 artists in terms of sales volumes, and a 5-year rolling window estimation period [@Econex2012]. The estimation below will build on the CAPI in order to contribute to the research on the South African art market.[^5]

[^5]: We can remove all references to the CAPI, since it was not an academic exercise? 

#South African Art Auction Data
Auction prices are the only consistently available price data on the South African art market. This paper will therefore rely on publicly available auction prices, similar to almost all other studies estimating art price indices. As explained above, there should be a strong correlation between auction prices and private prices [@Olckers2015].

Strauss & Co and Stephan Welz & Co are the two local auction houses that have handled the bulk of sales in recent years, with auctions in Cape Town and Johannesburg. Other local auction houses include Bernardi in Pretoria and Russell Kaplan in Johannesburg. Bonhams in London is the only major international auction house with a dedicated South African art department, though some competition is emerging from Sotheby's and Christie's. Bonhams has two major South African art sales annually. The auction houses follow an open ascending auction, where the winner pays the highest bid. A sale is only made if the hammer price is above the secret reserve price. Otherwise the artwork is unsold and is said to be bought in [@Fedderke2014].

The indices are based on data recorded by AuctionVault. This data cover sales of South African art at 8 auction houses[^6] from the year 2000 onwards. The database includes 52,059 sales by 4,123 different artists. The following characteristics are available for each auction record: hammer price, artist name, title of work, medium, size, whether or not the artwork is signed, dated and titled, auction house, date of auction, and the number of distinct works in the lot. Like most studies, the database lacks information on buy-ins and the analysis is forced to disregard the potential sample selection problem.[^7]

[^6]: These are: 5th Avenue, Ashbeys, Bernardi, Bonhams, Christies, Russell Kaplan, Stephan Welz & Co and Strauss & Co.

[^7]: If the database included information on the artwork characteristics, censored regression techniques such as the Heckman selection model, could be used to look at the sample selection bias. However, the dataset does not include the artworks that were bought-in, which means that it is a truncated sample. Unfortunately, truncated regression techniques cannot be performed to correct for the bias, as the cut-off points (i.e. the secret reserve prices) are different for each individual artwork and more importantly, unknown.

The South African art market has grown markedly over the last decade. Figure 1 illustrates the increase in auction turnover over the sample period (2000-2015) by auction house. Turnover increased dramatically in 2007 and again in 2010. Stephan Welz & Co dominated sales until 2009, when Strauss & Co and Bonhams begin to account for the bulk of turnover.[^8] At its peak in 2011, turnover at auction had reached almost R400 million. 

[^8]: This was just after Stephan Welz sold his stake in Stephan Welz & Co and moved to Strauss & Co.

```{r figure1, echo=FALSE, cache = TRUE, fig.height=4, fig.width=7.5, fig.cap="Turnover (sum of hammer prices) by auction house (2000-2015)"}
#Plot turnover by auction house
artplot <- aggregate(artdata$hammer_price, by=list(artdata$year,artdata$ah_code), FUN = sum, na.rm=TRUE)
g <- ggplot(artplot, aes(x=Group.1, y=x,fill=Group.2))
g <- g + geom_bar(stat="identity")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + scale_fill_discrete(name="Auction House")
g <- g + scale_y_continuous(labels=comma)
g <- g + ylab("Turnover (Sum of Hammer Price)")
g <- g + xlab("Date")
g
```

Figure 2 illustrates the increase in the total number of sales lots over the period, as well as the median sales price. Total sales echoed the increase in auction turnover over the period. The increase in median annual sales prices was especially large, from R3,200 in 2003 to R10,000 at its peak in 2010. This confirms anecdotal evidence on the rise in popularity of the South African art market.

```{r figure2, echo=FALSE, cache = TRUE, fig.height=4, fig.width=6.5, fig.cap="Median hammer prices and total sales (lots) at auction (2000-2015)"}
#Plot total sales and annual median price
artplot1 <- aggregate(artdata$hammer_price, by=list(artdata$year), length)
artplot2 <- aggregate(artdata$hammer_price, by=list(artdata$year), FUN = median, na.rm=TRUE)
artplot <- merge(artplot1, artplot2, by="Group.1",all.x=TRUE)
names(artplot) <- c("Date","Total Sales","Median Price")
artplot <- melt(artplot, id="Date") 
g <- ggplot(artplot, aes(x=Date,value,colour=variable,fill=variable))
g <- g + geom_bar(subset=.(variable=="Total Sales"),stat="identity")
g <- g + geom_line(subset=.(variable=="Median Price"),size=1)
g <- g + theme(legend.position="bottom") + theme(legend.title=element_blank())
g 
```

Table 1 reports descriptive statistics for the hammer prices over the sample period. The mean price of R49,824 was much higher than the median of R7,000, indicating that the sample is skewed by very high prices for certain artworks.

```{r table1, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
summaryfunction <- function(x) {
    if( is.numeric(x)!=TRUE) {stop("Supplied X is not numeric")}
    mysummary = data.frame("Min." =as.numeric(min(x,na.rm=TRUE)),"1st Qu." = quantile(x,na.rm=TRUE)[2],
                           "Median" = median(x,na.rm=TRUE),"Mean" = mean(x,na.rm=TRUE),"3rd Qu." = quantile(x,na.rm=TRUE)[4],
                           "Max." = max(x,na.rm=TRUE),row.names="")
    names(mysummary) = c("Min.","1st Qu.","Median","Mean","3rd Qu.","Max.")
    return(mysummary)
}
xt <- xtable(summaryfunction(artdata$hammer_price), caption="Descriptive statistics of auction hammer prices")
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top")) 
```

#Central Tendency Indices
Figure 3 illustrates two central tendency price indices are estimated at a quarterly frequency, to act as a baseline for the index comparisons. The naïve index, which is simply the median price for each quarter, shows a large amount of variation and no consistent picture emerges. The Fisher index is a mix-adjusted central tendency index stratified by artist and medium.[^9] It allows the base periods to vary for each index point and the index points are then chained together. The Fisher index shows implausibly large increases over the sample period. In this case the stratification does not seem to be very effective. This is probably because the artist and medium categories only capture a small portion of the variation in the quality of artworks that come to market between each period. The mix-adjusted measure will not account for any changes in the mix of artworks sold that are unrelated to artist and medium. The stratified index also does not account for changes in the mix of artworks sold within each subgroup, in this case changes in the mix of artworks by a certain artist in a specific medium [@Eurostat2013]. Moreover, the subgroups become very small when separated in this way, which means that small changes can have a large effect on the index. 

[^9]: An alternative would be to segment by price category, as Fedderke and Li (2014) suggest? Ons kan hierdie section maar uitlos as dit nie nodig is nie, veral omdat die resultate so vreemd is? 

The results illustrate that central tendency measures are deficient in this case and should be used with caution. That is why regression-based measures are generally preferred in the academic literature. The hedonic indices in the following section control for quality changes by taking many more characteristics into account.

```{r naive, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
naive_index <- aggregate(artdata$price, by=list(artdata$timedummy), FUN=median, na.rm=TRUE)
naive_index$index <- naive_index$x
naive_index$index <- naive_index$index/naive_index[1,2]*100
naive_index$index <- as.numeric(naive_index$index)
colnames(naive_index) <- c("Date","Median","Index_Naive")

#Stratify by artist and medium
#get quantity and median price per group per quarter     
strat_p <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=mean)
strat_q <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=sum)
strat_q$x <- strat_q$x/strat_p$x        #the count q
strat_p <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=median)

chain2 <- function(strat_p, strat_q, kwartaal1,kwartaal2) {
    strat_p1 <- subset(strat_p, strat_p$Group.1==kwartaal1)
    strat_q1 <- subset(strat_q, strat_q$Group.1==kwartaal1)
    strat_p2 <- subset(strat_p, strat_p$Group.1==kwartaal2)
    strat_q2 <- subset(strat_q, strat_q$Group.1==kwartaal2)
    #get sample of median prices and quantities for specific artist for the two quarters
    strat_pc <- merge(strat_p1, strat_p2, by=c("Group.2","Group.3"))
    strat_qc <- merge(strat_q1, strat_q2, by=c("Group.2","Group.3"))
    #Laspeyres (keeps quantity weights fixed at base)
    Lasp <- sum(strat_pc$x.y*strat_qc$x.x,na.rm=TRUE)/sum(strat_pc$x.x*strat_qc$x.x,na.rm=TRUE)
    #Paasche (keeps quantity weights fixed at end)
    Paas <- sum(strat_pc$x.y*strat_qc$x.y,na.rm=TRUE)/sum(strat_pc$x.x*strat_qc$x.y,na.rm=TRUE)
    return(c(Lasp,Paas))
}

datum <- c("2000 Q1","2000 Q2","2000 Q3","2000 Q4","2001 Q1","2001 Q2","2001 Q3","2001 Q4","2002 Q1","2002 Q2","2002 Q3","2002 Q4","2003 Q1","2003 Q2","2003 Q3","2003 Q4","2004 Q1","2004 Q2","2004 Q3","2004 Q4","2005 Q1","2005 Q2","2005 Q3","2005 Q4","2006 Q1","2006 Q2","2006 Q3","2006 Q4","2007 Q1","2007 Q2","2007 Q3","2007 Q4","2008 Q1","2008 Q2","2008 Q3","2008 Q4","2009 Q1","2009 Q2","2009 Q3","2009 Q4","2010 Q1","2010 Q2","2010 Q3","2010 Q4","2011 Q1","2011 Q2","2011 Q3","2011 Q4","2012 Q1","2012 Q2","2012 Q3","2012 Q4","2013 Q1","2013 Q2","2013 Q3","2013 Q4","2014 Q1","2014 Q2","2014 Q3","2014 Q4","2015 Q1","2015 Q2","2015 Q3","2015 Q4")

ketting2 <- chain2(strat_p,strat_q,datum[1],datum[2])
ketting2 <- rbind(ketting2,chain2(strat_p,strat_q,datum[2],datum[3]))
for(i in 3:63) {
    ketting2 <- rbind(ketting2,chain2(strat_p,strat_q,datum[i],datum[(i+1)]))
}
ketting2 <- as.data.frame(ketting2)
ketting2$V3 <- sqrt(ketting2[,1]*ketting2[,2])  #Fisher index is the geometric mean
ketting2$V4[1] <- ketting2$V3[1]*100
for(i in 2:63) {                                #use the growth rates to generate the index
    ketting2$V4[i] <- ketting2$V4[(i-1)]*ketting2$V3[i]
}
ketting2$Date <- as.factor(datum[-1])
colnames(ketting2) <- c("Las","Paas","Fisher","Index_Fisher","Date")
```

```{r figure3, echo=FALSE, cache = TRUE, fig.height=4, fig.width=6.5, fig.cap="Central tendency South African art price indices (2000Q1=100)"}
index_plot <- merge(ketting2, naive_index, by.x="Date", by.y="Date",all.x=TRUE)
index_plot <- index_plot[,c(1,5,7)]
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.position="bottom") + theme(legend.title=element_blank())
g
```

#Hedonic Indices
##Artwork characteristics
Hedonic art price models typically include characteristics that are relatively easily observable and quantifiable. This section briefly discusses the main variables usually included in the hedonic models.[^10]

[^10]: Should some of the exploratory graphs comparing prices and various variables be included?

*Artist reputation*: Hedonic models typically include dummy variables to control for all of the artists. However, some artists often have to be excluded from estimation, due to a lack of degrees of freedom. Alternatively, a reputation variable can be constructed, either from the art literature, or from the auction data itself with a procedure like @Kraussl2008 2-step hedonic approach. The models in this section are estimated using a continuous reputation variable, as explained below. 

*Size*: The most common variable used to describe the physical characteristics of an artwork is the size or surface area. Prices are expected to increase with size, up to the point that the work becomes too large [@Renneboog2012]. Squared values are therefore occasionally included to take non-linearities into account. @Fedderke2014 found this to be the case for the South Africa art in their sample.[^11] The models that follow use the natural logarithm of the surface area of the artwork in $cm^2$. The models include an interaction term for sculpture size, as the size of a sculpture is usually only recorded in terms of its height (in cm). 

[^11]: The squared term in our case is positive, however, which is contrary to expectations. It should have a negative sign, and it is not clear why this is the case. Should I still include it?

*Auction house*: Dummy variables for the auction houses are also typically included. The more prominent auction houses usually have a positive effect on prices. One reason might be that the more renowned auction houses will offer higher quality work [@Kraussl2010a]. Thus, the variables might be picking up otherwise unobservable quality differences and do not necessarily reflect auction house certification [@Renneboog2012]. Moreover, different auction houses charge different commissions to both buyers and sellers. Strauss & Co reported a buyer's premium of 10%-15%, while Bonhams charged premiums of up to 25% [@Olckers2015]. The hammer prices exclude these premiums and are therefore not a perfect measure of the cost to the buyer and revenue to the seller. For the purposes of a price index, however, the auction house dummies should capture the different premiums charged by the auction houses. 

*Mediums*: Average prices vary across mediums. This might be due to the durability of the medium, the stage of production the medium is associated with (e.g. preparatory drawings) and in some case the replacement value of the materials used (e.g. sculptures cast in bronze). Oil paintings traditionally earn the highest prices. The availability of copies may decrease the prices of prints and photographs relative to other mediums. Studies typically include dummy variables for the different mediums as defined in their data [@Kraussl2010a]. The models in this section use the 9 mediums defined in the dataset, the same mediums used by @Olckers2015.[^12] 

[^12]: In a few cases studies have also differentiated between medium (e.g. oil) and material (e.g. canvas). The dataset includes enough detail in the latter years to identify the medium and the materials and this finer classification could be used as a robustness check? 
The subject matter or theme of an artwork can affect its value. A few studies (e.g. Renneboog & Spaenjers (2012) and Fedderke & Li (2014) have included controls for the theme of the artwork. Artwork can, for instance, be classified as portraits, landscapes, abstract works, etc. A classification of this kind would entail a much smaller sample, as the theme would have to be derived from the title of the artwork. Nevertheless, such classification could be carried out as a robustness check?
A few studies have included dummies to indicate whether an artist was alive. Artworks of artists who are no longer alive are generally thought to be more valuable, as the production has ceased. However, artists who are no longer alive are not able to build on their reputation, which might result in lower sale prices in the long run (Kräussl & Lee, 2010). Hence, it is not clear if the variable will be significant. Fedderke & Li (2014) found that the date of death and age of the artist were statistically insignificant for their South African sample. Nevertheless, the models could include this variable as a robustness check?

*Authenticity dummies*: Models often include dummies for whether the artwork is signed and dated. There might be a premium for these attributes, as there is less uncertainty about authenticity [@Renneboog2014]. These dummies are included in the models below and are expected to have positive coefficients. 

*Number of works in the lot*: The models below also control for cases in which more than one artwork was sold in the same auction lot. This is because the recorded size corresponded to each artwork separately and not the group. Moreover, it is possible that lots including more than one artwork fetch a lower price per artwork than if they sold separately.

*Date dummies*: The models below include time dummies at a quarterly frequency, which are used to estimate the indices (i.e. the time dummy hedonic method).[^13] The exponentials of the time dummy coefficients represent the appreciation in the value of art in that specific period, relative to the common base period.[^14]

[^13]: The double imputation hedonic method is sometimes favoured by statistical agencies, e.g. Eurostat (2013). However, the double imputation index could not be implemented in this case, as the models and the variables changed too much between periods. For example, if an artist was not present in the next period, his/her new price could not be estimated.

[^14]: Such an index will track the geometric mean, rather than the arithmetic mean, of prices over time, because of the log transformation prior to estimation. This is important for the estimation of returns if there is time variation in the (heterogeneity-controlled) dispersion of prices. If it is assumed that the regression residuals are normally distributed in each period, a correction can be made by defining corrected index values as: $I_t =\exp\left[\gamma_t+ 1/2(\sigma_t^2-\sigma_0^2 )\right]*100$, where $\sigma_t^2$ is the estimated variance of the residuals in period t (Renneboog & Spaenjers, 2012). In practice, however, this adjustment is often negligible (Hansen, 2009), as it is in this case.

Although there are probably still omitted variables that influence prices, and thus may bias the coefficients and the indices, the bias is often small in practice [@Triplett2004; @Renneboog2012].[^15] Relatively detailed data is available for art, which should capture a large part of the variation in sales prices. Omitted variable bias should therefore be less of a problem than for other real alternative assets like real estate.[^16] 

[^15]: According to Triplett (2004), even if the hedonic coefficients are biased it is not necessarily the case that the hedonic index will be biased. It will depend on whether the correlations among included and omitted characteristics in the cross section imply the same correlations in the time series. If cross section correlations and time series correlations are the same, the hedonic index may be unbiased, even though the hedonic coefficients are biased. It is possible that changes in (unobserved) characteristics quantities between two periods move to offset the error in estimating the implicit prices of included variables. The bias therefore becomes an empirical matter, because it is the effect on the price index that matters, not just the effect on the hedonic coefficients.

[^16]: According to Hansen (2009), there are various weighting approaches. If one is interested in the change in the value of the art stock (or a representative portfolio), then a higher weight should be given to price changes in higher-value artworks because of their greater share in the total value of the art stock. On the other hand, if one wishes to measure price changes in the representative artwork, then an equal weighting of observed art price inflation rates would be appropriate. This paper focuses on the pure price changes for a representative artwork, assuming an equal weighting. 

###Continuous artist reputation variable: two-step hedonic approach
The number of artist dummy variables that can be included in the hedonic regression is limited by the degrees of freedom, which means that some artists usually get excluded from the sample. @Kraussl2008 developed a two-step hedonic approach, which allows the use of every auction record, instead of only those auction records that belong to a sub-sample of selected artists. The approach involves the estimation of a continuous artist reputation variable, which is included in the regression instead of the artist dummy variables. In this way the approach accounts for the degrees-of-freedom consideration, which limits the number of artist dummy variables that can be included. It increases the sample size, reduces inherent selection bias, and reduces the impact of outliers when there are few observations for a specific artist.

@Triplett2004 showed that a hedonic function with a logarithmic dependent variable would yield an index equal to the ratio of the unweighted geometric means of prices in periods t and t+1, divided by a hedonic quality adjustment. The hedonic quality adjustment is a quantity measure of the mean change in the characteristics of assets sold in period t and t+1, valued by their implicit prices ($\beta_j$): 
$$Index = \frac{\prod_{i=1}^n(P_{i,t+1})^\frac{1}{n}}{\prod_{i=1}^m(P_{i,t})^\frac{1}{m}}/\text{hedonic adjustment} $$
$$\text{hedonic adjustment} = \exp \left[\sum_{j=1}^z\beta_j(\sum_{i=0}^n \frac{X_{ij,t+1}}{n}- \sum_{i=1}^m \frac{X_{ij,t}}{m})\right]$$

@Kraussl2008 argued the same method could be used to adjust the average price of an artist's work for differences in quality. The resulting index yields the value of artworks by artist y, relative to the base artist 0:
$$\text{Artist reputation index} = \frac{\prod_{i=1}^n(P_{i,y})^\frac{1}{n}/\prod_{i=1}^m(P_{i,0})^\frac{1}{m}}{\exp \left[\sum_{j=1}^z\beta_j(\sum_{i=0}^n \frac{X_{ij,y}}{n}- \sum_{i=1}^m \frac{X_{ij,0}}{m})\right]} $$

where $P_{i,y}$ is the value of painting i, created by artist y; $X_{ij}$ are the characteristics of the artworks, excluding the artist dummy variables. 

The first step is to estimate the full hedonic model on a sub-sample of artists to obtain the characteristic prices ($\beta_j$). Following @Kraussl2008, the sub-sample includes the top 100[^17] artists in terms of volume, representing 53% of records and 92% of the value. The coefficients are similar to those for the full pooled model and it is assumed that the characteristic prices are representative. Next the artist reputation index is calculated for each artist relative to the base artist (Walter Battiss), i.e. the relative quality-corrected prices for the works of artist y relative to artist 0. The reputation index is then used as a continuous proxy variable for artistic value in the hedonic models, instead of the artist dummies. As a robustness check, the models are also estimated including all of the artist dummies, except for those artists that only sold one artwork over the sample period. The results were very similar, in line with the findings in @Kraussl2008.

[^17]: More artists can be included in the first step, but the estimation takes very long to process.

```{r reputation, eval=FALSE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
##------------------------------------------##
##---ARTIST REPUTATION VARIABLE (Kraussl)---##
##------------------------------------------##
modeldata <- subset(artdata, artdata$rank_all<101)
list_expl_vars <- c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                    "nr_works","artist","timedummy")
expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
model_100 <- lm(expl_vars, data=modeldata)

#Second step: betaj coefficients are plugged into equation for every artist pair (base & another) 
rep <- list()
rep[[1]] <- 1
for(i in 2:(max(artdata$rank_total))) {
    list_vars <- c(list_expl_vars,"price")
    
    #geometric mean of paintings by artist y
    y <- subset(artdata[,list_vars], artdata$rank_total==1)
    y <- y[!rowSums(is.na(y)), ]
    py <-  exp(mean(log(y$price)))  
    ym1 <- subset(artdata[,list_vars], artdata$rank_total==i)
    ym1 <- ym1[!rowSums(is.na(ym1)), ]
    pym1 <-  exp(mean(log(ym1$price)))
    sbx <- 0
    
    #average of characteristics time implicit attribute price
    xy <- mean(y$lnarea)
    xym1 <- mean(ym1$lnarea)   
    b <- summary(model_100)$coefficients[grepl("lnarea", rownames(summary(model_100)$coefficients)),1]
    bx <- b*(xym1-xy)   
    sbx <- sbx + bx
    
    xy <- mean(y$lnsculpt_area)
    xym1 <- mean(ym1$lnsculpt_area)   
    b <- summary(model_100)$coefficients[grepl("lnsculpt_area", rownames(summary(model_100)$coefficients)),1]
    bx <- b*(xym1-xy)   
    sbx <- sbx + bx
    
    xy <- mean(y$nr_works)
    xym1 <- mean(ym1$nr_works)   
    b <- summary(model_100)$coefficients[grepl("nr_works", rownames(summary(model_100)$coefficients)),1]
    bx <- b*(xym1-xy)   
    sbx <- sbx + bx
    
    xy <- mean(as.numeric(y$dum_signed)-1)
    xym1 <- mean(as.numeric(ym1$dum_signed)-1)   
    b <- summary(model_100)$coefficients[grepl("dum_signed", rownames(summary(model_100)$coefficients)),1]
    bx <- b*(xym1-xy)   
    sbx <- sbx + bx
    
    xy <- mean(as.numeric(y$dum_dated)-1)
    xym1 <- mean(as.numeric(ym1$dum_dated)-1)   
    b <- summary(model_100)$coefficients[grepl("dum_dated", rownames(summary(model_100)$coefficients)),1]
    bx <- b*(xym1-xy)   
    sbx <- sbx + bx
    
    auc_house <- c("Ashbeys","Bernardi","Bonhams","Russell Kaplan","Stephan Welz","Strauss","Christies")  
    for(j in auc_house) {
        xy <- mean(as.numeric(y$ah_code==j))
        xym1 <- mean(as.numeric(ym1$ah_code==j))  
        b <- summary(model_100)$coefficients[grepl(j, rownames(summary(model_100)$coefficients)),1]
        bx <- b*(xym1-xy)   
        sbx <- sbx + bx
    }
    
    medium <- c("Watercolour","Oil","Acrylic","Print/Woodcut","Mixed Media","Sculpture","Photography","Other")  
    for(k in medium) {
        xy <- mean(as.numeric(y$med_code==k))
        xym1 <- mean(as.numeric(ym1$med_code==k))   
        b <- summary(model_100)$coefficients[grepl(k, rownames(summary(model_100)$coefficients)),1]
        bx <- b*(xym1-xy)   
        sbx <- sbx + bx
    }
    rep[i] <- (pym1/py)/exp(sbx)
}

for(i in 1:(max(artdata$rank_total))) { 
    artdata$reputation[(artdata$rank_total==i)] <- rep[i]
}

artdata$reputation <- as.numeric(unlist(artdata$reputation))
artdata$lnrep <- log(artdata$reputation)
```

##Estimation Results
```{r hedonicrep, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#------------------------------------------------------------------------------------------------------------------------------
#Load pre-calculated dataset (for speed) from API_3.R
artdata <- read.csv("artdata_lnrep.csv", header=TRUE)

#The result: index of average price per artist adjusted for quality, relative to the base artist 
#It can replace the artist dummies as a continuous variable in a second regression of equation 1 

#-------------------
# FULL SAMPLE MODEL
#-------------------
full_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                 "nr_works","artist","timedummy")) {
    if("artist" %in% list_expl_vars) { 
        modeldata <- subset(artdata, artdata$rank_all<max(artdata$rank_all,na.rm=TRUE))
    } else modeldata <- artdata
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    model_all <- lm(expl_vars, data=modeldata)
    time_results <- summary(model_all)$coefficients[grepl("time", rownames(summary(model_all)$coefficients)),1]
    time_results <- as.data.frame(time_results)
    time_results$index_all <- exp(time_results$time_results)*100
    return(time_results)
}

#-----------------------------
# OVERLAPPING PERIODS (1-year)
#-----------------------------
overlap1y_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                      "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:16) {
        if("artist" %in% list_expl_vars) {
            modeldata <- subset(artdata, artdata[,(44+i)]<max(artdata[,(44+i)],na.rm=TRUE))
        } else modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*4-5)& modeldata$counter<(i*4+1))
        model <- lm(expl_vars, data=modeldata)  
        res_list[[i]] <- summary(model)$coefficients[grepl("time", rownames(summary(model)$coefficients)),1]
    }
    #Merge all results
    if("artist" %in% list_expl_vars) {
        overlap <- time_results
    } else overlap <- rep_results
    overlap$time_results <- NULL
    overlap <- merge(overlap, res_list[[1]], by="row.names", all=TRUE)
    overlap[,3] <- exp(overlap[,3])*100
    for(i in 2:16) {
        overlap <- merge(overlap, res_list[[i]], by.x = "Row.names", by.y = "row.names", all=TRUE)
        overlap[,(i+2)] <- exp(overlap[i+2])*100
    } 
    #Calculate index
    overlap$ind <- overlap[,3]
    overlap[2,19] <- overlap[3,19]*overlap[2,2]/overlap[3,2]   #Interpolate
    overlap$teller <- c(3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,10,10,10,10,11,11,11,11,12,12,12,12,
                        13,13,13,13,14,14,14,14,15,15,15,15,16,16,16,16,17,17,17,17,18,18,18,18)
    for(i in 3:62) {
        j <- overlap[(i+1),20]
        if(is.na(overlap[i,j])) {
            overlap[(i+1),19] <- overlap[i,19]*overlap[(i+1),j]/100
        } else { 
            overlap[(i+1),19] <- overlap[i,19]*overlap[(i+1),j]/overlap[i,j] 
        }   
    }
    colnames(overlap) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5","Index_m6",
                           "Index_m7","Index_m8","Index_m9","Index_m10","Index_m11","Index_m12","Index_m13",
                           "Index_m14","Index_m15","Index_m16","Index_Adjacent1y","teller")
    overlap$Date <- c("2000Q2","2000Q3","2000Q4","2001Q1","2001Q2","2001Q3","2001Q4","2002Q1","2002Q2","2002Q3","2002Q4",
                      "2003Q1","2003Q2","2003Q3","2003Q4","2004Q1","2004Q2","2004Q3","2004Q4","2005Q1","2005Q2","2005Q3","2005Q4",
                      "2006Q1","2006Q2","2006Q3","2006Q4","2007Q1","2007Q2","2007Q3","2007Q4","2008Q1","2008Q2","2008Q3","2008Q4",
                      "2009Q1","2009Q2","2009Q3","2009Q4","2010Q1","2010Q2","2010Q3","2010Q4","2011Q1","2011Q2","2011Q3","2011Q4",
                      "2012Q1","2012Q2","2012Q3","2012Q4","2013Q1","2013Q2","2013Q3","2013Q4","2014Q1","2014Q2","2014Q3","2014Q4",
                      "2015Q1","2015Q2","2015Q3","2015Q4")
    overlap$Date <- factor(overlap$Date)
    return(overlap)
}

#-----------------------------
# OVERLAPPING PERIODS (2-year)
#-----------------------------
overlap2y_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                      "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:8) {
        if("artist" %in% list_expl_vars) {
            modeldata <- subset(artdata, artdata[,(60+i)]<max(artdata[,(60+i)],na.rm=TRUE))
        } else modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*8-9)& modeldata$counter<(i*8+1))
        model <- lm(expl_vars, data=modeldata)  
        res_list[[i]] <- summary(model)$coefficients[grepl("time", rownames(summary(model)$coefficients)),1]
    }
    #Merge all results
    if("artist" %in% list_expl_vars) {
        overlap2 <- time_results
    } else overlap2 <- rep_results
    overlap2$time_results <- NULL
    overlap2 <- merge(overlap2, res_list[[1]], by="row.names", all=TRUE)
    overlap2[,3] <- exp(overlap2[,3])*100
    for(i in 2:8) {
        overlap2 <- merge(overlap2, res_list[[i]], by.x = "Row.names", by.y = "row.names", all=TRUE)
        overlap2[,(i+2)] <- exp(overlap2[i+2])*100
    } 
    #Calculate index
    overlap2$ind <- overlap2[,3]
    overlap2[2,11] <- overlap2[3,11]*overlap2[2,2]/overlap2[3,2]   #Interpolate
    overlap2$teller <- c(3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,
                         7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10)
    for(i in 7:62) {
        j <- overlap2[(i+1),12]
        if(is.na(overlap2[i,j])) {
            overlap2[(i+1),11] <- overlap2[i,11]*overlap2[(i+1),j]/100
        } else { 
            overlap2[(i+1),11] <- overlap2[i,11]*overlap2[(i+1),j]/overlap2[i,j] 
        }   
    }
    colnames(overlap2) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5",
                            "Index_m6","Index_m7","Index_m8","Index_Adj2y","teller")
    overlap2$Date <- c("2000Q2","2000Q3","2000Q4","2001Q1","2001Q2","2001Q3","2001Q4","2002Q1","2002Q2","2002Q3","2002Q4",
                       "2003Q1","2003Q2","2003Q3","2003Q4","2004Q1","2004Q2","2004Q3","2004Q4","2005Q1","2005Q2","2005Q3","2005Q4",
                       "2006Q1","2006Q2","2006Q3","2006Q4","2007Q1","2007Q2","2007Q3","2007Q4","2008Q1","2008Q2","2008Q3","2008Q4",
                       "2009Q1","2009Q2","2009Q3","2009Q4","2010Q1","2010Q2","2010Q3","2010Q4","2011Q1","2011Q2","2011Q3","2011Q4",
                       "2012Q1","2012Q2","2012Q3","2012Q4","2013Q1","2013Q2","2013Q3","2013Q4","2014Q1","2014Q2","2014Q3","2014Q4",
                       "2015Q1","2015Q2","2015Q3","2015Q4")
    overlap2$Date <- factor(overlap2$Date)
    return(overlap2)
}

#----------------------
#ROLLING 5-YEAR WINDOWS
#----------------------

rolling_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                    "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:12) {
        if("artist" %in% list_expl_vars) {
            modeldata <- subset(artdata, artdata[,(32+i)]<max(artdata[,(32+i)],na.rm=TRUE))
        } else modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*4-4)&modeldata$counter<(i*4+17))
        model <- lm(expl_vars, data=modeldata)  
        summary(model)
        res_list[[i]] <- summary(model)$coefficients[grepl("time", rownames(summary(model)$coefficients)),1]
    }
    
    #Merge all results
    if("artist" %in% list_expl_vars) {
        rolling <- time_results
    } else rolling <- rep_results
    rolling$time_results <- NULL
    rolling <- merge(rolling, res_list[[1]], by="row.names", all=TRUE)
    rolling[,3] <- exp(rolling[,3])*100
    for(i in 2:12) {
        rolling <- merge(rolling, res_list[[i]], by.x = "Row.names", by.y = "row.names", all=TRUE)
        rolling[,(i+2)] <- exp(rolling[i+2])*100
    }    
    
    #Calculate index
    rolling$ind <- rolling[,3]
    rolling[2,15] <- rolling[3,15]*rolling[2,2]/rolling[3,2]  #interpolate
    rolling$teller <- c(3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,
                        10,10,10,10,11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14)
    for(i in 19:62) {  #chaining
        j <- rolling[(i+1),16]
        rolling[(i+1),15] <- rolling[i,15]*rolling[(i+1),j]/rolling[i,j]
    }
    
    colnames(rolling) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5","Index_m6",
                           "Index_m7","Index_m8","Index_m9","Index_m10","Index_m11","Index_m12","Index_Rolling","teller")
    rolling$Date <- c("2000Q2","2000Q3","2000Q4","2001Q1","2001Q2","2001Q3","2001Q4","2002Q1","2002Q2","2002Q3","2002Q4",
                      "2003Q1","2003Q2","2003Q3","2003Q4","2004Q1","2004Q2","2004Q3","2004Q4","2005Q1","2005Q2","2005Q3","2005Q4",
                      "2006Q1","2006Q2","2006Q3","2006Q4","2007Q1","2007Q2","2007Q3","2007Q4","2008Q1","2008Q2","2008Q3","2008Q4",
                      "2009Q1","2009Q2","2009Q3","2009Q4","2010Q1","2010Q2","2010Q3","2010Q4","2011Q1","2011Q2","2011Q3","2011Q4",
                      "2012Q1","2012Q2","2012Q3","2012Q4","2013Q1","2013Q2","2013Q3","2013Q4","2014Q1","2014Q2","2014Q3","2014Q4",
                      "2015Q1","2015Q2","2015Q3","2015Q4")
    rolling$Date <- factor(rolling$Date)
    return(rolling)
}

#========================================================================================
list_expl_vars <- c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed","dum_dated",  
                    "nr_works","lnrep","timedummy")

rep_results <- full_model(artdata,list_expl_vars)
suppressMessages(rep_overlap1 <- overlap1y_model(artdata,list_expl_vars))
suppressMessages(rep_overlap2 <- overlap2y_model(artdata,list_expl_vars))
suppressMessages(rep_rolling <- rolling_model(artdata,list_expl_vars))
```

The full pooled sample estimation results are reported in Table 2. The coefficients are all significant and have the expected signs.[^18] The size of the artwork is highly significant and positive, as is the sculpture interaction term. Bonhams and Strauss & Co are the auction houses with the highest average prices, after controlling for other factors, probably reflecting higher quality work and higher commission structures. Oil is the most expensive medium category. The authentication dummies are both positive and significant, as is the artist reputation variable. The number of works dummy indicates that more than one artwork in a lot leads to slightly lower prices per artwork. The adjusted $R^2$ is relatively high, suggesting that these variables capture a relatively large part of the variation in sales prices.

[^18]: The squared size term has the opposite coefficient (i.e. it is positive but should be negative), so I have excluded it for the time being. 

```{r table2, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
#Full model for regression results
list_expl_vars <- c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed","dum_dated",  
                    "nr_works","lnrep","timedummy")
expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+"))) 
modeldata <- artdata 
model_all <- lm(expl_vars, data=modeldata) 
stargazer(model_all, title = "Hedonic Regression results", omit=c("timedummy"), omit.labels = "Quarterly dummies", header=FALSE, single.row = TRUE, type = "latex")
```

The implicit prices of hedonic characteristics (e.g. tastes) may change over time [@Renneboog2012]. One way to allow for gradual shifts in parameters is to employ an adjacent-periods regression, in which the models are estimated using only sub-samples that are adjacent to each other. There are trade-offs in selecting the length of the estimation window. Shorter estimation windows decrease the likelihood of large breaks but also decrease the number of observations used to estimate the parameters [@Dorsey2010]. 

Two versions of the adjacent-period indices are estimated. Similar to @Dorsey2010 in the context of real estate, adjacent-period hedonic models for 1-year estimation windows are calculated. This seems to be reasonable for the South African art market, where large auctions are held relatively infrequently. To increase the estimation sample size, the models are also estimated for every 2-year period, which is similar to @Renneboog2012 in the context of art. The indices are then calculated by chain-linking the indices, as Figure 4 illustrates for the 2-year version of the index.[^19]

[^19]: We can exclude the figure if it is unnecessary?

```{r figure4, echo=FALSE, warning=FALSE, cache = TRUE, fig.height=4, fig.width=7.5, fig.cap="Chain-linked two-year adjacent period art price index"}
index_plot <- melt(rep_overlap2[,c(-2,-12)], id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank())
g 
```

In the context of real estate, @Shimizu2010 suggested a so-called overlapping-periods hedonic regression method using multiple "neighbourhood periods", allowing gradual shifts in the parameters. They estimated parameters by taking a certain length as the estimation window and shifting this period forward in rolling regressions. They argued that this method should be able to handle seasonal changes in parameters better than adjacent-periods regressions, although it may suffer more from the disadvantages associated with pooling. To apply this method, 5-year rolling regressions were run, which also corresponds to the rolling 5-year regression used to estimate the Citadel Art Price Index.[^20] The estimation window is then shifted forward one year, which allows gradual shifts in the parameters. 

[^20]: We can exclude all references to the CAPI?

The coefficients form these models are similar in magnitude to the full model and significant in virtually all cases. For example, the coefficient associated with the size of the artwork is 0.426 using the standard hedonic regression, while the average coefficients from the other regressions are 0.44, 0.43 and 0.42. However, there are a few cases in which the estimated parameters fluctuate quite substantially. For example, the coefficient of the Strauss auction house dummy varies between 1.04 in the pooled model and 0.77 in one the sub-samples, indicating that non-negligible structural changes might have occurred during the sample period.[^21]  

[^21]: We can include the average values of the coefficients in Table 2 if necessary? A formal Chow test can also be conducted to test for structural breaks in the coefficients. See Berndt (1991) for the use of Chow tests for hedonic functions.

Figure 5 illustrates the resulting quarterly art price indices from these four methods, using the continuous artist reputation variable. As one would expect, the indices follow a similar cyclical pattern and appreciated rapidly in the run-up to the financial crisis. The levels are slightly different, however, especially after the peak in 2008. These indices are also very similar to the indices based on the traditional time dummy method, which includes dummy for as many artists as possible, confirming the findings in @Kraussl2008. These hedonic measures also seem much more plausible than the simple central tendency measures, supporting the case for the regression-based measures.[^22] 

[^22]: This section could also include clustered standard errors, but would these change the index estimates themselves? For instance, Olckers et al (2015) suggest the use of robust or clustered standard errors. The auction results include the sales of multiple artworks by the same artist, which is likely to violate the assumption that all the observations are independent. There are likely to be unobservable characteristics associated with the artists, which will lead the error term of artworks by the same artist to be correlated. They cluster the errors by the artist to test the effect this may have on the significance of the independent variables. Clustering increases the standard error by a large amount for all the variables.

```{r figure5, echo=FALSE, cache = TRUE, fig.height=4, fig.width=7, fig.cap="Hedonic South African art price indices (2000Q1=100)"}
hedonic_indices <- rep_overlap1[,c(1,2)]
colnames(hedonic_indices) <- c("Date","Hedonic_Full")
hedonic_indices <- cbind(hedonic_indices,Adjacent_1y=rep_overlap1[,19])
hedonic_indices <- cbind(hedonic_indices,Adjacent_2y=rep_overlap2[,11])
hedonic_indices <- cbind(hedonic_indices,Rolling=rep_rolling[,15])

index_plot <- melt(hedonic_indices, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.position="bottom") + theme(legend.title=element_blank())
g 
```

All four of the indices peak in 2008Q1, which is before the peak in sales and annual median prices. Indeed, all four of the indices displayed quite dramatic increases in auction prices of more than 200% between 2003 and 2008. This conforms to the idea that there was a surge in the popularity of South African art market over the period, as well as the idea of the market overheating or forming a so-called bubble with this dramatic rise and subsequent decrease in prices.   

###Hedonic model extensions
Two additional hedonic model extensions were also applied to the data: stratified hedonic indices and quantile hedonic indices. The results are very similar to the previously reported hedonic results and are summarised here.[^23]

[^23]: We can exclude these results, as they just tell the same story?

@Eurostat2013 suggested the use of hedonic regressions at the stratum level to adjust for quality mix changes. This two-stage approach combines hedonics estimates at the lower stratum level and explicit weighting at the upper level to form an overall price index. This approach has the advantage of examining different price indices for different market segments, as it is possible that the different segments could exhibit different price trends. This process was followed by estimating hedonic models for each of the different mediums separately. The indices were then combined by weighting them by their corresponding sales value shares. Two stratified hedonic indices were estimated: the weighted average of the medium indices and the Fisher ideal price index. The results indicate that the stratified hedonic indices follow a similar cyclical pattern, but at a higher level that the full pooled sample index.

The South African art market may be segmented for a number of reasons. For instance, because art is indivisible, small investors are generally unable to invest in more expensive works. Wealthy individuals may be less tempted to buy at the lower end of the market, where works do not signal the same social status. The more expensive parts of the market may be more prone to speculation. The distribution of returns may therefore be skewed [@Renneboog2012]. Quantile regressions can be useful in such a case. Although OLS regressions provide estimates for the conditional means, quantile regressions can characterise the entire distribution of the dependent variable. The characteristic prices and the changes in price levels are allowed to vary across the distribution of auction prices. 

@Renneboog2012 used quantile regressions to show that historical rates of appreciation varied across the price distribution. They found larger average price appreciations and higher volatilities in the more expensive price brackets. High-end art appreciated faster during boom periods and decreased more during downturns. @Fedderke2014 suggested that the South African art market should be segmented into three price ranges and found different relationships for the three market segments. Quantile regressions were used to investigate this possibility, by estimating the hedonic models on the 75th, 50th, and 25th percentiles. The regression results indicated that a few of the variables had different impacts in different parts of the price distribution. For instance, the size of the artwork was more important in the higher quantiles. The resulting quantile art price indices exhibited very similar trends, although the lower end of the market seems to have depreciated slightly less after the peak in 2008. This could indicate that some people were being priced out of the higher end of the art market, increasing the lower end prices more than the higher end prices [@Els2014].

###Conclusion
All of the hedonic price indices displayed very similar trends over the period, with large price increases in the run-up to the Great Recession. However, the hedonic indices may be biased due to omitted variables. Ramsey RESET tests indicate misspecification of the model, which is to be expected, as there are many finer nuances that make artworks unique, e.g. finer classifications and interaction terms, which have not been included in the hedonic regression models. The omitted variables may bias the coefficients (if they are correlated with the other regressors), which in turn may bias the indices, although the bias is often small in practice [@Triplett2004; @Renneboog2012].

The following section estimates alternative art price indices using a hybrid methodology, which should be less prone to omitted variable bias. If the alternative method displays the same kind of trend and the same marked increase in prices as the hedonic indices, this should provide more confidence that the results are reasonable and robust to changes in methodology. 

#Hybrid Models: The Pseudo Repeat Sales Method
The limited number of repeat sales observations in the database (561)[^24] limits the usefulness of the classical repeated sales approach in this case. It leads to a very volatile index with a very large appreciation in prices over the period.[^25] The weakness of the classical repeat sales method is the limited sample size and sample selection bias caused by the need for repeat sales of the same asset. An alternative is to use the ''pseudo repeat sales'' (ps-RS) procedure recently introduced by @Guo2014 in the real estate literature. The ps-RS method can be applied here to supplement the repeat sales in the database, and in so doing allow the estimation of a variant of the repeat sale index, which mitigates some of the omitted variable bias of the hedonic model. 

[^24]: Unfortunately, the dataset does not uniquely identify each artwork. Repeat sales were identified by matching sales records using the artist name, title, size, medium, and the presence of a signature and a date.

[^25]: According to Ginsburgh, Mei & Moses (2006), the repeat sales method should not be used for time frames of less than 20 years, unless the number of repeated sales is large. We can still include the repeat sales index if necessary? It shows even larger appreciation than the other methods.

```{r repeatsales, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##=====================##
## REPEAT SALES METHOD ##
##=====================##
#check for duplicates (how many)
sum(duplicated(artdata[,c("artist","title","med_code","area","dum_signed","dum_dated")]))

allDup <- function (value)  { #identify duplicated values
    duplicated(value) | duplicated(value, fromLast = TRUE)
}
rsartdata <- artdata[allDup(artdata[,c("artist","title","med_code","area","dum_signed","dum_dated")]),]
rsartdata <- transform(rsartdata, id = as.numeric(interaction(artist,factor(title),med_code,factor(area),factor(dum_signed),
                                                              factor(dum_dated), drop=TRUE)))

repdata <- repsaledata(rsartdata$lnprice,rsartdata$counter,rsartdata$id)    #transform the data to sales pairs
repeatsales <- repsale(repdata$price0,repdata$time0,repdata$price1,repdata$time1,mergefirst=2,
                       graph=FALSE,graph.conf=TRUE,conf=.95)                 #generate the repeat sales index
repeatsales_index <- exp(as.data.frame(repeatsales$pindex))*100
repeatsales_index$Date <- c("2000Q4","2001Q1","2001Q2","2001Q3","2001Q4","2002Q1","2002Q2","2002Q4",
"2003Q1","2003Q2","2003Q4","2004Q1","2004Q2","2004Q4","2005Q1","2005Q2","2005Q3","2005Q4",
"2006Q1","2006Q2","2006Q3","2006Q4","2007Q2","2007Q3","2007Q4","2008Q1","2008Q2","2008Q3","2008Q4",
"2009Q1","2009Q2","2009Q3","2009Q4","2010Q1","2010Q2","2010Q3","2010Q4","2011Q1","2011Q2","2011Q3","2011Q4","2012Q1","2012Q2","2012Q3","2012Q4","2013Q1","2013Q2","2013Q3","2013Q4","2014Q1","2014Q2","2014Q3","2014Q4","2015Q1","2015Q2","2015Q3","2015Q4")
```

@Guo2014 used the ps-RS method to construct more reliable price indices for newly constructed homes in China. The ps-RS procedure was developed to deal with two unique features in the Chinese urban residential market. Firstly, new home sales accounted for a large share of total sales (87% in 2010). As a consequence there was a limited number of repeat sales, similar to the South African art market. Secondly, housing development in many high-density cities occurred with a high degree of homogeneity in the units built within the typical residential complex. The idea was then to match similar homes within each complex or building in order to construct a large pseudo repeat sales sample. They argued that the ps-RS procedure could produce a more reliable and accurate picture of home price appreciation in markets with these features.

This procedure is similar to the matched-sample procedure proposed by @McMillen2012. This approach views the repeat sales model as an extreme solution to a matching problem: it requires an exact match to estimate an index, e.g. the same Van Gogh *"Wheat Field with Crows"* is tracked over time, to take account all of the variation in attributes. The idea behind the ps-RS method (or imperfect matching) is that some assets may be similar enough to compare over time. For example, Van Gogh's well-known *Sunflowers* series, of which there are five versions, might be similar enough to be treated as repeat sales. The objective is to match, or pair together, individual sales observations across time, according to some criterion, so as to cancel out as much as possible of the unobservable attributes, making the model more parsimonious and robust [@Guo2014].

A hedonic matching criterion is used to create pseudo sales pairs and the repeat sales regression is then applied to these pseudo pairs. The approach is therefore a hybrid model of the type that has been demonstrated to have desirable features in the literature. It mitigates two of the main difficulties of these models: lack of repeat sales data and potential omitted variable bias. It mitigates the problems of small sample sizes and sample selection bias with repeated sales techniques by using more of the transaction data and reducing the probability that outliers will unduly affect the results [@McMillen2012]. This method has not been used in the art literature to date. The caveat is that even two artworks by the same artist of a similar size and in the same medium do not necessarily serve as close substitutes [@Olckers2015]. 

##Index construction methodology
Pseudo sales pairs can be generated when two non-simultaneous transactions share enough similarities in attributes. The pseudo repeat sales are generated for artist and medium pairs, i.e. within-artist/medium groups. A distance metric is used to identify the most similar transactions by an artist in the same medium across adjacent periods. For each artist, a hedonic model is estimated with physical attributes and time dummies. The distance metric is based on the predicted value for each artwork, excluding the time dummies (i.e. the non-temporal component). The distance metric between two sales is the absolute value of the difference between the two predicted log price values. The threshold for this distance metric can be customised. One option is to select pairs with the smallest value of the distance metric, which can be set to zero in this case. Alternatively, pairs with their distance metric smaller than a certain threshold can be selected.[^26] This is a trade-off between the within-pair homogeneity (for mitigating bias) and sample size (for reducing random errors).

[^26]: Guo et al. (2014) actually do this for each adjacent period, but I think the matches are too dissimilar in this case. Guo et al. (2014) select the lowest x% of the pairs with their distance metric smaller than a certain value y. They argue that it is easier and convenient to use x%, instead of y, to define this selection rule because for different adjacent-periods the exact distribution of the distance metric is different. For instance, they can select 20%, 40%, 60% or 80% of the pairs with their distance metric values lower than corresponding thresholds (they are also not very interested in the exact values of the distance metric thresholds). For instance, if they set x% to be equal to 100%, all within-building pairs will be kept, which returns every-adjacent-pair combination without any specific similarity threshold. I think this is too broad and my matching is a little stricter: I use pairs with very similar predicted values (small distance metrics) that are not necessarily from adjacent periods, which gives me very similar matched pairs in terms of the observable attributes. Again this is a trade-off between the within-pair ''similarity'' (higher similarity is good for mitigating bias) and the sample size (larger size is good for reducing random errors). I can change this if necessary, but the smallest say 20% seems just as arbitrary. We can also talk about larger distance metrics, but running time does increase exponentially.

Pseudo pairs are generated by matching each sale with its most adjacent subsequent sale. This is consistent with best practice in repeat sales estimation whenever a single item has more than two sales in the sample. This matching process may generate many more pairwise observations than the number of individual transactions in the sample. No unnecessary redundancy is created in the pseudo dataset, however, because each pseudo pair is unique.

For a given artist $j$, artwork $a$ in quarter $t=r$ and artwork $b$ in quarter $t=s$ are adjacent transactions $(s>r)$, and the two make a matched pair. The differential hedonic regression (ps-RS) model is expressed as:
$$\ln P_{bsj} - \ln P_{arj}=\sum_{k=1}^K\beta_k(X_{bsjk}-X_{arjk})+\sum_{t=0}^\tau\gamma_td_{t}+\epsilon_{srabj}$$
where $D_t$ is the dummy representing the time the sale occurs. $D_t=1$ if the later sale in the pair happened in quarter $t=s$; $D_t=-1$ if the former sale in the pair happened in quarter $t=r$; and $D_t=0$ otherwise. The $\epsilon_{srabj}$ term is the difference between the two error terms in the log prices of the two sales.

Within-pair first differencing will cancel out any variables for which the attributes are the same between the two units, including both observable and unobservable attributes. Only attributes that differ between the two units within a pair will be left on the right-hand side as independent variables, differenced between the second minus the first sale, reflecting the hybrid specification. This equation is then regressed over all the pseudo-pairs.[^27] The ps-RS indices are then calculated based on the coefficients of the time dummies. 

[^27]: In order to make an apples-to-apples comparison, Guo et al (2014) employed a weighted least squares (WLS) regression to estimate the ps-RS index, where the hedonic attribute weights over time will be the same as those in the hedonic index. But would this not just change the standard errors?

Thus, the pseudo-repeat sales approach addresses the problem of lack of repeat-sales data and to some extent the omitted variables problem in the hedonic method. The pseudo repeat sales pairs include the 561 true repeat sales, or perfect matches, where the model controls for all factors by taking first differences. The same is true when those omitted variables are the same for pseudo sales pairs, which supplement the true repeat sales. These pseudo-pairs should control for many of the unobservable (or difficult to measure) variables and interaction terms for a given artwork. For instance, it might control for interaction terms (e.g. artist and medium combinations) and squared terms, finer medium classifications (e.g. a specific artist's Print/Woodcuts might typically be linocuts), or attributes such as theme, style and material (e.g. oil paintings by a specific artist might typically have the same theme such as a portrait, or be on the same material such as canvas). In many cases these attributes will be removed by taking first differences and will no longer be omitted variables. 

##Pseudo repeat sales results
This section applies the ps-RS methodology to the South African art market. The hedonic specification is used to generate pseudo sales pairs, for three distance metrics. The larger the distance metrics the larger the pseudo repeat sample that is generated. Again, this is a trade-off between the within-pair homogeneity and sample size. The interest is not in the exact values of the distance metric thresholds and they are somewhat arbitrary. 

The first distance metric is set equal to zero, i.e. no differences in the observable hedonic attributes, which provides the ''purest'' sample in terms of homogeneity of the units. For the other two indices, pairs (for a given artist in a specific medium) are selected when the distance metric is below a threshold of 0.1% and 1%. These three distance metrics create samples of 17,684, 27,224 and 120,870 pseudo-sales pairs respectively. These samples are then used to estimate the hybrid repeat sales model and to generate the art price indices.

```{r psRSsample, echo=FALSE, eval= FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##------------------------------------------------------------------
##--------------- Pseudo Repeat Sales ------------------------------
##------------------------------------------------------------------
#The distance metric between any two sales (across the intervening time period) is the 
#absolute value of the difference between the two predicted hedonic log values. 
#The threshold for this distance metric can be customised. 
#At one extreme, one can choose to select only one pair with the smallest value of the distance metric. 

#match per artist by hedonic function
list_expl_vars <- c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed","dum_dated",  
                    "nr_works","timedummy")
expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))

ps.RSsample <- function(threshold) {
    teller <-0
    repdata <- data.frame()
    rartdata <- subset(artdata,artdata$rank_all<max(artdata$rank_all,na.rm=TRUE))  #exclude artists with one sale
    keep <- c("lnprice","artist","title","lnarea","ah_code","med_code","lnsculpt_area","dum_signed","dum_dated",  
              "nr_works","timedummy","counter")
    rartdata <- rartdata[,names(rartdata) %in% keep]
    rartdata <- rartdata[complete.cases(rartdata),]  
    
    #Rank total again for new sample
    rankings <- count(rartdata, artist)
    rankings$rank_total <- row_number(desc(rankings$n))
    rartdata <- merge(rartdata, rankings, by.x="artist", by.y="artist",all.x=TRUE)
    
    for(k in 1:max(rartdata$rank_total)) { 
        modeldata <- subset(rartdata, rartdata$rank_total==k)
        modeldata$med_code <- factor(modeldata$med_code)
        modeldata$ah_code <- factor(modeldata$ah_code)
        modeldata$dum_signed <- factor(modeldata$dum_signed)
        modeldata$dum_dated <- factor(modeldata$dum_dated)
        modeldata$timedummy <- factor(modeldata$timedummy)
        
        #if there is only one factor level => make it a zero
        if(length(levels(modeldata$med_code))==1)   { modeldata$med_code <- as.numeric(0) }
        if(length(levels(modeldata$ah_code))==1)    { modeldata$ah_code <- as.numeric(0) }
        if(length(levels(modeldata$dum_signed))==1) { modeldata$dum_signed <- as.numeric(0) }
        if(length(levels(modeldata$dum_dated))==1)  { modeldata$dum_dated <- as.numeric(0)   }
        
        if(length(levels(modeldata$timedummy))!=1) {   #if there were sales in more than one period
            model <- lm(expl_vars, data=modeldata, na.action = na.exclude)
            newdata <- modeldata
            newdata$timedummy <- newdata$timedummy[1]  #turn time dummy off by making them all the same
            modeldata <- cbind(modeldata,fitted=predict.lm(model,newdata=newdata))  #add predicted value
            
            for(i in 1:nrow(modeldata)) {  #give items below threshold the same id
                modeldata$id <- 0
                teller <- teller + 1
                modeldata$id[i] <- teller
                medium <- modeldata$med_code[i]
                modeldata$distance <- abs(modeldata$fitted[i]-modeldata$fitted)  #distance metric
                #distances <- cbind(distances,modeldata$distance)
                if(threshold=="nearest"){  #nearest match if same medium
                    #modeldata$id[(modeldata$distance==min(modeldata$distance[modeldata[,"id"]==0 & modeldata[,"med_code"]==medium],na.rm=TRUE))] <- teller
                    modeldata$id[(modeldata$distance==0 & modeldata[,"med_code"]==medium)] <- teller
                } else {                   #if same medium and distance smaller than threshold
                    modeldata$id[(modeldata$distance<threshold & modeldata[,"med_code"]==medium)] <- teller
                } 
                if(sum(modeldata[,"id"]!=0)>1) {
                    repdata <- rbind(repdata,modeldata[modeldata[,"id"]!=0, ])
                }
            }
        }
    }
    #transform the data to pseudo repeat sales pairs (for all attributes)
    fullrep <- cbind(repsaledata(repdata$lnprice,repdata$counter,repdata$id),
                     repsaledata(repdata$artist,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$title,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$lnarea,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$med_code,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$ah_code,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$lnsculpt_area,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$dum_signed,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$dum_dated,repdata$counter,repdata$id)[,4:5],
                     repsaledata(repdata$nr_works,repdata$counter,repdata$id)[,4:5])
    
    colnames(fullrep) <- c("id","time0","time1","price0","price1","artist0","artist1","title0","title1","area0","area1",
                           "med_code0","med_code1","ah_code0","ah_code1","sculpt0","sculpt1","sign0","sign1","date0","date1",
                           "nr0","nr1")
    return(fullrep)
}

ps.RSsample_1 <- ps.RSsample(0.01)
ps.RSsample_2 <- ps.RSsample(0.001)
ps.RSsample_n <- ps.RSsample("nearest") 
```

```{r psRS, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
ps.RSsample_1 <- read.csv("psRSsample_0.01.csv", header=TRUE, sep=",",na.strings = "NA", skipNul = TRUE)
ps.RSsample_2 <- read.csv("psRSsample_0.001.csv", header=TRUE, sep=",",na.strings = "NA", skipNul = TRUE)
ps.RSsample_n <- read.csv("psRSsample_n.csv", header=TRUE, sep=",",na.strings = "NA", skipNul = TRUE)

ps.RS <- function(fullrep) {
    dy <- fullrep$price1 - fullrep$price0
    timevar <- levels(factor(c(fullrep$time0, fullrep$time1)))
    nt = length(timevar)
    n = length(dy)
    xmat <- array(0, dim = c(n, nt - 1))
    for (j in seq(1 + 1, nt)) {
        xmat[, j - 1] <- ifelse(fullrep$time1 == timevar[j], 1, xmat[, j - 1])
        xmat[, j - 1] <- ifelse(fullrep$time0 == timevar[j],-1, xmat[, j - 1])
    }
    colnames(xmat) <- paste("Time", seq(1 + 1, nt))
    fit <- lm(dy ~ xmat + 0)
    
    fullrep$med_code0[is.na(fullrep$med_code0)] <- "Oil"
    fullrep$med_code1[is.na(fullrep$med_code1)] <- "Oil"
    fullrep$ah_code0[is.na(fullrep$ah_code0)] <- "Strauss"
    fullrep$ah_code1[is.na(fullrep$ah_code1)] <- "Strauss"
    
    darea <- fullrep$area1 - fullrep$area0
    
    med0 <- model.matrix(~fullrep$med_code0)
    med1 <- model.matrix(~fullrep$med_code1)
    dmed <- med1 - med0
    
    ah0 <- model.matrix(~fullrep$ah_code0)
    ah1 <- model.matrix(~fullrep$ah_code1)
    dah <- ah1 - ah0
    
    dsculpt <- fullrep$sculpt1 - fullrep$sculpt0
    dnr <- fullrep$nr1 - fullrep$nr0
    
    sign0 <- model.matrix(~fullrep$sign0)
    sign1 <- model.matrix(~fullrep$sign1)
    dsign <- sign1 - sign0
    
    date0 <- model.matrix(~fullrep$date0)
    date1 <- model.matrix(~fullrep$date1)
    ddate <- date1 - date0

    ps.RS <- lm(dy ~ darea + dah + dsculpt + dnr + dsign + ddate + xmat + 0)
    ps.RS_results <- summary(ps.RS)$coefficients[grepl("Time", rownames(summary(ps.RS)$coefficients)),1]
    ps.RS_results <- as.data.frame(ps.RS_results)
    ps.RS_results$index_all <- exp(ps.RS_results$ps.RS_results)*100
    ps.RS_results$pairs <- nrow(fullrep)
    
    return(ps.RS_results)
}

ps.RS_1 <- ps.RS(ps.RSsample_1)
ps.RS_2 <- ps.RS(ps.RSsample_2)
ps.RS_n <- ps.RS(ps.RSsample_n)

Dates <- c("2000Q2","2000Q3","2000Q4","2001Q1","2001Q2","2001Q3","2001Q4","2002Q1","2002Q2","2002Q3","2002Q4",
           "2003Q1","2003Q2","2003Q3","2003Q4","2004Q1","2004Q2","2004Q3","2004Q4","2005Q1","2005Q2","2005Q3","2005Q4",
           "2006Q1","2006Q2","2006Q3","2006Q4","2007Q1","2007Q2","2007Q3","2007Q4","2008Q1","2008Q2","2008Q3","2008Q4",
           "2009Q1","2009Q2","2009Q3","2009Q4","2010Q1","2010Q2","2010Q3","2010Q4","2011Q1","2011Q2","2011Q3","2011Q4",
           "2012Q1","2012Q2","2012Q3","2012Q4","2013Q1","2013Q2","2013Q3","2013Q4","2014Q1","2014Q2","2014Q3","2014Q4",
           "2015Q1","2015Q2","2015Q3","2015Q4")

ps.RS_1$Date <- Dates
ps.RS_2$Date <- Dates
ps.RS_n$Date <- Dates[-2]

rep_indices <- merge(repeatsales_index, ps.RS_1, by="Date", all=TRUE)
rep_indices <- merge(rep_indices, ps.RS_2, by="Date", all=TRUE)
rep_indices <- merge(rep_indices, ps.RS_n, by="Date", all=TRUE)
rep_indices <- rep_indices[,c(1,2,4,7,10)]
colnames(rep_indices) <- c("Date","Repeat Sales","ps.RS(1%)","ps.RS(0.1%)","ps.RS(nearest)")       
```

Figure 6 illustrates the three versions of the ps-RS indices. The indices point to similar trends in art prices over the sample period, although the "purest" pseudo repeat sales index (with the distance metric set to zero) is at a slightly higher level than the other two indices. The larger sample based on the higher distance metric does appear to noticeably reduce the volatility of the index. This is similar to the findings in @Guo2014, where they argued that larger estimation samples reduce the excess volatility caused by random estimation error in the time-dummy coefficients.[^28]

[^28]: I'm not clear on what exactly this means?

```{r figure6, echo=FALSE, cache = TRUE, warning=FALSE, fig.height=4, fig.width=7.5, fig.cap="Pseudo repeat sales South African art price indices (2000Q1=100)"}
index_plot <- melt(rep_indices[,-2], id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g 
```

###Conclusion
The pseudo repeat sales method allows us to estimate a hybrid version of the repeat sales model, by matching artworks by a specific artist in a specific medium to create a large sample of pseudo repeat sales. These pseudo repeat sales supplement the true repeat sales in the sample and allows the estimation of a hybrid version of the repeat sales model. This model controls for many of the potential omitted variables in the hedonic specifications, while reducing the sample selection bias of the classical repeat sales model. The results indicate that the indices are similar in terms of trend, echoing the findings in @Guo2014, and point to the same kind of marked increase in South African art prices as the hedonic indices. This provides more confidence that the results are reasonable and robust to changes in methodology. The following section evaluates and compares the different indices.

#Comparison and Evaluation
As all of these approaches involve estimating unknown parameters, there is no way to know with certainty which price index is closest to the truth [@McMillen2012]. This section will compare the different indices, in order to determine if they provide a consistent picture of price movements in the South African art market. It will also attempt to find out which model provide a better and more credible gauge of price movements in this specific case. 

##Comparison of the indices
The indices can be compared and evaluated in a number of ways. The first step is usually to compare the indices graphically. Figure 7 illustrates representative indices for the three methodologies: median values, the full sample hedonic index and the ps-RS(1%) index. The two regression-based approaches behave quite differently from a simple median. This implies that regression-based methods of adjusting for the changes in composition and quality of artworks sold provide better estimates of pure price changes for heterogeneous assets. These results confirm the finding for South African real estate in @Els2014.  

```{r figure7, echo=FALSE, cache = TRUE, warning=FALSE, fig.height=4, fig.width=7.5, fig.cap="Comparing South African art price indices (2000Q1=100)"}
all_indices <- cbind(hedonic_indices[-1],rep_indices[,c(-1,-2)])
all_indices <- rbind(c(seq(100,100, length.out = ncol(rep_indices))),all_indices)
all_indices[3,7] <- 100
all_indices <- cbind(all_indices,Median=naive_index$Index_Naive)
all_indices <- cbind(Date=naive_index$Date, all_indices)

index_plot <- melt(all_indices[,c(1,2,6,9)], id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g
```

The hedonic and ps-RS indices show remarkably similar trends over the sample period.[^29] This seems to imply that the theoretical concerns with the hedonic approach, regarding omitted variables and misspecification, may not be as much of a problem in this sample. Both measures indicate that the average price of a constant-quality artwork increased significantly between 2005 and 2007. All of the regression-based indices peaked in the first quarter of 2008, except for the "purest" ps-RS index (with zero distance metric), which exhibits a spike in 2010Q1. This shows that there is some consistency in the estimates from the different methodologies. The indices also showed troughs after the financial crisis, similar to other asset prices [@Shimizu2010]. 

[^29]: Confidence intervals of the time dummy estimates can be constructed to see if they are statistically different. It is then possible to examine whether there is any overlap between the confidence intervals. For instance, @Kraussl2008 constructed confidence intervals for the time dummy estimates as follows: $\exp(\gamma_t ± 2*\sigma_{\gamma t})$. We could also report the proportion of the sample over which the various measures are statistically different. 

Table 3 reports the correlations in the growth rates between the various indices. The significant positive correlations between the regression-based indices indicate that their general trends are similar, and are different from the simple median. Allowing for time variation in the hedonic coefficients does not materially affect the results and neither does the exact value of the distance metric chosen. 

```{r table3, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
source("corstarsl.R")
for(i in 2:ncol(all_indices)) {all_indices[,i] <- as.numeric(all_indices[,i]) }
ts.all_indices <- as.ts(all_indices[,-1],start =c(2000,1),end=c(2015,4),frequency=4) 

dl.indices <- as.data.frame(diff(log(ts.all_indices)))
xt <- xtable(corstarsl(dl.indices), caption="Correlations in DLogs")
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.8)
```

##Evaluating index smoothness
The "signature" of random error in time dummy coefficient estimation for price indices is that it imparts "noise" into the index. Two indicators are often useful to quantify a comparison of the relative amount of noise between two or more indices: the volatility and the first-order autocorrelation (AC(1)) in the index returns. The index volatility and AC(1) directly reflect the accuracy of the index returns. Other things being equal, the lower the volatility and the higher the AC(1), the more accurate and less noisy is the index. 

The volatility and AC(1) are signal-to-noise metrics, based directly on the index produced. @Guo2014 argued that such metrics are more appropriate for judging the quality of price indices than the more traditional approach based on the diagnostic metrics of the underlying regression (e.g. standard errors of the residuals). The regression residuals do not represent error in the price index, and hence do not directly reflect inaccuracy in the index returns.

Even if an index was perfectly accurate, measuring the central tendency of market price changes in each period, the regression would still have residuals and the time dummy coefficients might still have large standard errors, resulting simply from the dispersion of individual art prices around the central tendency. Moreover, when datasets become large, the regression diagnostics are often impressively good simply because of the size of the sample. This renders tests economic significance more interesting that tests of statistical significance. The signal-to-noise metrics directly reflect the economic significance of random error in the indices that are being compared. They are therefore more relevant indicators of index quality.

To explain the rationale, consider the simple model of random noise in the index: 
$$M_t=M_{t-1}+r_t$$ and $$I_t=M_t+\epsilon_t=\sum_{i=1}^tr_i+\epsilon_t$$
where $r_t$ (the log price difference) is the true return (the central tendency) of market art price in period t. The returns are arithmetically added across time to build the true market value level $M_t$ (in logs). $I_t$ is the index (beginning t periods ago at log value zero) as of the end of period t. $\epsilon_t$ is the index-level random error, the error that causes noise and therefore matters from the perspective of index users. It can be modelled as white noise, with zero mean and zero correlation. The noise, unlike true market volatility, does not accumulate over time. 

This leads to the formula for noise in the index return:
$$r_t^*=I_t-I_{t-1}=r_t+(\epsilon_t-\epsilon_{t-1})= r_t+\eta_t$$
where $r_t^*$ is the index return and $\eta_t$ is the noise component of the index return in period t. 

Based on this equation the standard deviation of the index return $\sigma_{r_t^*}$, which represents the volatility of the index (Vol) and the first order autocorrelation coefficient $\rho_{r^*}$ (AC(1)) can be derived as:
$$Vol=\sigma_{r_t^*}=\sqrt{\sigma_r^2+\sigma_\eta^2}$$ and $$AC(1)=\rho_{r^*}=(\rho_r\sigma_r^2-\sigma_\eta^2/2)/(\sigma_r^2+\sigma_\eta^2)$$ 
where $\sigma_r^2$ and $\sigma_\eta^2$ are the variance of the true return and the noise respectively, and $\rho_r$ is the 1st order autocorrelation coefficient of the true return (which is usually positive in real asset markets). 

Volatility is dispersion in returns over time. There is always true volatility as the true market prices evolve over time. The ideal price index filters out the noise-induced volatility to leave only the true market volatility. The noise (random error) in the index adds to volatility in the index, in addition to the true volatility, and therefore causes excess volatility in the index. This excess volatility brings down the 1st order autocorrelation (pure noise has an AC(1) value of -0.5). Less noise (lower $\sigma_\eta^2$) will lead to lower index volatility and higher AC(1) and better the estimation of market return each period. Thus, lower Vol or higher AC(1) will indicate a better quality art price index in the sense of less noise.

Table 4 reports these two metrics of index smoothness for the art price indices. The comparison suggests that that the regression-based indices are much smoother than the simple median. The volatility and autocorrelations of the regression-based measures are around the same size. The ps-RS(1%) index performs the best in terms of the noise metrics, with a lower volatility and a higher AC(1) in returns. This suggest that the large sample ps-RS has less noise than the hedonic indices, and the larger the matching space, the better performance in terms of noise reduction.[^30] However, the differences are not large.

[^30]: @Guo2014 suggest another test  of index quality in terms of minimising random error, which is based on the Hodrick & Prescott (HP) filter. The HP filter is a spline fitting procedure that divides a time series into smoothed trend and cyclical components. The idea is then to examine which index has the least deviation from its smoothed HP component. For each type of index the sum of squared differences between the index returns and its smoothed returns are calculated and compared. Moet ek dit ook doen?  

```{r table4, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
##Comparing index smoothness
# Check std dev or volatility en AC(1)
ac.1 <-numeric()
eval <- data.frame()
returns <- all_indices
for(i in 2:ncol(all_indices)) {
    for(j in 2:nrow(all_indices)) {
        returns[j,i] <- all_indices[j,i]/all_indices[j-1,i] - 1
    }
}
returns <- returns[-1,-1]

vol <- apply(returns,MARGIN=2, FUN=sd, na.rm=TRUE)
for(i in 1:ncol(returns)) {
    ac.1[i] <- acf(returns,na.action = na.pass, plot = FALSE, lag.max = 1)$acf[,,i][2,i]
}
eval <- cbind(vol=vol,ac.1=ac.1[1:8])
xt <- xtable(eval, caption="Smoothness Indicators")
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"))
```

##Comparing the art price indices to other asset prices
To check that the estimated South African art price indices provide reasonable results, they can be compared to available international art price indices and other South African assets over the sample period.[^31]

[^31]: This is supposed to address in part Rulof's comments on why auction prices increased to this extent. He asked whether there was a fundamental change in the art that came to market. From the observable characteristics it does not seem to be the case, but the indices control for those characteristics, and they tell us nothing about unobservable attributes. But we can always argue that the indices might not be representative of the entire art market, but that it at least captures price movements in the auction market.

Figure 8 illustrates the art price indices for the US, UK and France, calculated by Artprice, together with two representative South African art price indices. In contrast to the other art price indices, the South African art market index experienced a decrease at the start of the period. However, between 2005 and 2008 all of the art price indices experienced steep increases. The Top 500 Art Market index created by @Kraussl2010 also reflects this trend, with a sharp decline in 2008Q1, which they speculated may have been the consequence of the crisis. By the end of the period the international art price indices are around the same level as the South African art price index. This suggests that the estimated art price indices provide reasonable estimates of pure price changes over the period.   

```{r figure8, echo=FALSE, cache = TRUE, warning=FALSE, fig.height=4, fig.width=7.5, fig.cap="Comparing art price indices (2000Q1=100)"}
#Compared to other art markets
assets <- read.csv("Assets.csv", header=TRUE, na.strings = "", skipNul = TRUE)
index_plot <- cbind(all_indices[,c(1,2,6)],assets[,c(7,8,9)])
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g
```

@Renneboog2014 examined the extent to which art prices generated in Western auction markets moved together since the early 1970s. Despite the cross-country variation in long-term returns, art markets often displayed similar trends and almost all the correlations in returns were significantly positive, although in some cases the correlations were relatively low. In this case the correlations in levels are all positive and significant, but the correlations in returns are not significant over the whole sample period. This might be because the art auction markets covered in @Renneboog2014 were all in the developed Western world. 

Figure 9 compares the two representative art price indices with indices for other South African assets: the JSE All Share index, the ABSA House price index and the Bond index. Local asset prices have increased much more than art prices over the entire period, which suggests that the art price indices do not provide outlandish estimates of pure price changes over the period. The equity and property markets also peaked at around the same time as the art price indices. However, after declining for the first few years of the sample, the art price indices experienced more rapid price appreciation between 2005 and 2008 than the other assets.[^32] 

[^32]: I still need to update this graph if we want to keep it? 

```{r figure9, echo=FALSE, cache = TRUE, warning=FALSE, fig.height=4, fig.width=7.5, fig.cap="Comparing South African asset price indices (2000Q1=100)"}
index_plot <- cbind(all_indices[,c(1,2,6)],assets[,c(3,4,5)])
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g 
```

##Conclusion
This section has evaluated and compared the art price indices produced according to three methodologies: central tendency, hedonic and hybrid pseudo repeat sales models. The regression-based indices are significantly different from the central tendency measures and seem to produce better estimates of pure price changes. This is confirmed by the smoothness metrics and the consistent cyclical pattern indicated by the regression-based indices. This shows that regression-based methods are useful in producing quality-corrected price indices for heterogeneous assets.

Each of the regression-based methods employed above has strengths and weaknesses. The hedonic method may suffer from omitted variable bias, which would bias the indices, while the pseudo-repeat sales method may control for some of this omitted variable bias, but suffer more from possible sample selection bias. However, the regression-based indices seem to point to the same general trends in South African art prices, with a clear cyclical trend and a large increase in the run-up to the Great Recession. This increase was similar to international art price indices and traditional South African assets. The relatively consistent picture offers some confidence that the indices provide a relatively accurate measure of the price movements in the market. 

In the following section the indices will be used to ascertain whether there is evidence for the presence of a bubble in the market over this period. In answering this question, the paper turns to literature on the bubble detection. The bubble detection tests proposed by @Phillips2011 will be applied to the indices to investigate whether South African art prices exhibited mildly explosive behaviour over the period.[^33]

[^33]: Ons moet dit oorweeg om die paper in twee te split. Ons kan hierdie eerste helfte maklik uitbrei, deur bv. meer extensions te doen en meer toetse as evaluation te gebruik. Mens kan die tweede deel dan ook maklik uitbrei deur van die ander bubble detection toetse ook te probeer en meer navorsing oor moontlike redes vir die bubble periode te ondersoek.

#Bubble Detection Methods
Record prices for South African artworks at local and international auctions, especially between 2008 and 2011, prompted many commentators at the time to claim that the market was overheating and suggest the possibility of a "bubble" in the market [e.g. @Rabe2011; @Hundt2010; @Curnow2010]. According to the indices generated above there was a substantial increase in South African art prices in the run-up to the Great Recession, with a peak around 2008Q1. This section asks whether art prices exhibited bubble-like behaviour over the sample period. Over this period, both advanced and emerging economies experienced severe financial crises. @Yiu2013 argued that these crises were triggered by the collapse of bubbles in asset prices. The adverse effects of bubbles and their related crises have led to a large literature on bubble detection.  

##Explosive prices
The most commonly used detection methods are based on the present value model and the rational bubble assumption. According to the present value model, under rational expectations, the price of an asset is equal to the present value of its future income stream (i.e. the expected fundamental value): $$P_t = \frac{1}{1+r_f}E_t(P_{t+1} + \gamma_{t+1})$$, where $R_f$ is the constant discount rate, $P_{t+1}$ is the asset price at time t, and $\gamma_{t+1}$ is the payoff received (e.g. dividends, rents or a convenience yield) for owning the asset between $t$ and $t+1$. When $t+n$ is far into the future, $\frac{1}{1+r_f}E_t(P_{t+n})$ does not affect $P_t$ as it tends to zero as n becomes infinitely large. Thus, the present value (or market fundamental solution) could be written as: $$F_t = E_t[\Sigma_{i=1}^n \frac{1}{1+r_f} (\gamma_{t+n})]$$ 

Rational bubbles arise when investors are willing to pay more than the fundamental value to buy an asset because they expect that the asset price will significantly exceed its fundamental value in the future. When rational bubbles are present, the asset price is composed of the fundamental component and a bubble component [@Yiu2013]. In other words, if a gap between the market fundamental solution and the actual price exists and, therefore, the terminal condition does not hold, an additional "bubble component", $B_t$, has to be added to the solution of equation: $P_t = F_t + B_t$. In this case $F_t$ is often called the fundamental component of the price and $B_t$ is any random variable that satisfies the following condition: $$B_t = \frac{1}{1+r_f}E_t(B_{t+n})$$ 

Hence, the bubble component is included in the price process, and anticipated to be present in the next period with an expected value of $(1 + r_f)$ multiplied by its current value. Being in line with the rational expectations framework, the bubble component is called a "rational bubble" [@Kraussl2014a].

The statistical properties of $P_t$ are determined by those of $F_t$ and $B_t$. In the absence of a bubble, when $B_t=0$, the degree of non-stationarity in $P_t$ is controlled by the nature of the series $F_t$, which in turn is determined by the properties of $\gamma_t$. Thus, the current price of the commodity is determined according to market fundamentals. For example, if $\gamma_t$ is an I(1) process then $P_t$ would an I(1) process. 

In the presence of bubbles, if $B_t \neq 0$, current prices $P_t$ will exhibit explosive behaviour, as $B_t$ reflects a stochastic process in which the expected value of next period's value, as forecast on the basis of the current period's information, is greater than or equal to the current period's value [@Kraussl2014a]. Thus, in the absence a structural change in the fundamental process or explosiveness in the fundamentals, a period of explosive prices must have a non-fundamental explanation. Under the assumed conditions on $\gamma_t$, the observation of mildly explosive behaviour in $P_t$ (i.e. non-stationarity of an order greater than unit root non-stationarity) will offer evidence of bubble behaviour. This expression embodies an explosive property and introduces "bubble" movements in the price over the fundamental component [@Areal2013]. Thus, the theory predicts that if a bubble exists, prices should inherit its explosiveness property. This in turn enables formulating statistical tests which aim at detecting evidence of explosiveness in the data [@Caspi2013].

Given the different stochastic properties of the fundamental and the bubble components, early tests were based on unit root and cointegration tests. @Campbell1987 suggested a unit root test for explosiveness in prices, based on the idea that the gap between the asset price and the fundamental value will exhibit explosive behaviour during the process of bubble formation. They identified two scenarios that strongly suggest the presence of a rational bubble. In the first case, the asset price is non-stationary while the fundamental value is stationary. In the second, the asset price and fundamental value are both non-stationary [@Yiu2013]. In this case, if the asset price and its fundamental value are co-integrated, their non-stationary behaviour does not point to a bubble. @Diba1988 showed that if fundamental values are not explosive, then the explosive behaviour in prices is a sufficient condition for the presence of bubble. 
However, unit root and cointegration tests are not capable of detecting explosive prices when a series contains periodically collapsing bubbles. @Evans1991 argued that explosive behaviour is only temporary in the sense that bubbles eventually collapse and that asset prices may appear more like I(1) or even stationary series than an explosive series, thereby confounding empirical evidence. Using simulated data @Evans1991 showed that these tests could not differentiate between a periodically collapsing bubble and a stationary process. A series containing periodically collapsing bubbles could therefore be interpreted by the standard unit root test as a stationary series, leading to an incorrect conclusion that the data contained no explosive behaviour [@Phillips2011]. 

A number of methods have been proposed to deal with this critique [@Yiu2013]. The recursive tests proposed by @Phillips2011 and @Phillips2012 are not subject to this criticism and can effectively distinguish unit root processes from periodically collapsing bubbles, as well as date-stamp their origin and collapse. The tests proposed by @Phillips2011 are based on the idea of repeatedly implementing a right-tailed unit root test. The method involves the estimation of an autoregressive model, starting with a minimum fraction of the sample and repeatedly expanding the sample forward.  

The model typically takes the following form:
$$\Delta y_t = \alpha_w + (\rho_w-1)y_{t-1}+\sum_{i=1}^k\phi_w^i \Delta y_{t-i} + \epsilon_t $$
where $y_t$ is the asset price series, $\alpha$, $\rho$ and $\phi$ are the parameters to be estimated, $w$ is the sample window size, $k$ is the lag order, and $\epsilon$ is the error term. 

A sample of Augmented Dickey-Fuller test statistics are calculated from each regression. The null hypothesis of a unit root $(\rho = 1)$ is tested against the right-tailed alternative of mildly explosive behaviour $(\rho > 1)$. The sup value of the ADF sequence is then used to test for mildly explosive behaviour. By looking directly for evidence of explosive behaviour, the test avoids the risk of misinterpreting a rejection of the null hypothesis due to stationary behaviour. 

The method also allows date-stamping of the origination and terminations dates by matching the time series of the recursive test statistics to the critical value sequence. In other words, each element of the estimated ADF sequence is compared to the corresponding right-tailed critical values of the ADF statistic to identify a bubble period. The estimated origination point of a bubble is the first observation in which ADF value crosses the corresponding critical value (from below), while the estimated termination point is the first observation thereafter when the ADF value crosses below the critical value [@Caspi2013]. 

A limitation of the method is that it is designed to analyse a single bubble episode. @Phillips2012 expanded the method to account for the possibility of multiple bubbles. The sample is extended by varying both the starting and ending points of the sample over a feasible range of windows. The moving window provides greater flexibility in choosing a subsample that contains a bubble [@Yiu2013]. Thus, the method of @Phillips2011 is consistent and particularly effective when there is a single explosive episode in the data, while the method of @Phillips2012 can identify multiple explosive episodes. Simulations by @Homm2012 indicated that the procedure worked satisfactorily against other time series tests for the detection of bubbles and was particularly effective for real-time bubble detection.

@Phillips2011 originally implemented the test for stock prices, but since then a number of studies have used this method to investigate bubbles in a number of asset markets, including real estate, commodities and art. For instance, @Jiang2014 employed the method to detect explosive periods in real estate prices in Singapore. The results suggested the existence of an explosive period from 2006Q4 to 2008Q1. @Balcilar2015 used the method to date-stamp periods of US housing price explosiveness for the period 1830-2013 and found evidence of several bubble periods. @Figuerola2015 applied the method to examine the recent behaviour of non-ferrous metals futures prices on the London Metal Exchange. They found that some commodity futures markets were prone to bubble-like phenomena and that the majority of the bubbles occurred between August 2007 and July 2008, around the time of the financial crisis. @Areal2013 used the methodology to test for the presence of periods of explosive prices in agricultural markets and found that bubbles occurred for some of the commodities, especially around 2007 and 2008.

@Kraussl2014a used the method to detect explosive behaviour in the prices of four different art market segments (*Impressionist and Modern*, *Post-war and Contemporary*, *American* and *Latin American*). They found evidence of explosive behaviour in prices and identified historical bubble episodes in the "*Post-war and Contemporary*" and "*American*" art market segments, around 2006-2008 and 2004-2008 respectively. 

##Fundamentals
Bubbles are generally defined as high-volume trading of a given category of assets at prices that are far above their intrinsic or fundamental values [@Kraussl2014a]. The question is therefore whether price surges are generated by market fundamentals or by other drivers such as trend-following behaviour [@Areal2013]. The problem then becomes whether one can observe the fundamental value of the asset. In the case of stocks, dividends have been used to obtain the expected cash flow as a measure of fundamental value. For example, @Phillips2011 related stock prices to dividend yields and showed that there was evidence of explosiveness in the former but not the latter, implying that price explosiveness could not be explained by developments in fundamental. Rents and convenience yields could potentially be used for real estate prices and commodity prices [@Penasse2014].  

However, it is particularly challenging to determine the fundamental value of artworks from which prices potentially deviate. Artworks usually have little inherent value, unless the materials used have a high intrinsic value [@Goetzmann2014]. Artworks do not generate a future income stream (e.g. dividends or rents) that can be discounted to determine the fundamental value. Instead, artworks are acquired for a kind of non-pecuniary aesthetic or utility dividend (sometimes described as "aesthetic pleasure"). This dividend can be seen as the rent one would be willing to pay to own the artwork over a given time frame. It can reflect aesthetic pleasure, but may also provide additional utility as the signal of wealth [@Mandel2009]. The price of an artwork should equal the present value of these future private utility dividends over the holding period, plus the expected resale value. The value of this dividend is of course unobservable and is likely to vary tremendously across collectors [@Penasse2014]. Thus, reasons closely dependent on the motivations and characteristics of owners make it almost impossible to clearly determine the fundamental value of art [@Kraussl2014a].  

To overcome this issue with determining the fundamental value of the asset, many authors have resorted to the use of deflated prices. For instance, due to a lack of data on historical oil price derivatives to calculate a convenience yield, @Caspi2013 identified periods where oil prices deviate from the general price level in the US, as well as levels of oil inventory supplies. Similarly, @Balcilar2015 use of the general price level to identify house price fundamentals. The implicit assumption is that house prices tend to reflect general movements in prices across the economy. @Kraussl2014a also used real art price indices to perform the tests for mildly explosive behaviour. The following section will follow this convention and use the log value of real art price indices, deflated with the CPI. 

The aesthetic dividends fluctuate over time as they depend on buyers' willingness to pay for art, which in turn depends on preferences and wealth. However, in order to explain the observed art price volatility, preferences regarding art and culture would have to fluctuate dramatically. Even if fads can temporarily emerge for some specific artists or schools of art, the previous literature has shown that tastes tend to be very stable, even in the long run [@Penasse2014]. The aesthetic dividend can also change as wealth (as well as marginal utility and willingness to pay) fluctuates over time [@Goetzmann2014]. The literature has provided evidence supporting this idea, for example, with @Goetzmann2011 finding cointegrating relationships between top incomes and art prices. However, it is unlikely that the aesthetic dividends (and the factors like preferences and income levels) experienced similar explosive behaviour over the period.[^34] 

[^34]: To test this we could test income levels (especially of the top income brackets) directly, or relate art prices to income levels, as this should be an obvious driver of art prices. 

#Bubble Detection Results
This section tests whether the South African art market has exhibited bubble-like behaviour over the sample period, focusing on a specific aspect of bubbles: explosive prices. In this case there is only one potential bubble episode, so the @Phillips2011 method should be sufficient to provide consistent evidence of mildly explosive behaviour. 

As explained above, the method involves the estimation of an autoregressive model, starting with a minimum fraction of the sample and repeatedly expanding the sample forward. We start with 3 years (i.e. 12 observations) and expand the sample by one observation each time. Each estimation yields an ADF statistic. As results might be sensitive to model formulation, two versions of the autoregression models are used: one without a constant or trend and one with a constant or drift term. The k lags are included to take possible autocorrelation of the residuals into account and the number of lags are chosen with the Akaike Information Criterion. Critical values for the tests are derived from Monte Carlo simulations of a random walk process (without and with drift), with 2000 replications. In their original study @Phillips2011 use a random walk without drift to estimate the null.[^35] 

[^35]: According to @Phillips2014, when the model is estimated with a non-zero drift it produces a dominating deterministic component that has an empirically unrealistic explosive form. They argue that these forms are unreasonable for most economic and financial time series and an empirically more realistic description of explosive behaviour is given by models formulated without a constant or a deterministic trend. In their latest update, Phillips et al. (2014) suggest a random walk process with an asymptotically negligible drift (localising drift parameter). We can include this if necessary? They suggest it might be useful in allowing for intermediate cases where there may be drift in the data, but it may not be the dominating component (when including a constant, the $y_t$ behaviour will be dominated by the deterministic trend).

```{r bubbles, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#==============================#
# Bubbles: Explosive Behaviour
#==============================#
#Make them real
real_indices <- all_indices
for(i in 2:ncol(all_indices)) { 
    for(j in 1:64) {
        real_indices[j,i] <- all_indices[j,i]/assets$CPI[j]*100 
    }
}

#Calculate test statistics
y_indices <- log(real_indices[,-1])
bubble.nc <- list()
bubble.c <- list()
for(i in 1:ncol(y_indices)) {
    bubble1 <- numeric()
    bubble2 <- numeric()
    for(j in 12:64) {
        y <- y_indices[1:j,i]
        #toets1 <- adf.test(y, alternative = "explosive", k=4)
        #bubble1 <- rbind(bubble1,toets1$statistic)
        toets1 <- ur.df(y, type= "none", lags = 4, selectlags = c("AIC"))
        toets2 <- ur.df(y, type= "drift", lags = 4, selectlags = c("AIC"))
        #toets <- ur.df(y, type= "trend", lags = 4, selectlags = c("AIC"))
        bubble1 <- rbind(bubble1,toets1@teststat)
        bubble2 <- rbind(bubble2,toets2@teststat)
    }
    bubble.nc[[i]] <- bubble1
    bubble.c[[i]] <- bubble2
}

##--------------------------------------------------------------------------
#Calculate critical values
K1 <- numeric()
K2 <- numeric()
K3 <- numeric()
K4 <- numeric()

for(j in 12:64) {
    set.seed(123)                           #for replicability
    reps <- 2000                            #Monte Carlo replications
    burn <- 100                             #burn in periods: first generate a T+B sample
    #obs <- 62                              #To make "sure" that influence of initial values has faded
    obs <- j                                #ultimate sample size
    tstat.nc <- numeric()
    tstat.c <- numeric()
    tstat.ct <- numeric()
    tstat.lc <- numeric()
    
    for(i in 1:reps) {     
        e <- rnorm(obs+burn)
        e[1] <- 0
        Y1 <- cumsum(e)
        DY1 <- diff(Y1)
        
        y1 <- Y1[(burn+1):(obs+burn)]               #trim off burn period
        dy1 <- DY1[(burn+1):(obs+burn)]             
        ly1 <- Y1[burn:(obs+burn-1)] 
        trend <- 1:obs

        EQ1 <- lm(dy1 ~ 0 + ly1)       
        tstat.nc <- rbind(tstat.nc,summary(EQ1)$coefficients[1,3]) 
        EQ2 <- lm(dy1 ~ ly1)            
        tstat.c <- rbind(tstat.c,summary(EQ2)$coefficients[2,3])  
        EQ3 <- lm(dy1 ~ lag(y1) + trend)    
        tstat.ct <- rbind(tstat.ct,summary(EQ3)$coefficients[2,3]) 
    }                                       
    #hist(tstat.nc)
    K1 <- rbind(K1,quantile(tstat.nc, probs=c(0.9,0.95,0.99)))
    K2 <- rbind(K2,quantile(tstat.c, probs=c(0.9,0.95,0.99)))
    K3 <- rbind(K3,quantile(tstat.ct, probs= c(0.9,0.95,0.99)))
}   #Provides a vector of critical values

##---------------------------------------------------------------------------
#plot die test stats en critical values 
bubble.test1 <- numeric()
bubble.test2 <- numeric()

for(k in 1:8) { 
    bubble.test1 <- cbind(bubble.test1,bubble.nc[[k]])
    bubble.test2 <- cbind(bubble.test2,bubble.c[[k]][1:53])
}
bubble.test1 <- as.data.frame(bubble.test1)
bubble.test2 <- as.data.frame(bubble.test2)
bubble.test1 <- cbind(bubble.test1,K1)
bubble.test2 <- cbind(bubble.test2,K2)

Dates <- c("2002Q4","2003Q1","2003Q2","2003Q3","2003Q4","2004Q1","2004Q2","2004Q3","2004Q4","2005Q1","2005Q2","2005Q3","2005Q4",
           "2006Q1","2006Q2","2006Q3","2006Q4","2007Q1","2007Q2","2007Q3","2007Q4","2008Q1","2008Q2","2008Q3","2008Q4",
           "2009Q1","2009Q2","2009Q3","2009Q4","2010Q1","2010Q2","2010Q3","2010Q4","2011Q1","2011Q2","2011Q3","2011Q4",
           "2012Q1","2012Q2","2012Q3","2012Q4","2013Q1","2013Q2","2013Q3","2013Q4","2014Q1","2014Q2","2014Q3","2014Q4",
           "2015Q1","2015Q2","2015Q3","2015Q4")
bubble.test1$Date <- Dates
bubble.test2$Date <- Dates

colnames(bubble.test1)[1:8] <- c("Hedonic_Full","Adjacent_1y","Adjacent_2y","Rolling","ps.RS(1%)","ps.RS(0.1%)","ps.RS(nearest)","Median")
colnames(bubble.test2)[1:8] <- c("Hedonic_Full","Adjacent_1y","Adjacent_2y","Rolling","ps.RS(1%)","ps.RS(0.1%)","ps.RS(nearest)","Median")
```

The sup ADF test statistics indicate that there was a period of mildly explosive behaviour for each of the indices (according to both formulations), except for the simple median index. Therefore, the null hypothesis of a unit root is rejected in favour of the alternative hypothesis for each of the indices (except the median index). This provides evidence that real art prices experienced periods of explosiveness for the full sample.

This method can now be used to date stamp potential bubble periods. Figure 10 and Figure 11 illustrate the date stamping procedure for three representative series: the full sample hedonic index, the ps-RS(1%) index and the simple median index. Figure 10 illustrates the case of no drift term, while Figure 11 illustrates the case with a drift term. The figures compare the ADF test static sequence to the 95% and 99% critical value sequences. In both cases the test statistic sequences breach the 95% critical values around the same period, before falling below the critical values. The sequence of test statistics for the ps-RS(1%) index is higher than for the full sample hedonic index, and breaches the 99% critical value. This provides evidence of an explosive root over the sample period.

```{r figure10, echo=FALSE, message=FALSE, warning=FALSE, cache = TRUE, fig.height=4, fig.width=7.5, fig.cap="Test statistics and critical values for models with no drift of trend"}
index_plot <- bubble.test1[,c(1,5,8,10,11,12)]
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g
```

```{r figure11, echo=FALSE, message=FALSE, warning=FALSE, cache = TRUE, fig.height=4, fig.width=7.5, fig.cap="Test statistics and critical values for models with drift"}
index_plot <- bubble.test2[,c(1,5,8,10,11,12)]
index_plot <- melt(index_plot, id="Date")  # convert to long format
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line()
g <- g + ylab("Index")
g <- g + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g
```

Table 5 reports the origination and termination dates for all of the periods of explosive behaviour, based on a 95% critical value. The test statistic sequences for the hedonic indices all indicate a period of explosive prices beginning around 2006/2007 and ending in 2008. The test statistics for the pseudo-repeat sales indices all indicate periods of explosive behaviour that were slightly longer, beginning around 2005/2006 and ending in 2008/2009. @Phillips2012 recommend that only explosive periods lasting more than log(T) units of time should be identified as bubble periods. In this case this implies that the bubble should be at least 4 quarters in length. Virtually all of the explosive periods identified conform to this requirement. 

```{r dates, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#report bubble period dates
datum1 <- data.frame()
datum2 <- data.frame()
datums1 <- data.frame()
datums2 <- data.frame()

for(i in 1:7) {
    for(l in 1:53) {
        if(bubble.test1[l,i]>bubble.test1$"95%"[l]) { 
            datum1[l,i] <- bubble.test1[l,"Date"]
        }
        if(bubble.test2[l,i]>bubble.test2$"95%"[l]) { 
            datum2[l,i] <- bubble.test2[l,"Date"]
        }
    }
    NonNAindex <- which(!is.na(datum1[,i]))
    firstNonNA <- min(NonNAindex)
    datums1[1,i] <- datum1[firstNonNA,i]
    if (NonNAindex[NROW(NonNAindex)-1]==(max(NonNAindex)-1)) { 
         lastNonNA <- max(NonNAindex)
    } else lastNonNA <- NonNAindex[NROW(NonNAindex)-1]
    datums1[2,i] <- datum1[lastNonNA,i]
    
    NonNAindex <- which(!is.na(datum2[,i]))
    firstNonNA <- min(NonNAindex)
    datums2[1,i] <- datum2[firstNonNA,i]
    lastNonNA <- max(NonNAindex)
    datums2[2,i] <- datum2[lastNonNA,i]
}    
    
datums <- rbind(datums1,datums2)
colnames(datums) <- colnames(bubble.test1)[1:7]
rownames(datums) <- c("None-Start","None-End","Drift-Start","Drift-End")
datums <- t(datums)
```

```{r table5, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
xt <- xtable(datums, caption="Dates of explosive behaviour")
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"))
```

The dates identified correspond with many of the explosive periods identified in the literature for a range of assets. In the context of art, @Kraussl2014a, identified a bubble period for the "*Post-war and Contemporary*" art segment between 2006 and 2008 and for the "*American*" art segments between 2005 and 2008, with peaks in 2007 (which also corresponds to the pre-financial crisis period). Interestingly, their data pointed to evidence in the formation of another bubble in these market segments around the start of 2011. This is not present in the South African art market, which has remained relatively flat since the 2010. It is also interesting that many of the headline grabbing auction records for the South African art market occurred in 2011, after the period of explosive behaviour. This corresponds to findings by @Goetzmann2014, who observed long periods of (average) price increases without auction records.

##Discussion
This section has applied the reduced-form bubble detection method developed by @Phillips2011 to test for periods of explosive behaviour in the art price indices. The use of recursive tests enables the identification of mildly explosive subsamples of data and detect, as well as date stamp, periods of exuberance [@Phillips2011]. The results indicate that there is evidence of bubble-like behaviour in all of the regression-based art price indices. Again, this implies that it is important to control for quality when estimating indices like these. The regression-based indices provide remarkably consistent results in terms of the explosive periods in the South African art market, with a potential bubble beginning around 2006 and ending around 2008.[^36]

[^36]: I have a meeting with Emma Bedford, Strauss & Co's head of fine arts, to get their take on developments in the market over the period.

This section has not attempted to identify explicit sources of exuberance in art prices. @Caspi2014 pointed out that although this method provides an efficient and consistent basis for identifying periods of explosive behaviour, it provides no causal understanding of these periods. Indeed, this bubble detection method is compatible with several different explanations, including rational bubbles, herd behaviour, and exuberant and rational responses to fundamentals [@Phillips2011].

The periods of explosive prices could be compatible with a rational bubble, where investors are willing to pay more than their private value for an artwork, because they expect to resell later at a larger price. @Gerard-Varet1995 argued that the sharp rise in world art prices in the late 1980s could be explained by a rational bubble, where investors believed that although prices had attained unsustainable levels the short run, prospects for continued gains were sufficient to compensate for the risk that the bubble might burst. Prices increase at an accelerating rate because the probability of a crash increases and rational investors require an ever rising risk premium to cover this rising probability of a crash [@Rosser2012].

Investors might think that an artwork that they would normally consider too expensive is now an acceptable purchase because they will be compensated by significant further price increases. During a bubble investors may also worry that if they do not buy now, they will not be able to afford the purchase later. Furthermore, the expectation of large price increases may have a strong impact on demand if investors think that prices are unlikely to fall, or not likely to fall for long, so that there is little perceived risk associated with a purchase [@Case2003].

@Penasse2014 argue that limits to arbitrage induce a speculative component to art prices. High transaction costs and short-selling constraints could lead to prices diverging from fundamental levels, as they prevent arbitrageurs from pulling back prices to fundamentals [@Balcilar2015]. When prices are high, pessimists would like to short sell, but instead simply stay out of the market or sell to optimists at inflated prices. Optimists may be willing to pay higher prices than their own valuations, because they expect to resell to even more optimistic investors in the future. The difference between their willingness to pay and their own optimistic valuation is the price of the option to resell the asset in the future. The price of the resale option imparts a bubble component in asset prices, and can explain price fluctuations unrelated to fundamentals. Thus, these market failures hamper the ability of markets to correct price inefficiencies and implies that periods of bubble-like behaviour could exist with relatively little scope for arbitrage. This is especially applicable to art markets, where transaction costs are very high and short selling is not possible and, absent a rental market, the only possibility to make a profit is by reselling at a higher price [@Penasse2014]. 

@Penasse2014 investigated this theory by looking at the behaviour of art prices and volumes. They found that the art market was subject to frequent booms and busts in both prices and volumes. They showed that volume was mainly driven by short-term transactions, which was interpreted as speculative transactions or trading frenzies. Given the huge transaction costs that characterise the art market, it is unlikely that these works of art were bought for the pure aesthetic pleasure. The positive correlation between prices and volumes was persistent across art movements, and was larger for the most volatile segments of the art market (the works of Modern and Contemporary artists). When trading volume was high, they found that buyers tended to overpay, in that high volume strongly predicted negative returns in subsequent years. This results provides evidence for resale option theory or speculative trading models of bubble formation, which predicts that speculative trading can generate significant price bubbles, even if trading costs are large and leverage impossible.

@Renneboog2014 found evidence that changes in high-income consumer confidence predicted art returns. This suggests that time-varying optimism about the potential of art as an investment can partially explain the existence of art market cycles. Combined with the impossibility of short-selling, this uncertainty implies a potential role for art investor sentiment, which could be defined as unjustified optimism (or pessimism) about future resale values.[^37]

[^37]: This could provide a link between the two papers?

The use of the general price level (measured by CPI) to identify art price fundamentals makes the implicit assumption is that art prices tend to reflect general price movements across the economy. Large deviations from past levels could be considered as explosive in the short term as it could feasibly lead to higher allocation towards art as assets experiencing high capital growth. This, in turn, feeds into more demand and even higher prices, potentially driving an episode of unsustainable asset price increases, particularly as a result of factors inherent to art purchases, such as high transaction costs and difficulties with short-selling [@Balcilar2015]. Similarly, @Mandel2009 formalised the satisfaction derived from the conspicuous consumption that is increasing in the value of art. This part of the aesthetic dividend that is a signal of wealth could plausibly lead to price increases, which in turn could lead to another increase in the dividend related social status consumption. 

In general, speculative bubbles can be like self-fulfilling prophecies. Price rises because agents expect it to do so, with this ongoing expectation providing the increasing demand that keeps the price rising. If due to some exogenous shock like the financial crisis the price stops rising, this breaks the expectation and the speculative demand suddenly disappears, sending the price back to its fundamental value (or thereabouts), where there is no expectation of the price rising [@Rosser2012].

#Conclusion
This paper has estimated various price indices for the South African art market over time and then used these measures to test whether there was evidence of a bubble in the market over the period. Three broad methodologies are used to estimate quality-adjusted price indices for South African art since the turn of the millennium: central tendency methods, hedonic methods and hybrid repeat sales methods. 
The regression-based methods employed each has strengths and weaknesses. The hedonic method may suffer from omitted variable bias, while the pseudo-repeat sales method may control for some of this omitted variable bias, but suffer more from possible sample selection bias. However, the regression-based indices point to similar trends over the period, which shows the importance of regression-based methods to produce quality-corrected price indices. According to these measures, the South African art market experienced a huge price increase in the run-up to the Great Recession. This movement was similar to international art price indices and traditional South African assets. This relatively consistent picture offers some confidence that the indices provide a relatively accurate measure of the actual price movements in the market. 

The art price indices were then used to look for mildly explosive behaviour in prices over the sample period, using a reduced-form bubble detection method. The results indicate that there is evidence of bubble-like behaviour in all of the regression-based art price indices. The regression-based indices seem to point to consistent evidence of explosive prices in the run-up to the Great Recession, with a potential bubble beginning around 2006 and ending around 2008.

#References 
