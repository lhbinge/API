corstars <- function(x){
require(Hmisc)
x <- as.matrix(x)
R <- rcorr(x)$r
p <- rcorr(x)$P
## define notions for significance levels; spacing is important.
mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "* ", " ")))
## trunctuate the matrix that holds the correlations to two decimal
R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
diag(Rnew) <- paste(diag(R), " ", sep="")
rownames(Rnew) <- colnames(x)
colnames(Rnew) <- paste(colnames(x), "", sep="")
## remove upper triangle
Rnew <- as.matrix(Rnew)
Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
Rnew <- as.data.frame(Rnew)
## remove last column and return the matrix (which is now a data frame)
Rnew <- cbind(Rnew[1:length(Rnew)-1])
return(Rnew)
}
corstars(mtcars[,1:11])
#xtable(corstars(mtcars[,1:11]))
lcars <- cbind(mtcars[,12:17],cyl=mtcars[,2])
lcars <- cbind(lcars,mtcars[,8:11])
corstars(lcars)
#xtable(corstars(lcars))
#Simple models
model_am <- lm(mpg ~ fam, data = mtcars)
summary(model_am)
e <- resid(model_am)
g <- ggplot(mtcars, aes(x=am, y=e))
g <- g + geom_point(size = 5, color = "black", alpha =0.5)
g
model_lam <- lm(lmpg ~ fam, data = mtcars)
summary(model_lam)
#Sequentially add variables with highest correlations
library(stargazer)
model_am2 <- lm(mpg ~ wt+fam, data = mtcars)
model_am3 <- lm(mpg ~ fcyl+wt+fam, data = mtcars)
summary(model_am3)
model_am4 <- lm(mpg ~ fcyl+disp+wt+fam, data = mtcars)
anova(model_am, model_am2, model_am3, model_am4)
stargazer(model_am, model_am2, model_am3,  model_am4,
font.size = "tiny", type="text", no.space=TRUE)
xtable(model_am4)
xtable(model_lam4)
model_lam2 <- lm(lmpg ~ ldisp+fam, data = mtcars)
model_lam3 <- lm(lmpg ~ ldisp+lwt+fam, data = mtcars)
model_lam4 <- lm(lmpg ~ ldisp+lwt+lhp+fam, data = mtcars)
summary(model_lam4)
model_lam5 <- lm(lmpg ~ fcyl+ldisp+lwt+lhp+fam, data = mtcars)
anova(model_lam, model_lam2, model_lam3, model_lam4, model_lam5)
stargazer(model_lam, model_lam4, model_lam5,
font.size = "tiny", type="text", no.space=TRUE)
xtable(model_lam4)
xtable(corstars(mtcars[,1:11]))
xtable(summary(model_am)$coefficients)
xtable(summary(model_am)$coefficients)
?xtable
xtable(anova(model_am, model_am2, model_am3, model_am4))
?xyplot()
?xyplot
??xyplot()
?rggobi
??rggobi
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
kingstimeseries <- ts(kings)
kingstimeseries
?ts
births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")
birthstimeseries <- ts(births, frequency=12, start=c(1946,1))
birthstimeseries
souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
souvenirtimeseries <- ts(souvenir, frequency=12, start=c(1987,1))
souvenirtimeseries
plot.ts(kingstimeseries)
plot.ts(birthstimeseries)
plot.ts(souvenirtimeseries)
logsouvenirtimeseries <- log(souvenirtimeseries)
plot.ts(logsouvenirtimeseries)
library("TTR")
kingstimeseriesSMA3 <- SMA(kingstimeseries,n=3)
plot.ts(kingstimeseriesSMA3)
kingstimeseriesSMA8 <- SMA(kingstimeseries,n=8)
plot.ts(kingstimeseriesSMA8)
birthstimeseriescomponents <- decompose(birthstimeseries)
birthstimeseriescomponents$seasonal # get the estimated values of the seasonal component
plot(birthstimeseriescomponents)
birthstimeseriesseasonallyadjusted <- birthstimeseries - birthstimeseriescomponents$seasonal
plot(birthstimeseriesseasonallyadjusted)
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rainseries <- ts(rain,start=c(1813))
plot.ts(rainseries)
rainseriesforecasts <- HoltWinters(rainseries, beta=FALSE, gamma=FALSE)
rainseriesforecasts
rainseriesforecasts$fitted
plot(rainseriesforecasts)
rainseriesforecasts$SSE
HoltWinters(rainseries, beta=FALSE, gamma=FALSE, l.start=23.56)
rainseriesforecasts2 <- HoltWinters(rainseries, beta=FALSE, gamma=FALSE, l.start=23.56)
plot(rainseriesforecasts2)
library("forecast")
rainseriesforecasts2 <- forecast.HoltWinters(rainseriesforecasts, h=8)
rainseriesforecasts2
plot.forecast(rainseriesforecasts2)
acf(rainseriesforecasts2$residuals, lag.max=20)
Box.test(rainseriesforecasts2$residuals, lag=20, type="Ljung-Box")
plot.ts(rainseriesforecasts2$residuals)
plotForecastErrors <- function(forecasterrors) {
# make a histogram of the forecast errors:
mybinsize <- IQR(forecasterrors)/4
mysd <- sd(forecasterrors)
mymin <- min(forecasterrors) - mysd*5
mymax <- max(forecasterrors) + mysd*3
# generate normally distributed data with mean 0 and standard deviation mysd
mynorm <- rnorm(10000, mean=0, sd=mysd)
mymin2 <- min(mynorm)
mymax2 <- max(mynorm)
if (mymin2 < mymin) { mymin <- mymin2 }
if (mymax2 > mymax) { mymax <- mymax2 }
# make a red histogram of the forecast errors, with the normally distributed data overlaid:
mybins <- seq(mymin, mymax, mybinsize)
hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
# freq=FALSE ensures the area under the histogram = 1
# generate normally distributed data with mean 0 and standard deviation mysd
myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
# plot the normal curve as a blue line on top of the histogram of forecast errors:
points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
plotForecastErrors(rainseriesforecasts2$residuals)
skirts <- scan("http://robjhyndman.com/tsdldata/roberts/skirts.dat",skip=5)
skirtsseries <- ts(skirts,start=c(1866))
plot.ts(skirtsseries)
skirtsseriesforecasts <- HoltWinters(skirtsseries, gamma=FALSE)
skirtsseriesforecasts
skirtsseriesforecasts$SSE
plot(skirtsseriesforecasts)
HoltWinters(skirtsseries, gamma=FALSE, l.start=608, b.start=9)
skirtsseriesforecasts2 <- forecast.HoltWinters(skirtsseriesforecasts, h=19)
plot.forecast(skirtsseriesforecasts2)
acf(skirtsseriesforecasts2$residuals, lag.max=20)
Box.test(skirtsseriesforecasts2$residuals, lag=20, type="Ljung-Box")
plot.ts(skirtsseriesforecasts2$residuals) # make a time plot
plotForecastErrors(skirtsseriesforecasts2$residuals)
souvenirtimeseriesforecasts <- HoltWinters(logsouvenirtimeseries)
souvenirtimeseriesforecasts
souvenirtimeseriesforecasts$SSE
plot(souvenirtimeseriesforecasts)
souvenirtimeseriesforecasts2 <- forecast.HoltWinters(souvenirtimeseriesforecasts, h=48)
plot.forecast(souvenirtimeseriesforecasts2)
acf(souvenirtimeseriesforecasts2$residuals, lag.max=20)
Box.test(souvenirtimeseriesforecasts2$residuals, lag=20, type="Ljung-Box")
plot.ts(souvenirtimeseriesforecasts2$residuals) # make a time plot
plotForecastErrors(souvenirtimeseriesforecasts2$residuals) # make a histogram
?matchIt
library("TTR")
library("forecast")
skirts <- scan("http://robjhyndman.com/tsdldata/roberts/skirts.dat",skip=5)
skirtsseries <- ts(skirts,start=c(1866))
plot.ts(skirtsseries)
skirtsseriesforecasts <- HoltWinters(skirtsseries, gamma=FALSE)
skirtsseriesforecasts
skirtsseriesforecasts$SSE
plot(skirtsseriesforecasts)
HoltWinters(skirtsseries, gamma=FALSE, l.start=608, b.start=9)
skirtsseriesforecasts2 <- forecast.HoltWinters(skirtsseriesforecasts, h=19)
plot.forecast(skirtsseriesforecasts2)
acf(skirtsseriesforecasts2$residuals, lag.max=20)
Box.test(skirtsseriesforecasts2$residuals, lag=20, type="Ljung-Box")
plot.ts(skirtsseriesforecasts2$residuals) # make a time plot
plotForecastErrors(skirtsseriesforecasts2$residuals)
plotForecastErrors <- function(forecasterrors) {
# make a histogram of the forecast errors:
mybinsize <- IQR(forecasterrors)/4
mysd <- sd(forecasterrors)
mymin <- min(forecasterrors) - mysd*5
mymax <- max(forecasterrors) + mysd*3
# generate normally distributed data with mean 0 and standard deviation mysd
mynorm <- rnorm(10000, mean=0, sd=mysd)
mymin2 <- min(mynorm)
mymax2 <- max(mynorm)
if (mymin2 < mymin) { mymin <- mymin2 }
if (mymax2 > mymax) { mymax <- mymax2 }
# make a red histogram of the forecast errors, with the normally distributed data overlaid:
mybins <- seq(mymin, mymax, mybinsize)
hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
# freq=FALSE ensures the area under the histogram = 1
# generate normally distributed data with mean 0 and standard deviation mysd
myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
# plot the normal curve as a blue line on top of the histogram of forecast errors:
points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
plotForecastErrors(skirtsseriesforecasts2$residuals)
skirtsseriesdiff1 <- diff(skirtsseries, differences=1)
plot.ts(skirtsseriesdiff1)
skirtsseriesdiff2 <- diff(skirtsseries, differences=2)
plot.ts(skirtsseriesdiff2)
install.packages("fUnitRoots")
library(fUnitRoots)
?fUnitRoots
??fUnitRoots
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
kingstimeseries <- ts(kings)
kingstimeseries
kingstimeseriesSMA3 <- SMA(kingstimeseries,n=3)
plot.ts(kingstimeseriesSMA3)
kingstimeseriesSMA8 <- SMA(kingstimeseries,n=8)
plot.ts(kingstimeseriesSMA8)
kingtimeseriesdiff1 <- diff(kingstimeseries, differences=1)
plot.ts(kingtimeseriesdiff1)
acf(kingtimeseriesdiff1, lag.max=20) # plot a correlogram
acf(kingtimeseriesdiff1, lag.max=20, plot=FALSE) # get the autocorrelation values
pacf(kingtimeseriesdiff1, lag.max=20) # plot a partial correlogram
pacf(kingtimeseriesdiff1, lag.max=20, plot=FALSE) # get the partial autocorrelation values
auto.arima(kings)
volcanodust <- scan("http://robjhyndman.com/tsdldata/annual/dvi.dat", skip=1)
volcanodustseries <- ts(volcanodust,start=c(1500))
plot.ts(volcanodustseries)
acf(volcanodustseries, lag.max=20) # plot a correlogram
acf(volcanodustseries, lag.max=20, plot=FALSE) # get the values of the autocorrelations
pacf(volcanodustseries, lag.max=20)
pacf(volcanodustseries, lag.max=20, plot=FALSE)
auto.arima(volcanodust)
auto.arima(volcanodust,ic="bic")
kingstimeseriesarima <- arima(kingstimeseries, order=c(0,1,1)) # fit an ARIMA(0,1,1) model
kingstimeseriesarima
forecast.Arima(kingstimeseriesarima,h=5, level=c(99.5))
kingstimeseriesforecasts <- forecast.Arima(kingstimeseriesarima, h=5)
kingstimeseriesforecasts
plot.forecast(kingstimeseriesforecasts)
acf(kingstimeseriesforecasts$residuals, lag.max=20)
Box.test(kingstimeseriesforecasts$residuals, lag=20, type="Ljung-Box")
plot.ts(kingstimeseriesforecasts$residuals) # make time plot of forecast errors
plotForecastErrors(kingstimeseriesforecasts$residuals) # make a histogram
volcanodustseriesarima <- arima(volcanodustseries, order=c(2,0,0))
volcanodustseriesarima
volcanodustseriesforecasts <- forecast.Arima(volcanodustseriesarima, h=31)
volcanodustseriesforecasts
plot.forecast(volcanodustseriesforecasts)
volcanodustseries
volcanodustseriesforecasts
plot(volcanodustseries$fitted)
plot(volcanodustarima)
plot(volcanodustseriesarima$x,col="red")
lines(fitted(volcanodustseriesarima),col="blue")
volcanodustseriesarima <- Arima(volcanodustseries, order=c(2,0,0))
plot(volcanodustseriesarima$x,col="red")
lines(fitted(volcanodustseriesarima),col="blue")
plot.forecast(volcanodustseriesforecasts)
acf(volcanodustseriesforecasts$residuals, lag.max=20)
Box.test(volcanodustseriesforecasts$residuals, lag=20, type="Ljung-Box")
plot.ts(volcanodustseriesforecasts$residuals) # make time plot of forecast errors
plotForecastErrors(volcanodustseriesforecasts$residuals) # make a histogram
mean(volcanodustseriesforecasts$residuals)
data(AirPassengers)
AP <- AirPassengers
AP
plot(AP, ylab = "Passengers (1000's)")
layout(1:2)
plot(aggregate(AP))
boxplot(AP ~ cycle(AP))
plot(cbind(Elec.ts, Beer.ts, Choc.ts))
AP.elec <- ts.intersect(AP, Elec.ts)
plot(as.vector(AP), as.vector(Elec),
xlab = "Air passengers / 1000's",
ylab = "Electricity production / MWh")
Z.92.96 <- window(Z.ts, start = c(1992, 1), end = c(1996, 1))
Global.annual <- aggregate(Global.ts, FUN = mean)
library(urca)
data(npext)
y<-ts(na.omit(npext$unemploy),start=1909,end=1988,frequency=1)
op<-par(no.readonly=TRUE)
layout(matrix(c(1,1,2,3),2,2,byrow=TRUE))
plot(y,ylab="unemploymentrate(logarithm)")
acf(y,main="Autocorrelations",ylab='',ylim=c(-1,1))
pacf(y,main="PartialAutocorrelations",ylab='', ylim=c(-1,1))
par(op)
##tentative ARMA(2,0)
arma20<-arima(y,order=c(2,0,0))
ll20<-logLik(arma20)
aic20<-arma20$aic
res20<-residuals(arma20)
Box.test(res20,lag=20,type="Ljung-Box")
shapiro.test(res20)
##alternative specifications
##ARMA(3,0)
arma30<-arima(y,order=c(3,0,0))
ll30<-logLik(arma30)
aic30<-arma30$aic
lrtest<-as.numeric(2*(ll30-ll20))
chi.pval<-pchisq(lrtest,df=1,lower.tail=FALSE)
##ARMA(1,1)
arma11<-arima(y,order=c(1,0,1))
ll11<-logLik(arma11)
aic11<-arma11$aic
tsdiag(arma11)
res11<-residuals(arma11)
Box.test(res11,lag=20,type="Ljung-Box")
shapiro.test(res11)
#tsdiag(arma11)
##Using auto.arima()
library(forecast)
auto.arima(y,max.p=3,max.q=3,start.p=1,start.q=1,ic="aic")
##Forecasts
arma11.pred<-predict(arma11,n.ahead=10)
predict<-ts(c(rep(NA,length(y)-1),y[length(y)],
arma11.pred$pred),start=1909,frequency=1)
observed<-ts(c(y,rep(NA,10)),start=1909,frequency=1)
plot(observed,type="l",ylab="Actual and predicted values",xlab="")
lines(predict,col="blue",lty=2)
lines(lower,col="red",lty=5)
lines(upper,col="red",lty=5)
abline(v=1988,col="gray",lty=3)
upper<-ts(c(rep(NA,length(y)-1),y[length(y)],
arma11.pred$pred+2*arma11.pred$se),start=1909,frequency=1)
lower<-ts(c(rep(NA,length(y)-1),y[length(y)],
arma11.pred$pred-2*arma11.pred$se),start=1909,frequency=1)
plot(observed,type="l",ylab="Actual and predicted values",xlab="")
lines(predict,col="blue",lty=2)
lines(lower,col="red",lty=5)
lines(upper,col="red",lty=5)
abline(v=1988,col="gray",lty=3)
##Simulate VAR(2)-data1
library(dse1)
library(vars)
##Setting the lag-polynomial A(L)
Apoly<-array(c(1.0,-0.5,0.3,0,0.2,0.1,0,-0.2,
0.7,1,0.5,-0.3),c(3,2,2))
##Setting Covariance to identity-matrix
B<-diag(2)
##Setting constant term to 5 and 10
TRD<-c(5,10)
##Generating the VAR(2) model
var2<-ARMA(A=Apoly,B=B,TREND=TRD)
##Simulating 500 observations
varsim<-simulate(var2,sampleT=500,noise=list(w=matrix(rnorm(1000),
nrow=500,ncol=2)),rng=list(seed=c(123456)))
##Obtaining the generated series
vardat<-matrix(varsim$output,nrow=500,ncol=2)
colnames(vardat)<-c("y1","y2")
##Plotting the series
plot.ts(vardat,main="",xlab="")
##Determining an appropriate lag-order
infocrit<-VARselect(vardat,lag.max=3,type="const")
##Estimating the model
varsimest<-VAR(vardat,p=2,type="const",season=NULL,exogen=NULL)
##Alternatively, selection according to AIC
varsimest<-VAR(vardat,type="const",lag.max=3,ic="SC")
##Checking the roots
roots<-roots(varsimest)
install.packages("dse1")
library("vars")
data("Canada")
summary(Canada)
plot(Canada, nc = 2, xlab = "")
adf1 <- summary(ur.df(Canada[, "prod"], type = "trend", lags = 2))
R> adf1
adf1
adf2 <- summary(ur.df(diff(Canada[, "prod"]), type = "drift",lags = 1))
adf2
VARselect(Canada, lag.max = 8, type = "both")
Canada <- Canada[, c("prod", "e", "U", "rw")]
p1ct <- VAR(Canada, p = 1, type = "both")
p1ct
View(npext)
##===============================================================================================##
## -------------------------------- ART PRICE INDEX ---------------------------------------------##
##===============================================================================================##
##=====================##
## READING IN THE DATA ##
##=====================##
library(zoo)
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(stargazer)
library(micEcon)
library(quantreg)
library(McSpatial)
#setwd("C:/Users/Laurie/OneDrive/Documents/BING/METRICS/PhD Proposal Readings/Art Price Index")
setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\PhD Proposal Readings\\Art Price Index\\R Code")
#library(rJava)
#library(xlsxjars)
#library(xlsx)
#artdata <- read.xlsx("Auction database.xlsx",sheetIndex=1,header=TRUE)
artdata <- read.csv("Auction database.csv", header=TRUE, sep=";",na.strings = "", skipNul = TRUE,
colClasses=c("character","numeric","numeric","numeric","numeric","factor","factor","factor","character",
"factor","factor","factor","character","factor","factor","factor","numeric","character",
"numeric","numeric","numeric","numeric","numeric","numeric"))
##===================##
## CLEANING THE DATA ##
##===================##
artdata$date <- as.Date(artdata$date)
artdata$med_code <- factor(artdata$med_code, labels=c("Drawing", "Watercolour", "Oil", "Acrylic", "Print/Woodcut",
"Mixed Media","Sculpture","Photography", "Other"))
artdata$ah_code <- factor(artdata$ah_code, labels=c("5th Avenue","Ashbeys","Bernardi","Bonhams","Russell Kaplan",
"Stephan Welz","Strauss","Christies"))
artdata$timedummy <- factor(as.yearqtr(artdata$date, "%Y-%m-%d"))
#artdata$timedummy <- factor(paste(artdata$year, artdata$quarter, sep="_")
#dummies = model.matrix(~artdata$timedummy)
#For every unique value in the string column, create a new 1/0 column
#This is what Factors do "under-the-hood" automatically when passed to function requiring numeric data
artdata$lnprice <- log(artdata$price)
artdata$lnarea <- log(artdata$area)
artdata$lnarea2 <- artdata$lnarea*artdata$lnarea
#artdata$lnsculpt_area <- 0
#artdata$lnsculpt_area[na.omit(artdata$med_code==7)] <- artdata$lnarea   #inteaction term: sculptures often only reported with 1 dimension (height)
artdata$lnsculpt_area <- ifelse(artdata$med_code=="Sculpture", artdata$lnarea, 0)
artdata$counter <- as.numeric(artdata$timedummy)
#source(themes)
#source(materials)
#change reference category with relevel()
#artdata$ah_code <- relevel(artdata$ah_code, ref = "Stephan Welz & Co")
#artdata$artist <- relevel(artdata$artist, ref = "Battiss, Walter Whall")
#artdata$med_code <- relevel(artdata$med_code, ref = "Oil")
#head(artdata)
#str(artdata)
##----------------------
##Rank Artists by Volume
##----------------------
#Rank by Total Volume (all)
rankings <- count(artdata, artist)
rankings$rank_all <- dense_rank(desc(rankings$n))    #rank by density, with no gaps between ranks
#rankings$rank_all <- row_number(desc(rankings$n))   #equivalent to rank(ties.method = "first")
#rankings$rank_all <- min_rank(desc(rankings$n))     #equivalent to rank(ties.method = "min")
rankings$n <- NULL
#rankings$rank_all <- factor(rankings$rank_all, labels=c)
#artdata <- merge(artdata, tel, by.x="artist", by.y="artist")
##Rank by Rolling 5-year window
for(i in 1:11) {
teller <- 1998+i
som <- count(artdata[(artdata$year>teller & artdata$year<(teller+6)),], artist)
som$rank_new <- dense_rank(desc(som$n))
# the alternative is to rank by row_number or min_rank
som$n <- NULL
colnames(som) <- c("artist", paste0("rank_", i))
rankings <- merge(rankings, som, by.x="artist", by.y="artist",all.x=TRUE)
}
#Rank Update
som <- count(artdata[(artdata$counter>42 & artdata$counter<63),], artist)
som$rank_new <- dense_rank(desc(som$n))  # rank by equivalent to rank(ties.method = "first")
# the alternative is to rank by row_number or min_rank
som$n <- NULL
colnames(som) <- c("artist", "rank_update")
rankings <- merge(rankings, som, by.x="artist", by.y="artist",all.x=TRUE)
##Rank by Annual Volume
for(i in 1:16) {
teller <- 1999+i
som <- count(artdata[(artdata$year==teller),], artist)
som$rank_new <- dense_rank(desc(som$n))
# the alternative is to rank by row_number or min_rank
som$n <- NULL
colnames(som) <- c("artist", paste0("rank_y", teller))
rankings <- merge(rankings, som, by.x="artist", by.y="artist",all.x=TRUE)
}
##Rank by 2-year Volume
for(i in 1:8) {
teller <- 1998+(i*2-1)
som <- count(artdata[(artdata$year>teller & artdata$year<(teller+3)),], artist)
som$rank_new <- dense_rank(desc(som$n))
# the alternative is to rank by row_number or min_rank
som$n <- NULL
colnames(som) <- c("artist", paste0("rank_a", i))
rankings <- merge(rankings, som, by.x="artist", by.y="artist",all.x=TRUE)
}
artdata <- merge(artdata, rankings, by.x="artist", by.y="artist",all.x=TRUE)
## Rank total
rankings <- count(artdata, artist)
rankings$rank_total <- row_number(desc(rankings$n))
artdata <- merge(artdata, rankings, by.x="artist", by.y="artist",all.x=TRUE)
##======================##
## EXPLORATORY ANALYSIS ##
##======================##
# Example plot
#p4 <- ggplot(subset(ChickWeight, Time==21), aes(x=weight, fill=Diet)) +
#    geom_histogram(colour="black", binwidth=50) +
#    facet_grid(Diet ~ .) +
#    ggtitle("Final weight, by diet") +
#    theme(legend.position="none")        # No legend (redundant in this graph)
#    multiplot(p1, p2, p3, p4, cols=2)
#Simple plots
g <- ggplot(artdata, aes(x=year, y=lnprice))
g <- g + geom_point(size = 2, alpha = 0.5)
g <- g + stat_summary(fun.y="median", geom="line")
g <- g + ylab("log of Price")
g <- g + xlab("Year")
g
artplot <- aggregate(artdata$hammer_price, by=list(artdata$timedummy), FUN = sum, na.rm=TRUE)
g <- ggplot(artplot, aes(x=Group.1, y=x))
g <- g + geom_bar(stat="identity")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + ylab("Sum of Hammer Price")
g <- g + xlab("Date")
g
artplot1 <- aggregate(artdata$hammer_price, by=list(artdata$year), length)
artplot2 <- aggregate(artdata$hammer_price, by=list(artdata$year), FUN = median, na.rm=TRUE)
artplot <- merge(artplot1, artplot2, by="Group.1",all.x=TRUE)
names(artplot) <- c("Date","Total Sales","Median Price")
artplot <- melt(artplot, id="Date")
g <- ggplot(artplot, aes(x=Date,value,colour=variable,fill=variable))
g <- g + geom_bar(subset=.(variable=="Total Sales"),stat="identity")
g <- g + geom_line(subset=.(variable=="Median Price"),size=1.5)
g <- g + theme(legend.position="bottom") + theme(legend.title=element_blank())
g
Canada <- Canada[, c("prod", "e", "U", "rw")]
p1ct <- VAR(Canada, p = 1, type = "both")
p1ct
summary(p1ct, equation = "e")
adf1 <- summary(ur.df(Canada[, "prod"], type = "trend", lags = 2))
