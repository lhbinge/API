---
output: 
    pdf_document:
        fig_caption: yes
        number_sections: true
fontsize: 11pt
geometry: margin=1in
bibliography: References.bib
csl: harvard.csl
---
<!-- maketile -->
\begin{center}
\Large\scshape{South African Art Price Bubble} \\ 
\vspace{1em}
\large\normalfont{Laurie Binge}\footnote{PhD candidate at the Department of Economics at Stellenbosch University. Corresponding author at: lhbinge@gmail.com} \\
\large\normalfont{Willem H. Boshoff}\footnote{Associate Professor at the Department of Economics at Stellenbosch University. Email address: wimpie2@sun.ac.za} \\
\normalsize\textit{Stellenbosch University, Stellenbosch, South Africa.} \\
\normalsize\normalfont{\today} 
\end{center}
\begin{small}

South African art has experienced a surge in popularity over the last few decades, with large price increases and record prices at local and international auctions. This paper looks for evidence of a bubble in the South African art market by testing for mildly explosive prices. The outcome of the test depends critically on the estimation of a reasonably accurate quality-adjusted price index, which can be challenging for unique assets such as art. The choice of methodology determines whether there is compelling evidence of a bubble and what origination and termination dates are identified. This paper estimates three sets of art price indices, based on the three main methodologies used to estimate quality-adjusted price indices for unique assets. The indices point to consistent evidence of explosive price behaviour in the run-up to the financial crisis. The bubble was relatively dispersed throughout the market, although prices seem to have been especially explosive for high-end oil paintings by the top artists.

\vspace{0.5em}
\noindent{\textbf{JEL Classification:} C43, Z11, E31, G12 } \\
\noindent{\textbf{Keywords:} South African Art, Hedonic Price Index, Pseudo Repeat Sales, Explosive Prices }
\end{small}
\renewcommand{\thefootnote}{\arabic{footnote}}

#Introduction
Contemporary African art has experienced a surge in popularity over the last few decades. The South African art market in particular has received a lot of attention, and has grown markedly over the last two decades, both in terms of the number of transactions and total turnover [@Fedderke2014]. Artworks by South African artists have reached record prices at local and international auctions, both for the country's 'masters', including Irma Stern, JH Pierneef and Walter Battiss, and contemporary artists like William Kentridge [@Naidoo2013]. In 2011, Stern's *Two Arabs* was sold by Strauss & Co. for a hammer price of R19 million, a record for a South African auction. Also in 2011, Bonhams in London sold Irma Stern's *Arab Priest* for a hammer prices of £2.7 million, a world record for a South African artwork at an auction. The increase in interest in South African art, both locally and abroad, has sparked a vibrant market for collectors and investors. 

The increase in the popularity of South African art, at least partly as an investment vehicle, is commensurate with international trends, where fine art has become an important asset class in its own right. In 2010, around 6% of total wealth was held in passion investments, which include art, antiques, wines and jewellery [@Renneboog2014]. In 2013, art made up around 17% of high net worth individuals' allocations to passion investments [@Capgemini2013]. Of all these passion investments, art is the most likely to be purchased for potential value appreciation [@Capgemini2010]. In addition to the potential for appreciation in value, artworks may be used to aid portfolio diversification, as collateral for loans, or to take advantage of slacker regulatory and tax rules. Thus, unlike pure financial investments, artworks are durable goods with consumption and investment good characteristics [@Renneboog2014].

The large price increases and record prices for South African artworks at local and international auctions, especially between 2008 and 2011, prompted commentators at the time to claim that the market was overheating and suggest the possibility of a "bubble" in the market (e.g. @Rabe2011; @Hundt2010; @Curnow2010). According to the New Palgrave Dictionary of Economics, "*bubbles refer to asset prices that exceed an asset's fundamental value because current owners believe they can resell the asset at an even higher price*" [@Brunnermeier2008]. A bubble consists of a sharp rise in a given asset price, above a level sustainable by fundamentals, followed by a sudden collapse [@Kraussl2016]. 

When it comes to the art market, however, it is particularly challenging to determine the fundamental value from which prices potentially deviate. In the case of stocks, dividends have been used to obtain the expected cash flow as a measure of fundamental value. Rents and convenience yields can potentially be used for real estate prices and commodity prices [@Penasse2014]. 

In contrast, artworks do not generate a future income stream (e.g. dividends or rents) that can be discounted to determine the fundamental value. Artworks usually have little intrinsic value, unless the materials used have a high value [@Spaenjers2015]. Instead, artworks are acquired for a kind of non-monetary utility or aesthetic dividend, sometimes described as 'aesthetic pleasure' [@Gerard-Varet1995]. This dividend can be interpreted as the rent buyers are willing to pay to own the artwork over a given period. The rent can reflect aesthetic pleasure, but may also provide additional utility as a signal of wealth [@Mandel2009]. The price of an artwork should equal the present value of future private dividends over the holding period, and the expected resale value. The value of the dividend is unobservable and is likely to vary greatly among buyers, based on their motivations and characteristics [@Penasse2014]. Thus, it is almost impossible to determine the fundamental value of art [@Kraussl2016]. 

To overcome this issue, this paper follows @Kraussl2016 in using a direct bubble detection method developed by @Phillips2011. The method is based on a right-tailed augmented Dickey-Fuller (ADF) unit root test, which is able to detect explosive behaviour in time series. @Phillips2011 originally applied the method to stock prices. They showed that there was evidence of explosiveness in stock prices, but not in dividend yields, implying that price explosiveness could not be explained by developments in fundamentals. 

The test, however, requires the estimation of a reasonably accurate quality-adjusted price index, which can be challenging for unique assets such as art and real estate. The choice of methodology will determine whether there is compelling evidence of a bubble and what origination and termination dates are identified. Each methodology has shortcomings and the danger is that the biases inherent in each methodology may be driving the results. 

This paper uses three broad methodologies to develop quarterly price indices for South African art. The use of quarterly indices allows the paper to investigate higher frequency movements in art prices. Simple central tendency indices are estimated as a baseline for comparison, but do not adequately control for quality-mix changes over time. The hedonic regression method is able to control more adequately for quality-mix changes, but has the shortcoming of potential omitted variable bias. Repeat sales indices suffer less from potential omitted variable bias, but have the shortcoming of potential sample selection bias. In this case the scarcity of repeat sales observations in the database limits the usefulness of the classical repeated sales approach. The paper proposes a simple hybrid repeat sales method to address the problem of scarcity of repeat sales observations and to some extent the potential omitted variable bias inherent in the hedonic method. The paper then compares and evaluates the indices in terms of smoothness, which helps to determine the bubble period more accurately.

The regression-based indices seem to point to consistent evidence of mildly explosive price behaviour in the run-up to the financial crisis, between 2005/06 and 2008. Different segments of the market are then investigated to find out whether this bubble was dispersed throughout the market. The results indicate that the bubble was relatively dispersed, although prices seem to have been especially explosive for high-end oil paintings by the top artists.

#South African Art Auction Data
```{r cleaning, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##=====================##
## READING IN THE DATA ##
##=====================##
suppressMessages(library(zoo))
suppressMessages(library(ggplot2))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(stargazer))
suppressMessages(library(micEcon))
suppressMessages(library(quantreg))
suppressMessages(library(McSpatial))
suppressMessages(library(quantmod))
suppressMessages(library(xtable))
suppressMessages(library(scales))
suppressMessages(library(tseries))
suppressMessages(library(urca))
suppressMessages(library(mFilter))

setwd("C:\\Users\\Laurie\\OneDrive\\Documents\\BING\\Art Price Index\\R Code")

datums <- read.csv("datums.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)
datums$Datum <- as.Date(datums$Datum, format = "%Y/%m/%d")

assets <- read.csv("Assets.csv", header=TRUE, na.strings = "", skipNul = TRUE)

artdata <- read.csv("Auction database.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE, 
                    colClasses=c("character","numeric","numeric","numeric","numeric","factor","factor","factor","character",
                                 "factor","factor","factor","character","factor","factor","factor","numeric","character",
                                 "numeric","numeric","numeric","numeric","numeric","numeric"))

##===================##
## CLEANING THE DATA ##
##===================##
artdata$date <- as.Date(artdata$date)
artdata$med_code <- factor(artdata$med_code, labels=c("Drawing", "Watercolour", "Oil", "Acrylic", "Print/Woodcut",
                                                      "Mixed Media","Sculpture","Photography", "Other"))
artdata$ah_code <- factor(artdata$ah_code, labels=c("5th Avenue","Ashbeys","Bernardi","Bonhams","Russell Kaplan",
                                                    "Stephan Welz","Strauss","Christies"))
artdata$timedummy <- factor(as.yearqtr(artdata$date, "%Y-%m-%d"))
artdata$lnprice <- log(artdata$price)
artdata$lnarea <- log(artdata$area)
artdata$lnarea2 <- artdata$lnarea*artdata$lnarea
#inteaction term: sculptures often only reported with 1 dimension (height)
artdata$lnsculpt_area <- ifelse(artdata$med_code=="Sculpture", artdata$lnarea, 0)
artdata$counter <- as.numeric(artdata$timedummy)

##----------------------
##Rank Artists by Volume
##----------------------
#Rank by Total Volume (all)
rankings <- count(artdata, artist)
rankings$rank_all <- dense_rank(desc(rankings$n))    #rank by density, with no gaps (ties = equal)
rankings$rank_total <- row_number(desc(rankings$n))  #equivalent to rank(ties.method = "first")
rankings$n <- NULL
artdata <- merge(artdata, rankings, by.x="artist", by.y="artist",all.x=TRUE)
```

```{r loadrep, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#Load pre-calculated dataset (for speed) from API_3.R
artdata <- read.csv("artdata_lnrep2.csv", header=TRUE)
artdata$med_code <- factor(artdata$med_code, labels=c("Acrylic","Drawing","Mixed Media","Oil","Other",        
                                                      "Photography","Print/Woodcut","Sculpture","Watercolour"))

##Rank Artists by Value
value <- aggregate(artdata$hammer_price, by=list(artdata$artist), FUN = sum)
volume <- aggregate(artdata$hammer_price, by=list(artdata$artist), FUN = length)
value <- cbind(value,volume[,2],value$x/volume$x)    
colnames(value) <- c("artist","value","volume","ave_price")

rankings <- count(artdata, artist)
#rankings <- rankings[order(rankings$n, decreasing = TRUE),]
rankings$rank_all <- dense_rank(desc(rankings$n))    #rank by density, with no gaps (ties = equal)
rankings$rank_total <- row_number(desc(rankings$n))  #equivalent to rank(ties.method = "first")
rankings$n <- NULL

rankings$value <- row_number(desc(value$value)) 
rankings$volume <- row_number(desc(value$volume))
rankings$ave_price <- row_number(desc(value$ave_price))

artdata <- merge(artdata, rankings[,c(1,4,6)], by.x="artist", by.y="artist",all.x=TRUE)
```

##Public and Private Art Market Prices
The literature on estimating art price indices has relied almost exclusively on publicly available auction prices. Art is also sold privately, either directly by artists or through dealers. However, dealers' sales records are generally not available. Releasing such information may be damaging to dealers' businesses, and they have an incentive to give the impression that there is high demand for their artworks [@Olckers2015]. 

Nevertheless, it is generally accepted that auction prices provide a benchmark that is used in the private market [@Renneboog2012]. Anecdotal evidence suggests that private dealers are very aware of auction prices and follow them closely. Differences between auctions and private markets in terms of institutional arrangements, transaction costs, and available information might lead to different price levels for the same or similar artworks. However, in constructing price indices, the focus is not on the price levels of individual artworks, but on the trend in art prices over time. 

There is some empirical evidence that auction prices and private prices are correlated. @Candela2001 is one of the few studies to have access to private art market information. They used the prices from a leading gallery (Prandi) in Italy to construct price indices for private sales for the period 1977-1998. The indices were then compared with price indices based on auction prices. They found a high correlation between the indices, as well as evidence of a cointegrating relationship. They argued that this was due to substitutability between private and auctions markets, arbitrage by dealers between the markets, and the existence of common fundamentals. They also found that auction prices Granger-caused private market prices, while the converse was not true. Their results suggest that auction prices represent a benchmark for private market prices.

##South African Art Auction Prices
Auction prices are the only consistently available price data for the South African art market. This paper therefore relies on publicly available auction prices, similar to virtually all other studies estimating art price indices. As explained above, private sales prices are likely anchored by auction prices and are likely to be highly correlated over time for similar artworks, even if their levels are different [@Olckers2015]. 

Strauss & Co and Stephan Welz & Co are the two local auction houses that have handled the bulk of sales in recent years, with auctions in Cape Town and Johannesburg. Other local auction houses include Bernardi in Pretoria and Russell Kaplan in Johannesburg. Bonhams in London is currently the only major international auction house with a dedicated South African art department, although competition is emerging from Christie's and Sotheby's. Bonhams has two major South African art sales annually. The auction houses follow an open ascending auction, where the winner pays the highest bid. A sale is made only if the hammer price is above the secret reserve price; otherwise, the artwork is unsold and is said to be bought in [@Fedderke2014].

The indices are based on data recorded by AuctionVault. This data covers sales of South African art at eight auction houses[^9] from 2000 to 2015. The database includes 52,059 sales by 4,123 different artists. The following characteristics are available for each auction record: auction house; date of auction; artist's name; title of work; medium; size; whether the artwork is signed, dated and titled; hammer price; and the number of distinct works in the lot. Like most studies, the database lacks information on buy-ins, and therefore the analysis is forced to disregard the potential sample selection problem. The following section briefly discusses the variables available in the database and included in models of art prices.

[^9]: These are: 5th Avenue, Ashbeys, Bernardi, Bonhams, Christies, Russell Kaplan, Stephan Welz & Co and Strauss & Co.

##Artwork Characteristics
*Artist reputation*: Hedonic models typically include dummy variables to control for the artists. Alternatively, a reputation variable can be constructed, either from the art literature, or from the auction data itself. The models in this paper are estimated using a continuous reputation variable estimated with the two-step hedonic approach suggested by @Kraussl2008. This approach allows the use of every auction record, instead of only those auction records that belong to a sub-sample of selected artists. 

*Size*: The most common variable used to describe the physical characteristics of an artwork is its size or surface area. The models use the logarithm of the size of the artwork in $cm^2$. They also include size and medium interaction terms. This is particularly important for sculptures, as the size of a sculpture is usually only recorded in terms of its height (in cm). Squared values are occasionally included to take potential non-linearities into account [@Fedderke2014]. In this sample, however, the relationship does not seem to exhibit an inverted U-shape, and the squared term is positive and economically insignificant in the regression models.

*Auction house*: Dummy variables for the auction houses are also typically included. The more prominent auction houses usually have a positive effect on prices. One reason might be that more renowned auction houses offer higher quality work [@Kraussl2010a]. Thus, the variables might pick up otherwise unobservable quality differences and do not necessarily reflect auction house certification [@Renneboog2012]. Moreover, different auction houses charge different commissions to both buyers and sellers. For example, Strauss & Co reported a buyer's premium of 10%-15%, while Bonhams charged premiums of up to 25% [@Olckers2015]. The hammer prices exclude these premiums and are therefore not a perfect measure of the buyer's cost or the seller's revenue. For the purposes of a price index, the auction house dummies should capture the different premiums charged by the auction houses. 

*Mediums*: Average prices vary across mediums. This might be due to the durability of the medium, the production stage the medium is associated with (e.g. preparatory drawings), or in some cases, the value of the materials used (e.g. sculptures cast in bronze). Oil paintings traditionally earn the highest prices. The availability of copies may decrease the prices of prints and photographs relative to other mediums. Studies typically include dummy variables for the different mediums defined in their data [@Kraussl2010a]. The models in this chapter use the nine mediums defined in the dataset; the same mediums were used by @Olckers2015.

*Authenticity dummies*: Models often include dummies for whether the artwork is signed and dated. There might be a premium for these attributes, as there is less uncertainty about authenticity [@Renneboog2014]. These dummies are included in the models below and are expected to have positive coefficients. 

*Number of works in the lot*: The models below also control for cases in which more than one artwork was sold in the same auction lot. This is because the recorded size corresponds to each artwork separately and not to the group. Moreover, it is possible that lots including more than one artwork fetch a lower price per artwork than if they are sold separately.

#Methodology
##Methodologies for Constructing Art Price Indices
The construction of price indices for unique assets is challenging for at least two reasons [@Jiang2014]. Firstly, the low frequency of trading means that only a subset of the market is traded at a given time, while the prices of non-transacted items are unobservable. Secondly, the heterogeneity of these items means that the quality of assets sold is not constant over time. Thus, the composition of items sold will generally differ between periods, making it difficult to compare prices over time [@Hansen2009]. Constructing an index for unique items, like artworks, therefore requires a different approach than for indices of traditional assets such as stocks and bonds. Four broad measurement techniques have been used to construct these indices [@Eurostat2013]:

a)	Central tendency methods
b)	Hedonic methods
c)	Repeat sales methods
d)	Hybrid methods

The following sections provide a brief introduction to these methodologies. The literature does not provide an a priori indication of the most appropriate method and, in practice, the data dictates the choice of method.

###Central Tendency Methods
The simplest method for constructing a price index is to calculate a measure of the central tendency of the price distribution. The median is often preferred to the mean as a measure of central tendency, because price distributions are generally positively skewed [@Hansen2009]. These average measures have the advantages of being simple to construct and not requiring detailed data. 

Despite its advantages, an index based on average prices does not account for the difficulties mentioned above. For assets such as artworks, central tendency indices may dependent more on the composition, or quality-mix, of assets sold than on changes in the underlying market. For instance, if there is an increase in the share of higher quality assets, a measure based on averages will show an increase in prices, even if the market prices remained constant. Hence, such a measure may not be representative of the price movements of all the assets in the market. If there is a correlation between compositional changes and turning points in asset price cycles, the average could be especially inaccurate [@Hansen2009].

Stratified central tendency measures can control, to some extent, for compositional changes in assets sold over time, by dividing the sample into subgroups according to item attributes such as artist and medium. After constructing a measure of the central tendency for each subgroup, the aggregate mix-adjusted index is typically calculated as a weighted average of the indices for the subgroups (e.g. a Fisher index) [@Eurostat2013]. However, these mix-adjusted measures adjust only for the variation in the quality of assets across the subgroups. The number of subgroups may be increased to reduce the quality-mix problem, if the data permits, although some quality-mix changes will likely remain [@Hansen2009]. However, this will reduce the average number of observations per subgroup and raise the standard error of the overall index [@Eurostat2013]. If the subgroups become very small, small changes can have a large impact on the index. As a consequence of these difficulties, the repeat sales and hedonic methods have dominated the international literature, especially with regard to art price indices. 

Two central tendency price indices are estimated at a quarterly frequency to act as a baseline in comparing the indices resulting from the different methodologies. The median index is simply the median price for each quarter. The Fisher index is a mix-adjusted central tendency index, which is stratified by artist and medium. The base periods are allowed to vary for each index point and the index points are then chained together to form the overall chain-link index.

###The Hedonic Method
The hedonic method is based on hedonic price theory, which is useful for the analysis of differentiated good pricing [@Griliches1961]. The hedonic method is derived from the microeconomic theory of implicit prices, which supposes that utility is derived from the characteristics or attributes of goods [@Lancaster1966]. 

The hedonic method controls for compositional changes by assigning implicit prices to a set of value-adding attributes of an individual item. Thus, the hedonic approach can circumvent the problems of changes in composition or quality-mix over time [@Hansen2009]. Hedonic regressions control for the observable attributes of an item to produce a price index for the 'standard asset' [@Renneboog2002]. The approach entails regressing the logarithm of the sales price on the relevant attributes. The standard hedonic model usually takes the following form: 
$$\ln P_{it} = \sum_{t=1}^T \delta_t D_{it} + \sum_{j=1}^J \beta_{jt} X_{jit} + \sum_{k=1}^K \gamma_{kt} Z_{kit} + \epsilon_{it} ,$$
where $P_{it}$ represents the price of item $i$ at time $t$ $(t=1, ..., T)$; $D_{it}$ is a time dummy variable taking the value of 1 if item $i$ is sold in period $t$ and 0 otherwise; $X_{jit}$ is a set of $j$ $(j=1, ..., J)$ observed attributes of item $i$ at time $t$; $Z_{kit}$ is a set of $k$ $(k=1, ..., K)$ unobserved attributes that also influence the price; and $\epsilon_{it}$ is a random (white noise) error term. 

The coefficients on the time dummies provide an estimate of the average increase in prices between periods, holding the change in any of the measured quality dimensions constant [@Griliches1961]. In other words, they capture the "pure price effect" [@Kraussl2010]. The price index is then simply the series of estimated coefficients: $\hat{\delta_1}, ..., \hat{\delta_T}$.

The most common form of the hedonic equation assumes that the implicit prices (i.e. the coefficients $\beta_t$ and $\gamma_t$) are constant over the entire sample. However, when demand and supply conditions (e.g. tastes) change, the implicit prices of the attributes may change [@Renneboog2012]. One way to allow for shifts in parameters is to employ an adjacent-periods regression [@Triplett2004]. Separate regressions are estimated for adjacent time periods and the sequence of shorter indices are then chain-linked together to form the continuous overall index [@McMillen2012]. This method allows the coefficients, and therefore the implicit prices assigned to the characteristics, to vary in each regression [@Triplett2004]. There is a trade-off in selecting the length of the estimation window. Shorter estimation windows decrease the likelihood of large breaks but also decrease the number of observations used to estimate the parameters [@Dorsey2010]. 

In addition to the standard full sample hedonic index, two adjacent-period indices are calculated by estimating separate models for 1-year and 2-year adjacent sub-samples. A 5-year overlapping-periods index is also estimated, allowing gradual shifts in the implicit prices [@Shimizu2010].

The majority of studies on art price indices have used hedonic models to construct the indices, due to the lack of repeat sales of artworks and the availability of information on many of their important attributes. @Anderson1974 was the first to apply a hedonic regression to art prices. More recent examples include: @Renneboog2002, who estimated an index of Belgian paintings; @Kraussl2010, who studied the prices of the top 500 artists in the world; @Kraussl2010a, who analysed the performance of art in Russia, China and India; and @Kraussl2014 who analysed art from the Middle East and Northern Africa region. 

The primary difficulty with hedonic price indices is this potential omitted variable bias. If the functional form is misspecified or omitted variables are correlated with sales timing, it will result in misspecification or omitted variable bias, which will bias the indices [@Jiang2014]. Although omitted variables are a problem in every model, relatively detailed data is available for art, which should capture a large part of the variation in sales prices. Omitted variable bias should therefore be less of a problem than for other unique assets like real estate, and the omitted variable bias is often small in practice [@Triplett2004; @Renneboog2012].

###The Repeat Sales Method
The repeat sales method provides an alternative method for estimating quality-adjusted price indices, based on price changes in items sold more than once. It was initially proposed by @Bailey1963 to calculate house price changes, was subsequently extended by @Case1987, and is currently used to produce the S&P/Case-Shiller Home Price Indices in the US.

The repeat sales method tracks the sale of the same item over time. It aggregates sales pairs and estimates the average return on the set of items in each period [@Kraussl2010]. As a result, it does not require the measurement of quality, only that the quality of each item be constant over time [@Case1987]. 

The repeat sales model can be derived from the hedonic model, if the hedonic model is differenced with respect to consecutive sales of items that have sold more than once in the sample period [@McMillen2012]. In the standard repeat sales model the dependent variable is regressed on a set of dummy variables corresponding to time periods. This estimating equation provides unbiased estimates of pure time effects without having to correctly specify the item attributes or the functional form of the hedonic equation [@Deng2012]. By differencing the hedonic equation it also potentially controls for omitted variables. It also has the advantage of not being data intensive, as the only information required to estimate the index is the price, the sales date and a unique identifier (e.g. the address of the property). 

A few studies have utilised the repeat sales method to estimate art price indices. These studies have typically relied on very large sales databases, due to the infrequency of repeat sales of individual artworks. @Mei2002 constructed the seminal repeat sales index of art prices for the period 1875-2000. Their methodology is currently used to produce the Mei Moses Art Index for Beautiful Asset Advisors. Other examples include @Korteweg2013 and @Goetzmann2011, who used a database of over a million sales dating back to the 18th century. 

A disadvantage of the repeat sales method is the possibility of sample selection bias. Items that have traded more than once may not be representative of the entire population of items. For example, if cheaper artworks sell more frequently than expensive artworks, but high-quality artworks appreciate faster, a repeat sales index will tend to have a downward bias [@Eurostat2013]. The biggest problem with the repeat sales method in the current context is that single-sale data is discarded. This is problematic because the resale of a specific artwork may only occur infrequently, which substantially reduces the total number of observations available. Only 515 true repeat sales pairs could be identified in the sample, which limits the usefulness of the classical repeated sales approach in this case.

###Hybrid Methods
An interesting perspective, which is relevant to this chapter, is to view the repeat sales specification as an extreme solution to a matching problem. This is because the repeat sales approach requires an exact match to estimate the index. The idea behind the imperfect matching method proposed by @McMillen2012 is that some items may be similar enough to control for many of the differences in (observable and unobservable) attributes. The objective is to match sales observations over time, by some criterion, to cancel out as many as possible of the differences in attributes [@Guo2014].

This paper applies a simple hybrid repeat sales model to art prices for the first time. This procedure is similar in spirit to the "pseudo repeat sales" (ps-RS) procedure suggested by @Guo2014. The first ps-RS sample is created by matching artworks all the hedonic attributes, except the title of the artwork. Matching by this criteria increases the number of repeat sales pairs to 6,642, which includes the 515 true repeat sales or exact matches. The second ps-RS sample allows the sample to increase further by matching on all the hedonic attributes except the title and the presence of a signature and date on the artwork, i.e. the authenticity dummies. This increases the pseudo repeat sales sample to 7,965 sales pairs. 

The differential hedonic equation is then applied to the pseudo repeat sales samples, where artwork $i$ in quarter $t$ and artwork $h$ in quarter $s$ form a matched pair $(t>s)$:
$$\ln P_{it} - \ln P_{hs} = \sum_{j=1}^J \beta_j (X_{itj} - X_{hsj}) + \sum_{t=0}^T \delta_t G_{it} + \epsilon_{iths}$$
where $G_{it}$ is again a time dummy equal to 1 if the later sale occurred in quarter $t$, -1 if the former sale in the pair occurred in quarter $s$, and 0 otherwise; and $\epsilon_{iths}$ again represents a white noise residual.

For the first ps-RS sample, the only remaining independent variable is the difference in the auction house dummies $(X_{it1} - X_{hs1})$. This takes account of possible differences in quality and commission structures. In the second ps-RS sample the independent variables represent the differences in the auction house dummies and the differences in the two authenticity dummies. The independent variables therefore include indicators of the relatively small and easy to measure within-pair differentials in attributes between the two items.

The ps-RS approach mitigates the problem of potential omitted variable bias with the hedonic method. Taking first differences between similar items will control for omitted variables when they are the same for the two items that form the pseudo sales pairs. For example, if Van Gogh's *Sunflowers* paintings are treated as repeat sales, taking first differences would control for attributes such as theme, style, material, prominence, and the stage of the artist's career. Other potentially significant variables might include an array of interaction and non-linear terms. The ps-RS approach also mitigates the problems of small sample sizes and sample selection bias with repeat sales methods by using more of the transaction data [@McMillen2012]. @Calomiris2016 used a similar procedure, based on the differential hedonic equation, in analysing slave price indices. They argued that the similarities between the indices provided confidence that temporal variation in unobservable characteristics were not dictating the results.

There is no consensus regarding the preferred approach of constructing quality-adjusted price indices, either theoretically or empirically. However, there is reason to believe that more advanced measures may provide a better guide to pure price changes than simple central tendency methods [@Hansen2009]. The specific methodology adopted is dependent on the data available. Indices estimated with the different methodologies may provide different results for the bubble detection tests. The danger is that the biases inherent in each methodology may be driving the results (e.g. the omitted variable bias may be correlated with the cycle).

##Bubble Detection Framework
The adverse effects of bubbles and their related crises have led to a large literature on financial crises and the detection of bubbles in asset prices, including the seminal work by @Kindleberger2005 and the modelling approach by @Phillips2011. 

The most common bubble detection methods are based on the present value model and a rational bubble assumption [@Yiu2013]. According to the present value model, under rational expectations, the price of an asset is equal to the present value of its future income stream, i.e. the expected fundamental value: $$P_t = \frac{1}{1+r_f} E_t(P_{t+1} + \gamma_{t+1}) ,$$ where $r_f$ is the constant discount rate, $P_t$ is the asset price at time $t$, and $\gamma_{t+1}$ is the payment received (e.g. dividends, rents or a convenience yield) for owning the asset between period $t$ and $t+1$. When $t+n$ is far into the future, $\frac{1}{1+r_f} E_t(P_{t+n})$ does not affect $P_t$, as it tends to zero as $n$ becomes infinitely large. The present value or market fundamental solution may be written as: $$F_t = E_t[\sum_{i=1}^n \frac{1}{1+r_f} (\gamma_{t+n})]$$ 

Rational bubbles occur when buyers are willing to pay more for an asset than the fundamental, as they expect the future price to be higher than its fundamental value [@Yiu2013]. If a rational bubble occurs, the asset price consists of a fundamental component and a bubble component. In other words, if there is a gap between the fundamental value and the actual price, an additional 'bubble component', $B_t$, is added to the solution of equation: $P_t = F_t + B_t$. In this case $F_t$ is called the fundamental component of the price, and $B_t$ is a random variable of the following form: $$B_t = \frac{1}{1+r_f} E_t(B_{t+n})$$. 

Thus, the bubble component is included in the equation, with an expected value in period $t+1$ of $B_t$ multiplied by $(1 + r_f)$. The bubble component is called a 'rational bubble', as it is in line with the rational expectations framework [@Kraussl2016].

The statistical properties of $P_t$ are determined by those of $F_t$ and $B_t$. In the absence of a bubble, when $B_t=0$, the degree of non-stationarity in $P_t$ is determined by the series $F_t$, which in turn is determined by $\gamma_t$. The current price of the asset is therefore determined by market fundamentals: for example, if $\gamma_t$ is an I(1) process, then $P_t$ would be an I(1) process. 

When a bubble is present, if $B_t \neq 0$, current prices $P_t$ will exhibit explosive behaviour, as $B_t$ is a stochastic process for which the expected value in period $t+1$ is greater than or equal to the value in period $t$ [@Kraussl2016]. If there is no structural change in the fundamental process or explosiveness in the fundamentals, a period of explosive prices has a non-fundamental explanation. Mildly explosive behaviour in $P_t$ (i.e. non-stationarity greater than a unit root) provides evidence of bubble-like behaviour. According to this theory, if a bubble exists, prices should inherit its explosive property [@Areal2013]. Statistical tests can be formulated to detect evidence of explosiveness in the price series [@Caspi2013].

Early tests were based on unit root and cointegration tests. @Campbell1987 suggested a unit root test for explosiveness in prices, based on the idea that during the process of bubble formation, the gap between the asset price and the fundamental value will exhibit explosive behaviour. They identified two scenarios in which the presence of a rational bubble is implied. In the first case, the asset price is non-stationary while the fundamental value is stationary. In the second, the asset price and fundamental value are both non-stationary [@Yiu2013]. In this case, if the asset price and its fundamental value are not cointegrated, their non-stationary behaviour provides evidence of a bubble. @Diba1988 showed that explosive behaviour in prices is a sufficient condition for the presence of a bubble, if the fundamental value is not explosive. 

However, unit root and cointegration tests are incapable of detecting explosive prices when a series contains periodically collapsing bubbles. @Evans1991 argued that explosive behaviour is only temporary, as bubbles eventually collapse, which means that explosive asset prices may appear more like stationary or $I(1)$ series. Using simulated data, @Evans1991 showed that bubble detection tests could not differentiate between a stationary process and a periodically collapsing bubble. A series with periodically collapsing bubbles could therefore be interpreted by the standard unit root tests as a stationary series, leading to the incorrect conclusion that the series contained no explosive behaviour [@Phillips2011]. 

A number of methods have been proposed to deal with this critique [@Yiu2013]. The recursive tests proposed by @Phillips2011 and @Phillips2012 are not subject to this criticism and can effectively distinguish between unit root processes and periodically collapsing bubbles, as well as identify the dates of their origin and collapse. The test proposed by @Phillips2011 involves repeatedly implementing a right-tailed unit root test. The method involves estimating an autoregressive model, starting with a minimum sample window size and incrementally expanding the window. 

The model typically takes the following form:
$$\Delta y_t = \alpha_w + (\delta_w - 1) y_{t-1} + \sum_{i=1}^k \phi_w^i \Delta y_{t-i} + \epsilon_t$$
where $y_t$ is the asset price series, $\alpha$, $\delta$ and $\phi$ are the parameters to be estimated, $w$ is the sample window size, $k$ is the lag order, and $\epsilon_t$ is the white noise error term. 

The Augmented Dickey-Fuller test statistics are calculated from each regression. The null hypothesis of a unit root $(\delta = 1)$ is tested against the right-tailed alternative of mildly explosive behaviour $(\delta > 1)$. The supremum value of the ADF sequence is then used to test for mildly explosive behaviour. By testing directly for explosive behaviour, the test avoids the risk of misinterpreting a rejection of the null hypothesis due to stationary behaviour. 

The method also allows for date-stamping of the origination and termination dates, by comparing the time series of the test statistics with the critical value sequence. In other words, to identify a bubble period, each ADF test statistic is compared with the corresponding right-tailed critical value. The origination point of a bubble is the first observation in which ADF statistic exceeds the corresponding critical value (from below), while the termination point is the first subsequent observation when the ADF statistic falls below the critical value [@Caspi2013]. 

Various studies have used the method to investigate bubbles in a number of asset markets, including real estate [@Jiang2014; @Balcilar2015], commodities [@Areal2013; @Figuerola2015] and art [@Kraussl2016]. The results from these studies have often suggested the existence of periods of explosive prices between 2006/07 and 2008. 

#Results 
##Art Price Indices
```{r naive, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##----------------------##
##---CENTRAL TENDENCY---##
##----------------------##
naive_index <- aggregate(artdata$price, by=list(artdata$timedummy), FUN=median, na.rm=TRUE)
naive_index$index <- naive_index$x
naive_index$index <- naive_index$index/naive_index[1,2]*100
naive_index$index <- as.numeric(naive_index$index)
colnames(naive_index) <- c("Date","Median","Median_Index")

#------------------------------
#Stratify by artist and medium
#get quantity and median price per group per quarter    
strat_p <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=median)
strat_q <- aggregate(artdata$price, by=list(artdata$timedummy, artdata$artist, artdata$med_code), FUN=sum)
strat_q$x <- strat_q$x/strat_p$x        #the count q

chain2 <- function(strat_p, strat_q, kwartaal1,kwartaal2) {
    strat_p1 <- subset(strat_p, strat_p$Group.1==kwartaal1)
    strat_q1 <- subset(strat_q, strat_q$Group.1==kwartaal1)
    strat_p2 <- subset(strat_p, strat_p$Group.1==kwartaal2)
    strat_q2 <- subset(strat_q, strat_q$Group.1==kwartaal2)
    #get sample of median prices and quantities for specific artist for the two quarters
    strat_pc <- merge(strat_p1, strat_p2, by=c("Group.2","Group.3"))
    strat_qc <- merge(strat_q1, strat_q2, by=c("Group.2","Group.3"))
    #Laspeyres (keeps quantity weights fixed at base)
    Lasp <- sum(strat_pc$x.y*strat_qc$x.x,na.rm=TRUE)/sum(strat_pc$x.x*strat_qc$x.x,na.rm=TRUE)
    #Paasche (keeps quantity weights fixed at end)
    Paas <- sum(strat_pc$x.y*strat_qc$x.y,na.rm=TRUE)/sum(strat_pc$x.x*strat_qc$x.y,na.rm=TRUE)
    return(c(Lasp,Paas))
}

datum <- levels(artdata$timedummy)
ketting2 <- chain2(strat_p,strat_q,datum[1],datum[2])
ketting2 <- rbind(ketting2,chain2(strat_p,strat_q,datum[2],datum[3]))
for(i in 3:63) {
    ketting2 <- rbind(ketting2,chain2(strat_p,strat_q,datum[i],datum[(i+1)]))
}
ketting2 <- as.data.frame(ketting2)
ketting2$V3 <- sqrt(ketting2[,1]*ketting2[,2])  #Fisher index is the geometric mean
ketting2$V4[1] <- ketting2$V3[1]*100
for(i in 2:63) {                                #use the growth rates to generate the index
    ketting2$V4[i] <- ketting2$V4[(i-1)]*ketting2$V3[i]
}
ketting2$Date <- as.factor(datum[-1])
colnames(ketting2) <- c("Las","Paas","Fisher","Fisher_Index","Date")

naive_index <- merge(naive_index, ketting2, by.x="Date", by.y="Date",all=TRUE)
naive_index[1,4:7] <- 100
naive_indices <- naive_index[,c(1,3,7)]
```

```{r hedonicrep, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#-------------------
# FULL SAMPLE MODEL
#-------------------
full_model <- function(data, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                 "nr_works","artist","timedummy")) {
    modeldata <- data
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    model_all <- lm(expl_vars, data=modeldata)
    time_results <- summary(model_all)$coefficients[grepl("timedummy", rownames(summary(model_all)$coefficients)),1]
    time_results <- as.data.frame(time_results)
    time_results$Index <- exp(time_results$time_results)*100
    time_results$Date <- sub("timedummy","",row.names(time_results))
    time_results <- merge(datums,time_results,by="Date",all=TRUE)[,c(1,4)]
    return(time_results)
}

#-----------------------------
# OVERLAPPING PERIODS (1-year)
#-----------------------------
overlap1y_model <- function(data, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                      "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:16) {
        modeldata <- data
        modeldata <- subset(modeldata, modeldata$counter>(i*4-5)& modeldata$counter<(i*4+1))
        model <- lm(expl_vars, data=modeldata)  
        time_results <- as.data.frame(summary(model)$coefficients[grepl("timedummy", rownames(summary(model)$coefficients)),1])
        time_results$Date <- sub("timedummy","",row.names(time_results))
        colnames(time_results) <- c("Coef","Date")
        res_list[[i]] <- time_results
    }
    #Merge all results
    overlap <- rep_results
    overlap <- merge(overlap, res_list[[1]], by="Date", all=TRUE)
    overlap[,3] <- exp(overlap[,3])*100
    for(i in 2:16) {
        overlap <- merge(overlap, res_list[[i]], by = "Date",all=TRUE)
        overlap[,(i+2)] <- exp(overlap[i+2])*100
    } 
    #Calculate index
    overlap$ind <- overlap[,3]
    overlap[2,19] <- overlap[3,19]*overlap[2,2]/overlap[3,2]   #Interpolate
    overlap$teller <- as.numeric(substring(overlap$Date,1,4))-1997
        
    for(i in 3:62) {
        j <- overlap[(i+1),20]
        if(is.na(overlap[i,j])) {
            overlap[(i+1),19] <- overlap[i,19]*overlap[(i+1),j]/100
        } else { 
            overlap[(i+1),19] <- overlap[i,19]*overlap[(i+1),j]/overlap[i,j] 
        }   
    }
    colnames(overlap) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5","Index_m6",
                           "Index_m7","Index_m8","Index_m9","Index_m10","Index_m11","Index_m12","Index_m13",
                           "Index_m14","Index_m15","Index_m16","Index_Adjacent1y","teller")
    overlap$Date <- factor(levels(artdata$timedummy)[-1])
    return(overlap)
}

#-----------------------------
# OVERLAPPING PERIODS (2-year)
#-----------------------------
overlap2y_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                      "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:8) {
        modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*8-9)& modeldata$counter<(i*8+1))
        model <- lm(expl_vars, data=modeldata)  
        time_results <- as.data.frame(summary(model)$coefficients[grepl("timedummy", rownames(summary(model)$coefficients)),1])
        time_results$Date <- sub("timedummy","",row.names(time_results))
        colnames(time_results) <- c("Coef","Date")
        res_list[[i]] <- time_results
    }
    #Merge all results
    overlap2 <- rep_results
    overlap2 <- merge(overlap2, res_list[[1]], by="Date", all=TRUE)
    overlap2[,3] <- exp(overlap2[,3])*100
    for(i in 2:8) {
        overlap2 <- merge(overlap2, res_list[[i]], by = "Date", all=TRUE)
        overlap2[,(i+2)] <- exp(overlap2[i+2])*100
    } 
    #Calculate index
    overlap2$ind <- overlap2[,3]
    overlap2[2,11] <- overlap2[3,11]*overlap2[2,2]/overlap2[3,2]   #Interpolate
    overlap2$teller <- c(3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,
                         7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10)
    for(i in 7:62) {
        j <- overlap2[(i+1),12]
        if(is.na(overlap2[i,j])) {
            overlap2[(i+1),11] <- overlap2[i,11]*overlap2[(i+1),j]/100
        } else { 
            overlap2[(i+1),11] <- overlap2[i,11]*overlap2[(i+1),j]/overlap2[i,j] 
        }   
    }
    colnames(overlap2) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5",
                            "Index_m6","Index_m7","Index_m8","Index_Adj2y","teller")
    overlap2$Date <- factor(levels(artdata$timedummy)[-1])
    return(overlap2)
}

#----------------------
#ROLLING 5-YEAR WINDOWS
#----------------------
rolling_model <- function(artdata, list_expl_vars=c("lnarea","ah_code","med_code","lnsculpt_area","dum_signed", "dum_dated",  
                                                    "nr_works","artist","timedummy")) {
    expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
    res_list <- list()
    for(i in 1:12) {
        modeldata <- artdata
        modeldata <- subset(modeldata, modeldata$counter>(i*4-4)&modeldata$counter<(i*4+17))
        model <- lm(expl_vars, data=modeldata)  
        time_results <- as.data.frame(summary(model)$coefficients[grepl("timedummy", rownames(summary(model)$coefficients)),1])
        time_results$Date <- sub("timedummy","",row.names(time_results))
        colnames(time_results) <- c("Coef","Date")
        res_list[[i]] <- time_results    
    }
    
    #Merge all results
    rolling <- rep_results
    rolling <- merge(rolling, res_list[[1]], by="Date", all=TRUE)
    rolling[,3] <- exp(rolling[,3])*100
    for(i in 2:12) {
        rolling <- merge(rolling, res_list[[i]], by = "Date", all=TRUE)
        rolling[,(i+2)] <- exp(rolling[i+2])*100
    }    
    
    #Calculate index
    rolling$ind <- rolling[,3]
    rolling[2,15] <- rolling[3,15]*rolling[2,2]/rolling[3,2]  #interpolate
    rolling$teller <- c(3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,
                        10,10,10,10,11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14)
    for(i in 19:62) {  #chaining
        j <- rolling[(i+1),16]
        rolling[(i+1),15] <- rolling[i,15]*rolling[(i+1),j]/rolling[i,j]
    }
    
    colnames(rolling) <- c("Date","Index_Full","Index_m1","Index_m2","Index_m3","Index_m4","Index_m5","Index_m6",
                           "Index_m7","Index_m8","Index_m9","Index_m10","Index_m11","Index_m12","Index_Rolling","teller")
    rolling$Date <- factor(levels(artdata$timedummy)[-1])
    return(rolling)
}

#========================================================================================
list_expl_vars <- c("lnarea","ah_code","med_code","dum_signed","dum_dated",  
                    "nr_works","lnrep","lnarea:med_code","timedummy")
rep_results <- full_model(artdata,list_expl_vars)
suppressMessages(rep_overlap1 <- overlap1y_model(artdata,list_expl_vars))
suppressMessages(rep_overlap2 <- overlap2y_model(artdata,list_expl_vars))
suppressMessages(rep_rolling <- rolling_model(artdata,list_expl_vars))

#----------------------------------------------
hedonic_indices <- rep_overlap1[,c(1,2)]
colnames(hedonic_indices) <- c("Date","Hedonic_full")
hedonic_indices <- cbind(hedonic_indices,Adjacent_1y=rep_overlap1[,19])
hedonic_indices <- cbind(hedonic_indices,Adjacent_2y=rep_overlap2[,11])
hedonic_indices <- cbind(hedonic_indices,Rolling_5y=rep_rolling[,15])
hedonic_indices <- cbind(Date=factor(levels(artdata$timedummy)),
                         rbind(c(seq(100,100, length.out=4)),hedonic_indices[,-1]))
```

```{r repeatsales, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
##=====================##
## REPEAT SALES METHOD ##
##=====================##
#check for duplicates (how many)
sum(duplicated(artdata[,c("artist","title","med_code","area","dum_signed","dum_dated","nr_works")]))
allDup <- function(value) {  #identify duplicated values
    duplicated(value) | duplicated(value, fromLast = TRUE)
}
rsartdata <- artdata[allDup(artdata[,c("artist","title","med_code","area","dum_signed","dum_dated","nr_works")]),]
rsartdata <- transform(rsartdata, id = as.numeric(interaction(artist,factor(title),med_code,factor(area),factor(dum_signed),
                                                              factor(dum_dated), factor(nr_works),drop=TRUE)))

repdata <- repsaledata(rsartdata$lnprice,rsartdata$counter,rsartdata$id)  #transform the data to sales pairs
repdata <- repdata[complete.cases(repdata),]
repeatsales <- repsale(repdata$price0,repdata$time0,repdata$price1,repdata$time1,mergefirst=1,
                       graph=FALSE)   #generate the repeat sales index
RS_index <- exp(as.data.frame(repeatsales$pindex))*100   

n <- as.numeric(sub("Time ","",row.names(RS_index)))
n[1] <- 1
RS_index$Date <- levels(rsartdata$timedummy)[unique(c(repdata$time0,repdata$time1))[order(unique(c(repdata$time0,repdata$time1)))]][n] #missing values

RS_index <- merge(datums,RS_index, by="Date", all=TRUE)[,-2]
colnames(RS_index) <- c("Date","Repeat Sales_Index")      
```

```{r psRSsample, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#do the same but expand it to not match by title - i.e. all other attributes are the same
sum(duplicated(artdata[,c("artist","med_code","area","dum_signed","dum_dated","nr_works")]))
rsartdata1 <- artdata[allDup(artdata[,c("artist","med_code","area","dum_signed","dum_dated","nr_works")]),]
rsartdata1 <- transform(rsartdata1, id = as.numeric(interaction(artist,med_code,factor(area),factor(dum_signed),
                                                                factor(dum_dated),factor(nr_works), drop=TRUE)))

repdata1 <- cbind(repsaledata(rsartdata1$lnprice,rsartdata1$counter,rsartdata1$id),
                  repsaledata(rsartdata1$ah_code,rsartdata1$counter,rsartdata1$id)[,4:5]) #transform the data to sales pairs
repdata1 <- repdata1[complete.cases(repdata1),]
colnames(repdata1) <- c("id","time0","time1","price0","price1","ah_code0","ah_code1")

dy <- repdata1$price1 - repdata1$price0
ah0 <- model.matrix(~repdata1$ah_code0)
ah1 <- model.matrix(~repdata1$ah_code1)
dah <- ah1 - ah0

timevar <- levels(factor(c(repdata1$time0, repdata1$time1)))
nt = length(timevar)
n = length(dy)
xmat <- array(0, dim = c(n, nt - 1))
for (j in seq(1 + 1, nt)) {
    xmat[,j-1] <- ifelse(repdata1$time1 == timevar[j], 1, xmat[,j-1])
    xmat[,j-1] <- ifelse(repdata1$time0 == timevar[j],-1, xmat[,j-1])
}
colnames(xmat) <- paste("Time", seq(1 + 1, nt))

ps.RS <- lm(dy ~ dah + xmat + 0)
RS_index1 <- summary(ps.RS)$coefficients[grepl("Time", rownames(summary(ps.RS)$coefficients)),1]
RS_index1 <- as.data.frame(RS_index1)
RS_index1$index <- exp(RS_index1$RS_index1)*100
RS_index1$Date <- levels(rsartdata1$timedummy)[-1]
RS_index1 <- RS_index1[,c(2,3)]

##=====================##
#do the same but expand it to not match by title or authenticity dummies
sum(duplicated(artdata[,c("artist","med_code","area","nr_works")]))
rsartdata2 <- artdata[allDup(artdata[,c("artist","med_code","area","nr_works")]),]
rsartdata2 <- transform(rsartdata2, id = as.numeric(interaction(artist,med_code,factor(area),factor(nr_works), drop=TRUE)))

repdata2 <- cbind(repsaledata(rsartdata2$lnprice,rsartdata2$counter,rsartdata2$id),
                  repsaledata(rsartdata2$ah_code,rsartdata2$counter,rsartdata2$id)[,4:5],
                  repsaledata(rsartdata2$dum_signed,rsartdata2$counter,rsartdata2$id)[,4:5],
                  repsaledata(rsartdata2$dum_dated,rsartdata2$counter,rsartdata2$id)[,4:5])
repdata2 <- repdata2[complete.cases(repdata2),]
colnames(repdata2) <- c("id","time0","time1","price0","price1","ah_code0","ah_code1","sign0","sign1","date0","date1")

dy <- repdata2$price1 - repdata2$price0
dsign <- repdata2$sign1 - repdata2$sign0
ddate <- repdata2$date1 - repdata2$date0
ah0 <- model.matrix(~repdata2$ah_code0)
ah1 <- model.matrix(~repdata2$ah_code1)
dah <- ah1 - ah0

timevar <- levels(factor(c(repdata2$time0, repdata2$time1)))
nt = length(timevar)
n = length(dy)
xmat <- array(0, dim = c(n, nt - 1))
for (j in seq(1 + 1, nt)) {
    xmat[,j-1] <- ifelse(repdata2$time1 == timevar[j], 1, xmat[,j-1])
    xmat[,j-1] <- ifelse(repdata2$time0 == timevar[j],-1, xmat[,j-1])
}
colnames(xmat) <- paste("Time", seq(1 + 1, nt))

ps.RS <- lm(dy ~ dah + dsign + ddate + xmat + 0)
RS_index2 <- summary(ps.RS)$coefficients[grepl("Time", rownames(summary(ps.RS)$coefficients)),1]
RS_index2 <- as.data.frame(RS_index2)
RS_index2$index <- exp(RS_index2$RS_index2)*100
RS_index2$Date <- levels(rsartdata2$timedummy)[-1]
RS_index2 <- RS_index2[,c(2,3)]

#------------------------------------------------------------------------
RS_indices <- merge(RS_index, RS_index1, by="Date", all=TRUE)
RS_indices <- merge(RS_indices, RS_index2, by="Date", all=TRUE)
RS_indices <- cbind(Date=factor(levels(artdata$timedummy)),
                         rbind(c(seq(100,100, length.out=3)),RS_indices[,-1]))
colnames(RS_indices) <- c("Date","Repeat Sales","pseudo-RS1","pseudo-RS2") 
```

Figure 1 illustrates representative indices for the three methodologies: median index, the 1-year adjacent-period hedonic index and the second version (larger sample) of the ps-RS index. The two regression-based indices point to a similar general trend in South African art prices. The simple median index, on the other hand, does not reflect this trend and is much more volatile than the regression-based indices. 

The hedonic and ps-RS indices exhibit similar trends over the sample period, although the hedonic index is at a lower level after 2009. Both measures indicate that the average price of a quality-adjusted artwork increased significantly between 2005 and 2008 and then declined sharply after the financial crisis, similar to other asset prices [@Shimizu2010]. Both indices are relatively flat after 2009 in nominal terms, implying that art prices decreased in real terms over latter part of the sample period. 

The fact that the regression-based indices are similar, even when the hybrid repeat sales indices are based on smaller subsamples of the data, implies that the potential omitted variable and sample selection bias may not be too pervasive in this case. The ps-RS method acts as an internal validity test, to check that the results are not driven by the inherent biases of a specific method.

Table 1 reports the correlations in the growth rates between the various indices. There is a significant positive correlation between the regression-based indices. This indicates that their general trends are similar, and are different from the simple median. The Fisher central tendency index is also significantly positively correlated with the hedonic indices. This shows that there is some consistency in the estimates from the different methodologies, which provides some confidence that the indices provide a reasonably accurate measure of the price movements in the South African art market

```{r figure1, echo=FALSE, cache = TRUE, warning=FALSE, fig.height=4, fig.width=7.5, fig.cap="Comparing South African art price indices (2000Q1=100)"}
all_indices <- cbind(naive_indices,hedonic_indices[-1],RS_indices[-1])
all_indices[is.na(all_indices)]<- 100
index_plot <- melt(all_indices[,c(1,2,5,10)], id="Date")  # convert to long format
index_plot$Date <- as.Date(as.yearqtr(index_plot$Date, format = "%Y Q%q"))
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line(aes(linetype=variable))
g <- g + scale_linetype_manual(values = c(4,1,2))
g <- g + ylab("Index") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g
```

```{r table2, echo=FALSE, results='asis', message=FALSE, warning=FALSE, cache = TRUE}
source("corstarsl.R")
temp_indices <- all_indices[-1:-3,]
colnames(temp_indices) <- c("Date","Median","Fisher","Hedonic","Adjacent-1y","Adjacent-2y","Rolling-5y","Repeat Sales","ps-RS1","ps-RS2")
for(i in 2:ncol(temp_indices)) {temp_indices[,i] <- as.numeric(temp_indices[,i]) }
ts.all_indices <- as.ts(temp_indices[,-1],start =c(2000,4),end=c(2015,4),frequency=4) 
returns <- as.data.frame(diff(log(ts.all_indices)))
xt <- xtable(corstarsl(returns), caption="Correlations between art price indices (in growth rates)")
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.8)
```

###Evaluation
In this section, the indices produced with the different methodologies are evaluated, in order to assess which index provides the most accurate measure of South African art prices over time. In other applications, the quality of price indices has often been evaluated based on the diagnostic metrics of the underlying regressions, such as the standard errors of the residuals (see e.g. @Hansen2009). 

However, @Guo2014 argued that the regression residuals do not reflect errors in the price index itself, and hence do not directly reflect inaccuracy in the index returns. Even if an index is a perfectly accurate measure of the central tendency of price changes, the regression would still have residuals, and the time dummy coefficients may still exhibit large standard errors, resulting simply from the dispersion of individual art prices around the central tendency. When datasets become large, regression diagnostics can become impressive simply due to the size of the sample. In such cases, tests of economic significance are more valuable than tests of statistical significance. In this case, not all of the indices were generated with regression models. The regressions models that were employed differ in their specifications (in levels or first differences) and the underlying data sets used for estimation. 

@Guo2014 suggested that signal-to-noise or smoothness metrics are more appropriate for judging the quality of the price index, as they are based directly on the index, as opposed to the underlying model. Signal-to-noise metrics directly reflect the accuracy of the index returns and the economic significance of random error in the indices. Random error in the coefficient estimation leads to 'noise' in the index. The volatility $Vol$ and the first-order autocorrelations $AC(1)$ of the index returns are signal-to-noise metrics that may be useful in comparing the amount of noise in the indices.

Volatility is a measure of the dispersion in returns over time. There is always true volatility, as the true market prices change over time. An ideal price index will filter the volatility induced by the noise to leave only the true market volatility. In addition to the true volatility, the noise in the index causes excess volatility in the index returns. Excess volatility decreases the first-order autocorrelation of the index returns. Less noise (lower $\sigma_\eta^2$) will lead to lower index volatility and higher AC(1). Other things being equal, the lower the volatility and the higher the AC(1), the less noisy and more accurate the index. Thus, lower Vol or higher AC(1) will indicate a more accurate art price index in the sense of less noise or random error.

@Guo2014 suggested another test of index quality in terms of minimising random error that is based on the Hodrick-Prescott (HP) filter. The HP filter is a spline-fitting procedure that divides a time series into smoothed trend and cyclical components. The idea is to examine which index has the least deviation from its smoothed HP component, by comparing the sum of squared differences between the index returns and the smoothed returns. 

Another option is to compare the smoothness coefficients proposed by @Froeb1994. The smoothness coefficient is defined as the average long-run variance of a time series divided by the average short-run variance. The idea is to obtain the spectral density of the time series, which shows the contribution of all frequencies to the data series. The smoothness measure is then taken as the average of the lower half of the frequency range (i.e. the low-frequency or longer-term movements) over the average of the upper half of the frequencies (i.e. the high-frequencies or shorter-term movements). A higher smoothness coefficient indicates a larger share of variance in the low frequencies and a smoother time series. 

Table 2 reports these four metrics of index smoothness for the art price indices. The comparison suggests that that the regression-based indices are much smoother than the central tendency measures and the classical repeat sales index. The volatilities, autocorrelations and HP filter deviations of the regression-based indices are around the same size. The 1-year adjacent-period hedonic index performs the best in terms of these metrics, with the lowest volatility $Vol$ and highest autocorrelation $AC(1)$ in returns, the smallest deviation from its smoothed returns, and the highest smoothness coefficient. However, the smoothness coefficients of the regression-based indices are not significantly different.

```{r table2, echo=FALSE, results='asis', warning=FALSE, message=FALSE, cache = TRUE}
# Check std dev or volatility en AC(1)
ac.1 <- numeric()
eval <- data.frame()
HPdev <- numeric()
smoothness <- numeric()

vol <- apply(returns,MARGIN=2, FUN=sd, na.rm=TRUE)
for(i in 1:ncol(returns)) {
    ac.1[i] <- acf(returns,na.action = na.pass, plot = FALSE, lag.max = 1)$acf[,,i][2,i]
}

hp <- temp_indices
for(i in 2:10) {
    hp[,i] <- hpfilter(temp_indices[,i],freq = 1600)[2]
}
ts.hp <- as.ts(hp[,-1],start =c(2000,1),end=c(2015,4),frequency=4)
hpreturns <- as.data.frame(diff(log(ts.hp)))
for(i in 1:ncol(returns)) {
    HPdev[i] <- sum((hpreturns[,i] - returns[,i])^2)
}

#spectral density
smooth <- function (datavec,k,l) {     # calculates smoothness coefficient for 'datavec' with 
    # 'k' specifies the width of the Daniell window which smooths the raw periodogram 
    ## Step 1: Calculate and record power spectral density using 'speccalcs'
    speccalcs <- spec.pgram(datavec,spans=c(k,l),demean=TRUE,plot=FALSE)
    spectra <- speccalcs$spec
    
    ## Step 2 Take natural logs of power spectral frequencies
    logspec <- log(spectra)
    n <- length(logspec)
    m <- n/2
    p1 <- mean(logspec[1:m])
    p2 <- mean(logspec[(m+1):n])
    smcoef <- p1-p2
    smcoefvar <- (pi^2)/6*((1/m)+(1/(n-m)))
    smcoefse <- sqrt(smcoefvar)
    #list(smcoef,smcoefse)
    return(smcoef)
}
for(i in 1:10) { smoothness[i] <- smooth(all_indices[,i],3,3) }

eval <- cbind(Vol=vol,AC.1=ac.1[1:9],HPDeviation=HPdev,Smoothness=smoothness[-1])
colnames(eval) <- c("Vol","AC(1)","HP-Deviation","Smoothness")
row.names(eval) <- c("Median","Fisher","Hedonic","Adjacent-1y","Adjacent-2y","Rolling-5y","Repeat Sales","ps-RS1","ps-RS2")
xt <- xtable(eval, caption="Smoothness indicators", digits=c(0,3,3,2,2))
print(xt, "latex",comment=FALSE, caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9) 
```

##Bubble Detection Results
In this section the South African art market is tested for bubble-like behaviour over the sample period, focusing on a specific aspect of bubbles: explosive prices. This section follows the convention of using the log value of real asset prices, deflated with the CPI (e.g. @Kraussl2016, @Caspi2013 and @Balcilar2015). In this case, there was only one potential bubble episode, so the @Phillips2011 method is sufficient to provide evidence of mildly explosive behaviour.

As explained above, the method involves estimating an autoregressive model, starting with a minimum fraction of the sample and incrementally expanding the sample window. The model starts with 3 years (i.e. 12 observations) and expands the sample by 1 observation each time. Each estimation yields an ADF statistic. In this case, there did not seem to be a deterministic drift present in the log real art price indices, and the intercept was not statistically significant at conventional levels. However, as the results could have been sensitive to model formulation, two versions of the autoregressive models were used: one without a constant or drift term and one with a drift term. Lags were included to take possible autocorrelation of the residuals into account, and the number of lags $k$ was chosen with the Akaike Information Criterion. 

Critical values for the tests were derived from Monte Carlo simulations of a random walk process, both including and excluding a drift term, with 2000 replications. In their original study, @Phillips2011 use a random walk without drift to estimate the null hypothesis. According to @Phillips2014, when the model is estimated with a non-zero drift, it produces a dominating deterministic component that is unrealistic for most economic and financial time series. They argue that a more realistic description of explosive behaviour is given by models formulated without a constant or deterministic trend. Nevertheless, as a robustness check, the models were formulated with and without a constant, or drift term, included. 

```{r bubbles, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#==============================#
# Bubbles: Explosive Behaviour
#==============================#
#Make them real
real_indices <- all_indices
for(i in 2:ncol(all_indices)) { 
    for(j in 1:64) {
        real_indices[j,i] <- all_indices[j,i]/assets$CPI[j]*100 
    }
}

#Calculate test statistics
y_indices <- log(real_indices[,-1])
bubble.nc <- list()
bubble.c <- list()
for(i in 1:ncol(y_indices)) {
    bubble1 <- numeric()
    bubble2 <- numeric()
    for(j in 12:64) {
        y <- y_indices[1:j,i]
        toets1 <- ur.df(y, type= "none", lags = 4, selectlags = c("AIC"))
        toets2 <- ur.df(y, type= "drift", lags = 4, selectlags = c("AIC"))
        bubble1 <- rbind(bubble1,toets1@teststat)
        bubble2 <- rbind(bubble2,toets2@teststat)
    }
    bubble.nc[[i]] <- bubble1
    bubble.c[[i]] <- bubble2
}

##--------------------------------------------------------------------------
#Calculate critical values
K1 <- numeric()
K2 <- numeric()
K3 <- numeric()
K4 <- numeric()

for(j in 12:64) {
    set.seed(123)                           #for replicability
    reps <- 2000                            #Monte Carlo replications
    burn <- 100                             #burn in periods: first generate a T+B sample
    #obs <- 62                              #To make "sure" that influence of initial values has faded
    obs <- j                                #ultimate sample size
    tstat.nc <- numeric()
    tstat.c <- numeric()
    tstat.ct <- numeric()
    tstat.lc <- numeric()
    
    for(i in 1:reps) {     
        e <- rnorm(obs+burn)
        e[1] <- 0
        Y1 <- cumsum(e)
        DY1 <- diff(Y1)
        
        y1 <- Y1[(burn+1):(obs+burn)]               #trim off burn period
        dy1 <- DY1[(burn+1):(obs+burn)]             
        ly1 <- Y1[burn:(obs+burn-1)] 
        trend <- 1:obs
        
        EQ1 <- lm(dy1 ~ 0 + ly1)       
        tstat.nc <- rbind(tstat.nc,summary(EQ1)$coefficients[1,3]) 
        EQ2 <- lm(dy1 ~ ly1)            
        tstat.c <- rbind(tstat.c,summary(EQ2)$coefficients[2,3])  
    }                                       
    #hist(tstat.nc)
    K1 <- rbind(K1,quantile(tstat.nc, probs=c(0.9,0.95,0.99)))
    K2 <- rbind(K2,quantile(tstat.c, probs=c(0.9,0.95,0.99)))
}   #Provides a vector of critical values

bubble.test1 <- numeric()
bubble.test2 <- numeric()

for(k in 1:9) { 
    bubble.test1 <- cbind(bubble.test1,bubble.nc[[k]])
    bubble.test2 <- cbind(bubble.test2,bubble.c[[k]][1:53])
}
bubble.test1 <- as.data.frame(bubble.test1)
bubble.test2 <- as.data.frame(bubble.test2)
bubble.test1 <- cbind(bubble.test1,K1)
bubble.test2 <- cbind(bubble.test2,K2)

Dates <- levels(artdata$timedummy)[-1:-11]
bubble.test1$Date <- Dates
bubble.test2$Date <- Dates

colnames(bubble.test1)[1:9] <- colnames(all_indices[-1])
colnames(bubble.test2)[1:9] <- colnames(all_indices[-1])
```

The supremum ADF test statistics from both formulations are above the 95% critical values for each of the indices, except for the median index. Therefore, the null hypothesis of a unit root may be rejected in favour of the alternative hypothesis for each of the indices, except the median index. This provides evidence that real art prices experienced periods of explosiveness over the sample period.

The method can be used to date stamp potential bubble periods. Figure 2 illustrates the date stamping procedure for three representative series: median values, the 1-year adjacent-period hedonic index and the second version (larger sample) of the ps-RS index. The figure compares the ADF test static sequence to the 95% and 99% critical value sequences. In both cases the test statistic sequences breach the 95% critical values in the run-up to the financial crisis (2005 and 2006 respectively), before falling below the critical values. The sequence of test statistics for the ps-RS index is higher than for the hedonic index, and breaches the 99% critical value. 

```{r figure2, echo=FALSE, message=FALSE, warning=FALSE, cache = TRUE, fig.height=4.5, fig.width=7.5, fig.cap="Test statistics and critical values for models without drift"}
index_plot <- bubble.test1[,c(1,4,9,11,12,13)]
index_plot <- melt(index_plot, id="Date")  # convert to long format
index_plot$Date <- as.Date(as.yearqtr(index_plot$Date, format = "%Y Q%q"))
g <- ggplot(data=index_plot,aes(x=Date, y=value, group=variable, colour=variable)) 
g <- g + geom_point(size = 1) 
g <- g + geom_line(aes(linetype=variable))
g <- g + scale_linetype_manual(values = c(1,1,1,4,4))
g <- g + ylab("") + xlab("")
g <- g + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
g <- g + theme(legend.title=element_blank()) + theme(legend.position="bottom")
g <- g + scale_x_date(labels = date_format("%Y"),breaks = date_breaks("year"))
g
```

Table 3 reports the origination and termination dates for all of the periods of explosive behaviour, based on 95% critical values. The test statistic sequences for the hedonic indices all indicate a period of explosive prices beginning around 2006/2007 and ending in 2008. The test statistics for the ps-RS indices indicate periods of explosive behaviour that were slightly longer, beginning around 2005/2006 and ending in 2008 or even 2010, depending on the specification. The preferred index in terms of smoothness (i.e. the 1-year adjacent index) suggests a period of bubble formation from 2007Q1 to 2008Q3. @Phillips2012 recommend that only explosive periods lasting more than log(T) units of time should be identified as bubble periods. In this case it implies that the bubble should be at least four quarters in length, and virtually all of the explosive periods identified satisfy this requirement. 

```{r dates, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#report  bubble period dates
datum1 <- data.frame()
datum2 <- data.frame()
datums1 <- data.frame()
datums2 <- data.frame()

bubble.test1 <- bubble.test1[,c(-1,-7)]
bubble.test2 <- bubble.test2[,c(-1,-7)]

for(i in 1:7) {
    for(l in 1:53) {
        if(bubble.test1[l,i]>bubble.test1$"95%"[l]) { 
            datum1[l,i] <- bubble.test1[l,"Date"]
        }
        if(bubble.test2[l,i]>bubble.test2$"95%"[l]) { 
            datum2[l,i] <- bubble.test2[l,"Date"]
        }
    }
    NonNAindex <- which(!is.na(datum1[,i]))
    firstNonNA <- min(NonNAindex)
    datums1[1,i] <- datum1[firstNonNA,i]
    if (NonNAindex[NROW(NonNAindex)-1]==(max(NonNAindex)-1)) { 
        lastNonNA <- max(NonNAindex)
    } else lastNonNA <- NonNAindex[NROW(NonNAindex)-1]
    datums1[2,i] <- datum1[lastNonNA,i]
    
    NonNAindex <- which(!is.na(datum2[,i]))
    firstNonNA <- min(NonNAindex)
    datums2[1,i] <- datum2[firstNonNA,i]
    lastNonNA <- max(NonNAindex)
    datums2[2,i] <- datum2[lastNonNA,i]
}    

datum <- rbind(datums1,datums2)
datum <- t(datum)
datum <- rbind(c("Start","End","Start","End"), datum)
rownames(datum) <- c(' ', colnames(bubble.test1)[1:7])
```

```{r table3, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
xt <- xtable(datum, caption="Dates of explosive price behaviour")
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(paste0('& \\multicolumn{2}{c}{  No Drift  } & \\multicolumn{2}{c}{ Drift }', collapse=''), '\\\\')

print(xt, "latex",comment=FALSE, add.to.row=addtorow, include.colnames=F,
      caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9)
```

The dates identified correspond with many of the explosive periods identified in the literature for a range of assets. In the context of art, @Kraussl2016 identified bubble periods for the 'Post-war and Contemporary' art segment between 2006 and 2008 and for the 'American' art segments between 2005 and 2008, which also corresponds to the pre-financial crisis period. Interestingly, their findings point to evidence in the formation of another bubble in these market segments around the start of 2011. This is not present in the South African art market, which has remained flat since 2009. This is mirrored by South Africa's experience during the Great Recession, which was not as deep as in most developed countries, but was more protracted.

It is also interesting that many of the headline-grabbing auction records for the South African art market occurred in 2011, well after the period of explosive behaviour. This corresponds with findings by @Spaenjers2015, who observed that the timing of record prices does not always coincide with periods of general price increases. They argued that auction price records often occur in situations characterised by extreme supply constraints, resolution of uncertainty about the potential resale value, social competition among 'nouveaux riches', and idiosyncratic shifts in hedonic weights.

###Art Market Segments
```{r robust1, eval=FALSE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
source("full_model.R")

list_expl_vars <- c("lnarea","ah_code","dum_signed","dum_dated","nr_works",
                    "artist","timedummy")
drawing     <- full_model_a(subset(artdata, artdata$med_code=="Drawing"),list_expl_vars)
watercolour <- full_model_a(subset(artdata, artdata$med_code=="Watercolour"),list_expl_vars)
oil         <- full_model_a(subset(artdata, artdata$med_code=="Oil"),list_expl_vars)
acrylic     <- full_model_a(subset(artdata, artdata$med_code=="Acrylic"),list_expl_vars)
print       <- full_model_a(subset(artdata, artdata$med_code=="Print/Woodcut"),list_expl_vars)
mixed       <- full_model_a(subset(artdata, artdata$med_code=="Mixed Media"),list_expl_vars)
sculpture   <- full_model_a(subset(artdata, artdata$med_code=="Sculpture"),list_expl_vars)
photo       <- full_model_a(subset(artdata, artdata$med_code=="Photography"),list_expl_vars)
other       <- full_model_a(subset(artdata, artdata$med_code=="Other"),list_expl_vars)

l <- list(drawing,watercolour,oil,acrylic,print,mixed,sculpture,photo,other)
l <- lapply(l, function(x) data.frame(x, rn = row.names(x)))
mediums <- merge(l[1], l[2], by="rn", all=TRUE) # merge by row names (by=0 or by="row.names")
for(i in 3:9) { mediums <- merge(mediums, l[i], by="rn", all=TRUE)}
mediums <- mediums[,c(-1,-2,-4,-6,-8,-10,-12,-14,-16,-18)]
names(mediums) <- c("drawing","watercolour","oil","acrylic","print","mixed","sculpture","photo","other")
mediums <- rbind(c(100,100,100,100,100,100,100,100,100),mediums)
mediums <- cbind(mediums,hedonic_indices[,1])

write.csv(mediums,"mediums.csv")
#---------------------------------------------------
list_expl_vars <- c("lnarea","ah_code","med_code","dum_signed","dum_dated",  
                    "nr_works","lnrep","med_code:lnarea","timedummy")
expl_vars <- as.formula(paste("lnprice~",paste(list_expl_vars,collapse="+")))
quant <- rq(expl_vars, tau=c(0.25,0.5,0.75), data=artdata)
quant_results <- coef(quant)[grepl("time", rownames(coef(quant))),1:3]
quant_results <- as.data.frame(quant_results)
quant_results <- exp(quant_results)*100
quant_results[,4] <- hedonic_indices[-1,1]
write.csv(quant_results,"quant_results.csv")
```

```{r robust2, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
source("full_model.R")
list_expl_vars <- c("lnarea","ah_code","med_code","dum_signed","dum_dated",  
                    "nr_works","artist","med_code:lnarea","timedummy")
model4.1 <- full_model_a(artdata[artdata$price<=2400,],list_expl_vars) 
model4.2 <- full_model_a(artdata[artdata$price>2400 & artdata$price<22000,],list_expl_vars) 
model4.3 <- full_model_a(artdata[artdata$price>=22000,],list_expl_vars) 

pr_seg <- cbind(model4.1[,c(1,2)],model4.2[,2],model4.3[,2])
colnames(pr_seg) <- c("Date", "Lower", "Middle", "Upper")
pr_seg <- cbind(Date=factor(levels(artdata$timedummy)),
                rbind(c(seq(100,100, length.out=2)),pr_seg[,-1]))

model4.1 <- full_model_a(artdata[artdata$ave_price>=1693,],list_expl_vars) 
model4.2 <- full_model_a(artdata[artdata$ave_price>262 & artdata$ave_price<1693,],list_expl_vars) 
model4.3 <- full_model_a(artdata[artdata$ave_price<=262,],list_expl_vars) 

ave_seg <- cbind(model4.1,model4.2[,2],model4.3[,2])
colnames(ave_seg) <- c("Date", "Lower", "Middle", "Upper")
ave_seg <- cbind(Date=factor(levels(artdata$timedummy)),
                 rbind(c(seq(100,100, length.out=2)),ave_seg[,-1]))

#---------------------------------------------------
mediums <- read.csv("mediums.csv", header=TRUE, sep=",",na.strings = "NA", skipNul = TRUE)
#---------------------------------------------------
quant_results <- read.csv("quant_results.csv", header=TRUE, sep=",",na.strings = "", skipNul = TRUE)[,c(5,2,3,4)]
colnames(quant_results) <- c("Date","tau=0.25","tau=0.50","tau=0.75")
quant_results <- cbind(Date=factor(levels(artdata$timedummy)),
                       rbind(c(seq(100,100, length.out=2)),quant_results[,-1]))
```

Different segments of the South African art market may have exhibited different price trends over time. This section examines different segments of the market, in order to establish in which segments the marked price increases occurred. The market may be segmented in a number of ways, such as by price, artist value, and medium category. The caveat is that slicing the data thinly results in small sample sizes and more volatile indices. This makes it more difficult to discern a pattern and to distinguish the signal from the noise. The indices should therefore be interpreted with caution. 
As discussed above, different segments of the South African art market exhibited different price trends over time. In this section the price indices for the different segments of the art market are examined, in order to establish how widely dispersed the bubble process was and which segments were responsible for the explosive price increases that occurred. The bubble detection tests were performed on the market segment indices defined above in terms of price, medium and artist value. Again, the caveat is that slicing the data thinly results in small sample sizes and more volatile indices. This makes it more difficult to discern a pattern and to distinguish the signal from the noise. 

@Fedderke2014 suggested that the South African art market should be segmented into three price ranges and found different hedonic relationships for the three market segments. The art market may be segmented for a variety of reasons, such as the following: first, wealthy individuals may be less tempted to buy artworks at the lower end of the market that do not signal the same social status; second, small investors are typically unable to purchase more expensive artworks; and third, more expensive artworks may be more prone to speculation [@Renneboog2012]. 

Historical rates of appreciation may therefore have varied across the price distribution [@Renneboog2012]. In order to test this possibility, different parts of the price distribution may be investigated by estimating separate hedonic indices for each segment. This allows the characteristic prices to vary across the price distribution. Three indices were estimated for the bottom 25% of the price distribution ('Lower'), the interquartile range ('Middle'), and the upper 25% of the price distribution ('Upper'). The indices suggest that the dramatic price increases occurred in the upper part of the price distribution, which includes artworks with a hammer price of more than R22,000.

Quantile regressions provide an alternative means to investigate different parts of the price distribution and are also more robust to potential outliers. Quantile regressions can characterise the entire distribution of the dependent variable, as opposed to OLS regressions, which provide estimates for conditional means. The indices resulting from quantile regressions for the 25th, 50th, and 75th percentiles are relatively similar. The quantile art price indices do not exhibit large differences between the different segments. The lower end of the market seems to have depreciated slightly less after the peak in 2008. 

Another potential segmentation is by medium category. It is possible that historical rates of appreciation have varied widely over time for different medium categories. Separate hedonic models may be estimated for each of the mediums. Oil paintings are by far the largest category, representing 52% of the volume and 78% of the value of artworks in the sample. The indices indicate that oil was the medium for which there were the largest price increases.

The results for the origination and termination dates are reported in Table 4. In terms of the price segments, the indices for the upper quartile (top 25%) of the price distribution, from both the OLS and quantile regressions, exhibit evidence of explosive behaviour. This implies that the explosive behaviour occurred mainly in the high-end segment of the art market.

In terms of the medium segments, the indices for watercolour and oil paintings show evidence of explosive behaviour according to both model formulations, with more limited evidence for prints/woodcuts and mixed media. In terms of average artist value, all three indices show evidence of explosive behaviour. This may be because the artist segment indices were estimated based on the average values for a specific artist and include their relatively cheap and more expensive artworks. This implies that the bubble-like behaviour occurred mainly for expensive artworks, and not necessarily for artworks by 'expensive' artists.

```{r bubble_seg, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
#---------------------------------------
#Segmented versions
make_real <- function(indeks) {
    for(i in 2:ncol(indeks)) { 
        for(j in 1:64) {
            indeks[j,i] <- indeks[j,i]/assets$CPI[j]*100 
        }
    }
    return(indeks)
}
alt <- cbind(pr_seg,ave_seg[,-1],mediums[,c(-1,-5,-9,-10)],quant_results[,-1])
alt <- make_real(alt)

y_indices <- log(na.approx(alt[,-1]))
bubble.nc <- list()
bubble.c <- list()
for(i in 1:ncol(y_indices)) {
    bubble1 <- numeric()
    bubble2 <- numeric()
    for(j in 12:64) {
        y <- y_indices[1:j,i]
        toets1 <- ur.df(y, type= "none", lags = 4, selectlags = c("AIC"))
        toets2 <- ur.df(y, type= "drift", lags = 4, selectlags = c("AIC"))
        bubble1 <- rbind(bubble1,toets1@teststat)
        bubble2 <- rbind(bubble2,toets2@teststat)
    }
    bubble.nc[[i]] <- bubble1
    bubble.c[[i]] <- bubble2
}

bubble.test1 <- numeric()
bubble.test2 <- numeric()

for(k in 1:ncol(y_indices)) { 
    bubble.test1 <- cbind(bubble.test1,bubble.nc[[k]])
    bubble.test2 <- cbind(bubble.test2,bubble.c[[k]][1:53])
}
bubble.test1 <- as.data.frame(bubble.test1)
bubble.test2 <- as.data.frame(bubble.test2)
bubble.test1 <- cbind(bubble.test1,K1)
bubble.test2 <- cbind(bubble.test2,K2)

Dates <- levels(artdata$timedummy)[-1:-11]
bubble.test1$Date <- Dates
bubble.test2$Date <- Dates

colnames(bubble.test1)[1:ncol(y_indices)] <- colnames(alt[-1])
colnames(bubble.test2)[1:ncol(y_indices)] <- colnames(alt[-1])

datum1 <- bubble.test1
datum1[!is.na(datum1)] <- NA
datum2 <- bubble.test1
datum2[!is.na(datum2)] <- NA
datums1 <- data.frame()
datums2 <- data.frame()

for(i in 1:15) {
    for(l in 1:53) {
        if(bubble.test1[l,i]>bubble.test1$"95%"[l]) { 
            datum1[l,i] <- bubble.test1[l,"Date"]
        }
        if(bubble.test2[l,i]>bubble.test2$"95%"[l]) { 
            datum2[l,i] <- bubble.test2[l,"Date"]
        }
    }
    if(sum(!is.na(datum1[,i]))>1) {
        NonNAindex <- which(!is.na(datum1[,i]))
        firstNonNA <- min(NonNAindex)
        datums1[1,i] <- datum1[firstNonNA,i]
        if (NonNAindex[NROW(NonNAindex)-1]==(max(NonNAindex)-1)) { 
            lastNonNA <- max(NonNAindex)
        } else lastNonNA <- NonNAindex[NROW(NonNAindex)-1]
    }
    
    datums1[2,i] <- datum1[lastNonNA,i]
    
    
    if(sum(!is.na(datum2[,i]))>1) {
        NonNAindex <- which(!is.na(datum2[,i]))
        firstNonNA <- min(NonNAindex)
        datums2[1,i] <- datum2[firstNonNA,i]
        lastNonNA <- max(NonNAindex)
    }    
    datums2[2,i] <- datum2[lastNonNA,i]
}    

datum <- rbind(datums1,datums2)
datum <- t(datum)
datum <- rbind(c("Start","End","Start","End"), datum)
rownames(datum) <- c(' ', colnames(bubble.test1)[1:15])
rownames(datum)[2:7] <- c("price_lower","price_middle","price_upper",
                          "value_lower","value_middle","value_upper")
datum[is.na(datum)] <- ""       
```

```{r table4, echo=FALSE, results='asis', message=FALSE, cache = TRUE}
xt <- xtable(datum, caption="Dates of explosive price behaviour in the different market segments")
addtorow <- list()
addtorow$pos <- list(0)
addtorow$command <- paste0(paste0('& \\multicolumn{2}{c}{  No Drift  } & \\multicolumn{2}{c}{ Drift }',collapse=''), '\\\\')
xt <- xt[c(1:4,14:16,8:12,5:7),]
print(xt, "latex",comment=FALSE, add.to.row=addtorow, include.colnames=F,
      caption.placement = getOption("xtable.caption.placement", "top"), scalebox = 0.9)
```

The explosive behaviour in art market prices therefore seems to have occurred mainly in the high-end oil and watercolour segments of the market. The origination and termination dates seem to be consistent in suggesting a bubble formation period between 2006/2007 and 2008.

##Discussion
In this section the reduced-form bubble detection method developed by @Phillips2011 was applied to test for periods of explosive behaviour in the art price indices. The use of recursive tests enabled the identification of mildly explosive subsamples in the series. The results indicated that there was evidence of bubble-like behaviour in all of the regression-based art price indices, whereas the simple median index did not exhibit such behaviour. Again, this implies that it is important to control for the composition or quality-mix of items when estimating indices. 

The regression-based indices provide consistent results in terms of the explosive periods in the South African art market, with a potential bubble most likely beginning in 2006 and ending in 2008. The bubble detection tests performed on the different market segments indicated that the bubble process occurred mainly in the high-end oil and watercolour segments. The origination and termination dates were also consistent in suggesting a bubble formation period between 2006/2007 and 2008.

The results assume that the aesthetic or utility dividends associated with South African art did not exhibit explosive behaviour over the period. Aesthetic dividends fluctuate over time, as they depend on buyers' willingness to pay for art, which in turn depends on preferences and wealth. Preferences regarding art and culture would have had to fluctuate dramatically to explain the movements in art prices over the period. Even if trends can temporarily emerge for specific artists, previous findings in the literature have shown that preferences tend to be stable, even in the long run [@Penasse2014]. 

The aesthetic dividend can also fluctuate as wealth fluctuates over time [@Spaenjers2015]. The literature has provided evidence supporting this idea, with @Goetzmann2011 finding cointegrating relationships between art prices and top income brackets. @Mandel2009 analysed the satisfaction derived from conspicuous consumption, which increases as the value of art increases. The part of the aesthetic dividend that is a signal of wealth could plausibly lead to price increases, which in turn could lead to another increase in social status consumption. However, it is unlikely that aesthetic dividends, and factors such as collectors' preferences and wealth, exhibited similar explosive behaviour over the period.

The large increase in art prices between 2005 and 2008 does not seem to be due to a fundamental shift in the types of artworks that were sold over that period. For instance, the top 100 artists in terms of volumes sold, which accounts for 60% of the volume traded and 90% of total turnover, remained remarkably stable over time. Even if the exact same artworks were not being resold, the same artists' work still made up the majority of the market, and the hedonic model controls for the different artists. It is unlikely that the results are driven by sales of systematically better or higher quality artworks by specific artists that appreciated in price before the crisis, and by sales of systematically lower quality artworks by those artists after the crisis. Moreover, paintings are not sold at auction only to profit from price appreciation, or capital gains. A substantial portion of consignments come from the so-called three D's: Debt, Divorce and Death. In other words, many sellers are forced to sell their artworks, even if those artworks have not experienced the largest price appreciation.

Although the bubble detection method provides a consistent basis for identifying periods of explosive behaviour, it does not provide an explanation of the bubble episode. The findings are compatible with several different explanations, including rational bubbles, herd behaviour, and rational responses to fundamentals [@Phillips2011]. 

@Penasse2014 argued that limits to arbitrage induce a speculative component to art prices. Constraints on short selling and high transaction costs may lead to prices diverging from fundamentals, as they prevent arbitrageurs from pulling back prices to fundamental levels [@Balcilar2015]. When prices are high, pessimists would like to short-sell, but instead they simply stay out of the market or sell to optimists at inflated prices. Optimists may be willing to pay higher prices than their own valuations, because they expect to resell to even more optimistic buyers in the future. The price of the option to resell the artwork in the future is the difference between their willingness to pay and their own optimistic valuation. The price of the resale option imparts a bubble component in art prices, and can explain price fluctuations unrelated to fundamentals. These market failures impede the ability to correct price inefficiencies and imply that periods of bubble-like behaviour could exist with little scope for arbitrage. This is especially relevant in art markets, where transaction costs are high, short selling is not possible, and without a rental market, the only possibility to make a profit is by reselling at a higher price [@Penasse2014]. 

@Kindleberger2005 argued that a boom in one market often spills over into other markets. A famous example in the context of art is the link between the boom in Japanese stock and real estate prices and the Impressionist art market in the second half of the 1980s. @Hiraki2009 found a high correlation between Japanese stock prices and the demand for art by Japanese collectors, leading to an increase in the price of Impressionist art during this period. @Kraussl2016 found corroborating evidence of a bubble period in the 'Impressionist and Modern' art segment between 1986 and 1991. During this period Japanese credit was freely available, backed by increasing values of stocks and real estate, which led to a consumption and investment spree through the wealth effect. Japanese investors invested heavily in international art and particularly French Impressionist art in the late 1980s. Luxury consumption by Japanese art collectors increased international art prices until the art bubble burst, as a consequence of the collapse of the Japanese real estate market [@Penasse2014].

Similarly, the run-up to the financial crisis saw large increases in asset prices and credit expansion. It is likely that these conditions contributed to the explosive behaviour in South African art prices between 2006 and 2008. The financial crisis caused the bubble to burst and led to a decline in South African art prices. While an in-depth investigation is outside the scope of the chapter, it does illustrate the usefulness of the art price indices for investigating developments in the South African art market. 

#Conclusion
This paper used a direct method of bubble detection developed by @Phillips2011 to test for explosive behaviour in South African art prices. The test requires the estimation of a reasonably accurate quality-adjusted price index, which can be challenging for unique assets such as art. Each methodology has shortcomings and the danger is that the biases inherent in each methodology may be driving the results. This paper estimates three sets of art price indices, based on the three main methodologies used to estimate quality-adjusted price indices for unique assets.

This section has assessed the validity of the indices produced using the central tendency, hedonic and hybrid repeat sales methods. Each of the regression-based methods employed above has strengths and weaknesses. The hedonic method may suffer from omitted variable bias, which would bias the indices, while the pseudo-repeat sales method may control for some of this omitted variable bias, but suffer more from possible sample selection bias. 

However, the regression-based indices seem to point to the same general movement in South African art prices, with a large increase in the run-up to the Great Recession and a relatively flat trend after 2009. The fairly consistent picture offers some confidence that the indices provide a reasonably accurate measure of the price movements in the South African art market. The external validity tests also suggested that the art price indices provide reasonable estimates of price movements over time, although art prices have lagged substantially behind the asset price indices for other South African assets since 2009.

Moreover, the regression-based indices are significantly different from the central tendency measures and seem to produce better estimates of pure price changes. This is confirmed by the smoothness metrics and the consistent cyclical pattern displayed by the regression-based indices. The 1-year adjacent-period hedonic index performs the best in terms of these smoothness metrics. The implication is that the regression-based methods are useful in producing quality-adjusted price indices for unique items, when the composition or quality-mix is not constant over time.

This section has also investigated different segments of the market, in order to establish in which segments the marked price increases occurred. The results for the different market segments seem to indicate that the dramatic price increases occurred in more expensive or high-end parts of the art market, and especially for oil paintings.

The regression-based indices seem to point to consistent evidence of mildly explosive price behaviour in the run-up to the financial crisis, between 2005/06 and 2008. The bubble seems to have been relatively dispersed throughout the market, although prices seem to have been especially explosive for high-end oil paintings by the top artists.

#References

